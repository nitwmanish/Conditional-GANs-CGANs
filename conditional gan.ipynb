{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.layers import Activation, Dense, Input\n",
    "from keras.layers import Conv2D, Flatten\n",
    "from keras.layers import Reshape, Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The generator network\n",
    "This network consists of four deconvolutional layers. First, I take our input z-vector and labels and concatenated before Dense layer, the feed it into the first deconvolutional layer. Each deconvolutional layer performs a deconvolution operations and then performs batch normalization operations and a leaky ReLu operations as well. Then, finally it return the sigmoid activation function.\n",
    "\n",
    "The Model instance is modified for the z-vector and one-hot vector inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(inputs, labels, image_size):\n",
    "    \"\"\"Build a Generator Model\n",
    "    Inputs are concatenated before Dense layer.\n",
    "    Stack of BatchNormalization-ReLU-Conv2DTranpose to generate fake images.\n",
    "    Output activation is sigmoid instead of tanh in orig DCGAN.\n",
    "    Sigmoid converges easily.\n",
    "    # Arguments\n",
    "        inputs (Layer): Input layer of the generator (the z-vector)\n",
    "        labels (Layer): Input layer for one-hot vector to condition\n",
    "            the inputs\n",
    "        image_size: Target size of one side (assuming square image)\n",
    "    # Returns\n",
    "        Model: Generator Model\n",
    "    \"\"\"\n",
    "    image_resize = image_size // 4\n",
    "    # network parameters\n",
    "    kernel_size = 5\n",
    "    layer_filters = [128, 64, 32, 1]\n",
    "\n",
    "    x = concatenate([inputs, labels], axis=1)\n",
    "    x = Dense(image_resize * image_resize * layer_filters[0])(x)\n",
    "    x = Reshape((image_resize, image_resize, layer_filters[0]))(x)\n",
    "\n",
    "    for filters in layer_filters:\n",
    "        # first two convolution layers use strides = 2\n",
    "        # the last two use strides = 1\n",
    "        if filters > layer_filters[-2]:\n",
    "            strides = 2\n",
    "        else:\n",
    "            strides = 1\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2DTranspose(filters=filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
    "\n",
    "    x = Activation('sigmoid')(x)\n",
    "    # input is conditioned by labels\n",
    "    generator = Model([inputs, labels], x, name='generator')\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The discriminator network\n",
    "The discriminator is the “art critic”, who tries to distinguish between real and fake images. This is a convolutional neural network for image classification. The discriminator network consists of four stack of LeakyReLU-Conv2D. For every layer of the network, I am adding LeakyReLU and convolutional operations, Then, finally it return the sigmoid activation function.\n",
    "\n",
    "The code processes the one-hot vector using a Dense layer and concatenates it with the image input. The Model instance is modified for the image and one-hot vector inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(inputs, labels, image_size):\n",
    "    \"\"\"Build a Discriminator Model\n",
    "    Inputs are concatenated after Dense layer.\n",
    "    Stack of LeakyReLU-Conv2D to discriminate real from fake.\n",
    "    The network does not converge with BN so it is not used here\n",
    "    unlike in DCGAN paper.\n",
    "    # Arguments\n",
    "        inputs (Layer): Input layer of the discriminator (the image)\n",
    "        labels (Layer): Input layer for one-hot vector to condition\n",
    "            the inputs\n",
    "        image_size: Target size of one side (assuming square image)\n",
    "    # Returns\n",
    "        Model: Discriminator Model\n",
    "    \"\"\"\n",
    "    kernel_size = 5\n",
    "    layer_filters = [32, 64, 128, 256]\n",
    "\n",
    "    x = inputs\n",
    "\n",
    "    y = Dense(image_size * image_size)(labels)\n",
    "    y = Reshape((image_size, image_size, 1))(y)\n",
    "    x = concatenate([x, y])\n",
    "\n",
    "    for filters in layer_filters:\n",
    "        # first 3 convolution layers use strides = 2\n",
    "        # last one uses strides = 1\n",
    "        if filters == layer_filters[-1]:\n",
    "            strides = 1\n",
    "        else:\n",
    "            strides = 2\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1)(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    # input is conditioned by labels\n",
    "    discriminator = Model([inputs, labels], x, name='discriminator')\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "The CGAN discriminator is firstly trained with one batch of real and fake data conditioned on their respective one-hot labels. Then, the generator parameters are updated by training the adversarial network given one-hot label conditioned fake data pretending to be real. Similar to DCGAN, the discriminator weights are frozen during adversarial training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(models, data, params):\n",
    "    \"\"\"Train the Discriminator and Adversarial Networks\n",
    "    Alternately train Discriminator and Adversarial networks by batch.\n",
    "    Discriminator is trained first with properly labelled real and fake images.\n",
    "    Adversarial is trained next with fake images pretending to be real.\n",
    "    Discriminator inputs are conditioned by train labels for real images,\n",
    "    and random labels for fake images.\n",
    "    Adversarial inputs are conditioned by random labels.\n",
    "    Generate sample images per save_interval.\n",
    "    # Arguments\n",
    "        models (list): Generator, Discriminator, Adversarial models\n",
    "        data (list): x_train, y_train data\n",
    "        params (list): Network parameters\n",
    "    \"\"\"\n",
    "    # the GAN models\n",
    "    generator, discriminator, adversarial = models\n",
    "    # images and labels\n",
    "    x_train, y_train = data\n",
    "    # network parameters\n",
    "    batch_size, latent_size, train_steps, num_labels, model_name = params\n",
    "    # the generator image is saved every 500 steps\n",
    "    save_interval = 500\n",
    "    # noise vector to see how the generator output evolves during training\n",
    "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_size])\n",
    "    # one-hot label the noise will be conditioned to\n",
    "    noise_class = np.eye(num_labels)[np.arange(0, 16) % num_labels]\n",
    "    # number of elements in train dataset\n",
    "    train_size = x_train.shape[0]\n",
    "\n",
    "    print(model_name,\n",
    "          \"Labels for generated images: \",\n",
    "          np.argmax(noise_class, axis=1))\n",
    "\n",
    "    for i in range(train_steps):\n",
    "        # train the discriminator for 1 batch\n",
    "        # 1 batch of real (label=1.0) and fake images (label=0.0)\n",
    "        # randomly pick real images from dataset\n",
    "        rand_indexes = np.random.randint(0, train_size, size=batch_size)\n",
    "        real_images = x_train[rand_indexes]\n",
    "        # corresponding one-hot labels of real images\n",
    "        real_labels = y_train[rand_indexes]\n",
    "        # generate fake images from noise using generator\n",
    "        # generate noise using uniform distribution\n",
    "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
    "        # assign random one-hot labels\n",
    "        fake_labels = np.eye(num_labels)[np.random.choice(num_labels, batch_size)]\n",
    "\n",
    "        # generate fake images conditioned on fake labels\n",
    "        fake_images = generator.predict([noise, fake_labels])\n",
    "        # real + fake images = 1 batch of train data\n",
    "        x = np.concatenate((real_images, fake_images))\n",
    "        # real + fake one-hot labels = 1 batch of train one-hot labels\n",
    "        labels = np.concatenate((real_labels, fake_labels))\n",
    "\n",
    "        # label real and fake images\n",
    "        # real images label is 1.0\n",
    "        y = np.ones([2 * batch_size, 1])\n",
    "        # fake images label is 0.0\n",
    "        y[batch_size:, :] = 0.0\n",
    "        # train discriminator network, log the loss and accuracy\n",
    "        loss, acc = discriminator.train_on_batch([x, labels], y)\n",
    "        log = \"%d: [discriminator loss: %f, acc: %f]\" % (i, loss, acc)\n",
    "\n",
    "        # train the adversarial network for 1 batch\n",
    "        # 1 batch of fake images conditioned on fake 1-hot labels w/ label=1.0\n",
    "        # since the discriminator weights are frozen in adversarial network\n",
    "        # only the generator is trained\n",
    "        # generate noise using uniform distribution        \n",
    "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
    "        # assign random one-hot labels\n",
    "        fake_labels = np.eye(num_labels)[np.random.choice(num_labels, batch_size)]\n",
    "        # label fake images as real or 1.0\n",
    "        y = np.ones([batch_size, 1])\n",
    "        # train the adversarial network \n",
    "        # note that unlike in discriminator training, \n",
    "        # we do not save the fake images in a variable\n",
    "        # the fake images go to the discriminator input of the adversarial\n",
    "        # for classification\n",
    "        # log the loss and accuracy\n",
    "        loss, acc = adversarial.train_on_batch([noise, fake_labels], y)\n",
    "        log = \"%s [adversarial loss: %f, acc: %f]\" % (log, loss, acc)\n",
    "        print(log)\n",
    "        if (i + 1) % save_interval == 0:\n",
    "            if (i + 1) == train_steps:\n",
    "                show = True\n",
    "            else:\n",
    "                show = False\n",
    "\n",
    "            # plot generator images on a periodic basis\n",
    "            plot_images(generator,noise_input=noise_input,noise_class=noise_class,show=show, step=(i + 1),model_name=model_name)\n",
    "    \n",
    "    # save the model after training the generator\n",
    "    # the trained generator can be reloaded for future MNIST digit generation\n",
    "    generator.save(model_name + \".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "In the last step of our preparation, This is small helper function to display the generated images in the notebook for us, using the matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(generator,noise_input,noise_class,show=False,step=0,model_name=\"gan\"):\n",
    "    \"\"\"Generate fake images and plot them\n",
    "    For visualization purposes, generate fake images\n",
    "    then plot them in a square grid\n",
    "    # Arguments\n",
    "        generator (Model): The Generator Model for fake images generation\n",
    "        noise_input (ndarray): Array of z-vectors\n",
    "        show (bool): Whether to show plot or not\n",
    "        step (int): Appended to filename of the save images\n",
    "        model_name (string): Model name\n",
    "    \"\"\"\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "    filename = os.path.join(model_name, \"%05d.png\" % step)\n",
    "    images = generator.predict([noise_input, noise_class])\n",
    "    print(model_name , \" labels for generated images: \", np.argmax(noise_class, axis=1))\n",
    "    plt.figure(figsize=(2.2, 2.2))\n",
    "    num_images = images.shape[0]\n",
    "    image_size = images.shape[1]\n",
    "    rows = int(math.sqrt(noise_input.shape[0]))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(rows, rows, i + 1)\n",
    "        image = np.reshape(images[i], [image_size, image_size])\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.savefig(filename)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build And Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_models():\n",
    "    # load MNIST dataset\n",
    "    (x_train, y_train), (_, _) = mnist.load_data()\n",
    "\n",
    "    # reshape data for CNN as (28, 28, 1) and normalize\n",
    "    image_size = x_train.shape[1]\n",
    "    x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
    "    x_train = x_train.astype('float32') / 255\n",
    "\n",
    "    num_labels = np.amax(y_train) + 1\n",
    "    y_train = to_categorical(y_train)\n",
    "\n",
    "    model_name = \"cgan_mnist\"\n",
    "    # network parameters\n",
    "    # the latent or z vector is 100-dim\n",
    "    latent_size = 100\n",
    "    batch_size = 64\n",
    "    train_steps = 4000\n",
    "    lr = 2e-4\n",
    "    decay = 6e-8\n",
    "    input_shape = (image_size, image_size, 1)\n",
    "    label_shape = (num_labels, )\n",
    "\n",
    "    # build discriminator model\n",
    "    inputs = Input(shape=input_shape, name='discriminator_input')\n",
    "    labels = Input(shape=label_shape, name='class_labels')\n",
    "\n",
    "    discriminator = build_discriminator(inputs, labels, image_size)\n",
    "    # [1] or original paper uses Adam, \n",
    "    # but discriminator converges easily with RMSprop\n",
    "    optimizer = RMSprop(lr=lr, decay=decay)\n",
    "    discriminator.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "    discriminator.summary()\n",
    "\n",
    "    # build generator model\n",
    "    input_shape = (latent_size, )\n",
    "    inputs = Input(shape=input_shape, name='z_input')\n",
    "    generator = build_generator(inputs, labels, image_size)\n",
    "    generator.summary()\n",
    "\n",
    "    # build adversarial model = generator + discriminator\n",
    "    optimizer = RMSprop(lr=lr*0.5, decay=decay*0.5)\n",
    "    # freeze the weights of discriminator during adversarial training\n",
    "    discriminator.trainable = False\n",
    "    outputs = discriminator([generator([inputs, labels]), labels])\n",
    "    adversarial = Model([inputs, labels],outputs,name=model_name)\n",
    "    adversarial.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "    adversarial.summary()\n",
    "\n",
    "    # train discriminator and adversarial networks\n",
    "    models = (generator, discriminator, adversarial)\n",
    "    data = (x_train, y_train)\n",
    "    params = (batch_size, latent_size, train_steps, num_labels, model_name)\n",
    "    train(models, data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generator(generator, class_label=None):\n",
    "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "    step = 0\n",
    "    if class_label is None:\n",
    "        num_labels = 10\n",
    "        noise_class = np.eye(num_labels)[np.random.choice(num_labels, 16)]\n",
    "    else:\n",
    "        noise_class = np.zeros((16, 10))\n",
    "        noise_class[:,class_label] = 1\n",
    "        step = class_label\n",
    "\n",
    "    plot_images(generator,noise_input=noise_input,noise_class=noise_class,show=True,step=step,model_name=\"test_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "class_labels (InputLayer)       (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 784)          8624        class_labels[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "discriminator_input (InputLayer (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 28, 28, 1)    0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 28, 28, 2)    0           discriminator_input[0][0]        \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 28, 28, 2)    0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 14, 14, 32)   1632        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 14, 14, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 7, 7, 64)     51264       leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 7, 7, 64)     0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 4, 4, 128)    204928      leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 4, 4, 128)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 4, 4, 256)    819456      leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4096)         0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            4097        flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,090,001\n",
      "Trainable params: 1,090,001\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "z_input (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "class_labels (InputLayer)       (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 110)          0           z_input[0][0]                    \n",
      "                                                                 class_labels[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 6272)         696192      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 7, 7, 128)    0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 7, 7, 128)    512         reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 7, 7, 128)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 14, 14, 128)  409728      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 14, 14, 128)  512         conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 14, 14, 128)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 28, 28, 64)   204864      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 28, 28, 64)   256         conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 28, 28, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 28, 28, 32)   51232       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 28, 28, 32)   128         conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 28, 28, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 28, 28, 1)    801         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 28, 28, 1)    0           conv2d_transpose_4[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 1,364,225\n",
      "Trainable params: 1,363,521\n",
      "Non-trainable params: 704\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "z_input (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "class_labels (InputLayer)       (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "generator (Model)               (None, 28, 28, 1)    1364225     z_input[0][0]                    \n",
      "                                                                 class_labels[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "discriminator (Model)           (None, 1)            1090001     generator[1][0]                  \n",
      "                                                                 class_labels[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 2,454,226\n",
      "Trainable params: 1,363,521\n",
      "Non-trainable params: 1,090,705\n",
      "__________________________________________________________________________________________________\n",
      "cgan_mnist Labels for generated images:  [0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maninaya\\Anaconda3\\envs\\env\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [discriminator loss: 0.693949, acc: 0.398438] [adversarial loss: 1.280427, acc: 0.000000]\n",
      "1: [discriminator loss: 0.604973, acc: 0.500000] [adversarial loss: 1.701057, acc: 0.000000]\n",
      "2: [discriminator loss: 0.479374, acc: 0.812500] [adversarial loss: 1.689416, acc: 0.000000]\n",
      "3: [discriminator loss: 0.339849, acc: 1.000000] [adversarial loss: 3.346819, acc: 0.000000]\n",
      "4: [discriminator loss: 0.266478, acc: 0.945312] [adversarial loss: 1.145521, acc: 0.000000]\n",
      "5: [discriminator loss: 0.356519, acc: 1.000000] [adversarial loss: 4.918969, acc: 0.000000]\n",
      "6: [discriminator loss: 0.249324, acc: 0.921875] [adversarial loss: 2.853182, acc: 0.000000]\n",
      "7: [discriminator loss: 0.107997, acc: 0.992188] [adversarial loss: 2.444647, acc: 0.000000]\n",
      "8: [discriminator loss: 0.093041, acc: 0.992188] [adversarial loss: 2.005009, acc: 0.000000]\n",
      "9: [discriminator loss: 0.063101, acc: 1.000000] [adversarial loss: 2.015433, acc: 0.000000]\n",
      "10: [discriminator loss: 0.069285, acc: 0.992188] [adversarial loss: 1.599821, acc: 0.000000]\n",
      "11: [discriminator loss: 0.051125, acc: 1.000000] [adversarial loss: 1.660400, acc: 0.000000]\n",
      "12: [discriminator loss: 0.066982, acc: 0.976562] [adversarial loss: 0.921411, acc: 0.046875]\n",
      "13: [discriminator loss: 0.050761, acc: 1.000000] [adversarial loss: 2.438244, acc: 0.000000]\n",
      "14: [discriminator loss: 0.048686, acc: 0.992188] [adversarial loss: 1.073489, acc: 0.046875]\n",
      "15: [discriminator loss: 0.075053, acc: 0.992188] [adversarial loss: 2.002257, acc: 0.000000]\n",
      "16: [discriminator loss: 0.059693, acc: 0.984375] [adversarial loss: 0.660466, acc: 0.562500]\n",
      "17: [discriminator loss: 0.118775, acc: 0.992188] [adversarial loss: 7.758383, acc: 0.000000]\n",
      "18: [discriminator loss: 0.423729, acc: 0.781250] [adversarial loss: 0.620137, acc: 0.734375]\n",
      "19: [discriminator loss: 0.467181, acc: 0.617188] [adversarial loss: 8.913607, acc: 0.000000]\n",
      "20: [discriminator loss: 0.526109, acc: 0.742188] [adversarial loss: 4.461012, acc: 0.000000]\n",
      "21: [discriminator loss: 0.145146, acc: 0.953125] [adversarial loss: 3.517312, acc: 0.000000]\n",
      "22: [discriminator loss: 0.109851, acc: 0.984375] [adversarial loss: 3.474675, acc: 0.000000]\n",
      "23: [discriminator loss: 0.097056, acc: 0.976562] [adversarial loss: 3.662211, acc: 0.000000]\n",
      "24: [discriminator loss: 0.082302, acc: 0.992188] [adversarial loss: 3.595984, acc: 0.000000]\n",
      "25: [discriminator loss: 0.098295, acc: 0.976562] [adversarial loss: 3.630830, acc: 0.000000]\n",
      "26: [discriminator loss: 0.071294, acc: 1.000000] [adversarial loss: 3.941473, acc: 0.000000]\n",
      "27: [discriminator loss: 0.065458, acc: 0.984375] [adversarial loss: 3.774082, acc: 0.000000]\n",
      "28: [discriminator loss: 0.067351, acc: 0.992188] [adversarial loss: 3.934037, acc: 0.000000]\n",
      "29: [discriminator loss: 0.051887, acc: 0.992188] [adversarial loss: 3.988952, acc: 0.000000]\n",
      "30: [discriminator loss: 0.038354, acc: 1.000000] [adversarial loss: 4.420761, acc: 0.000000]\n",
      "31: [discriminator loss: 0.066312, acc: 0.992188] [adversarial loss: 3.590973, acc: 0.000000]\n",
      "32: [discriminator loss: 0.036673, acc: 1.000000] [adversarial loss: 4.944130, acc: 0.000000]\n",
      "33: [discriminator loss: 0.034865, acc: 1.000000] [adversarial loss: 4.085091, acc: 0.000000]\n",
      "34: [discriminator loss: 0.082472, acc: 1.000000] [adversarial loss: 7.185210, acc: 0.000000]\n",
      "35: [discriminator loss: 0.259607, acc: 0.906250] [adversarial loss: 0.643377, acc: 0.703125]\n",
      "36: [discriminator loss: 0.830574, acc: 0.500000] [adversarial loss: 11.857768, acc: 0.000000]\n",
      "37: [discriminator loss: 0.824307, acc: 0.632812] [adversarial loss: 5.854188, acc: 0.000000]\n",
      "38: [discriminator loss: 0.081756, acc: 1.000000] [adversarial loss: 5.028270, acc: 0.000000]\n",
      "39: [discriminator loss: 0.093911, acc: 1.000000] [adversarial loss: 4.656189, acc: 0.000000]\n",
      "40: [discriminator loss: 0.248438, acc: 0.984375] [adversarial loss: 5.898431, acc: 0.000000]\n",
      "41: [discriminator loss: 0.416553, acc: 0.929688] [adversarial loss: 5.978182, acc: 0.000000]\n",
      "42: [discriminator loss: 0.610728, acc: 0.828125] [adversarial loss: 3.766306, acc: 0.000000]\n",
      "43: [discriminator loss: 0.634905, acc: 0.445312] [adversarial loss: 5.944750, acc: 0.000000]\n",
      "44: [discriminator loss: 0.648549, acc: 0.742188] [adversarial loss: 2.763955, acc: 0.000000]\n",
      "45: [discriminator loss: 0.691727, acc: 0.460938] [adversarial loss: 5.886263, acc: 0.000000]\n",
      "46: [discriminator loss: 0.740865, acc: 0.718750] [adversarial loss: 2.398555, acc: 0.000000]\n",
      "47: [discriminator loss: 0.687590, acc: 0.500000] [adversarial loss: 5.137380, acc: 0.000000]\n",
      "48: [discriminator loss: 0.556373, acc: 0.781250] [adversarial loss: 2.978274, acc: 0.000000]\n",
      "49: [discriminator loss: 0.422454, acc: 0.867188] [adversarial loss: 4.017217, acc: 0.000000]\n",
      "50: [discriminator loss: 0.325890, acc: 0.929688] [adversarial loss: 3.060207, acc: 0.000000]\n",
      "51: [discriminator loss: 0.278702, acc: 0.976562] [adversarial loss: 3.595265, acc: 0.000000]\n",
      "52: [discriminator loss: 0.281783, acc: 0.945312] [adversarial loss: 2.658431, acc: 0.000000]\n",
      "53: [discriminator loss: 0.238819, acc: 0.992188] [adversarial loss: 3.511086, acc: 0.000000]\n",
      "54: [discriminator loss: 0.267548, acc: 0.929688] [adversarial loss: 1.682533, acc: 0.000000]\n",
      "55: [discriminator loss: 0.370054, acc: 0.820312] [adversarial loss: 5.198748, acc: 0.000000]\n",
      "56: [discriminator loss: 0.506417, acc: 0.750000] [adversarial loss: 1.240864, acc: 0.000000]\n",
      "57: [discriminator loss: 0.725825, acc: 0.500000] [adversarial loss: 4.819734, acc: 0.000000]\n",
      "58: [discriminator loss: 0.641724, acc: 0.718750] [adversarial loss: 1.612366, acc: 0.000000]\n",
      "59: [discriminator loss: 0.581545, acc: 0.500000] [adversarial loss: 3.810444, acc: 0.000000]\n",
      "60: [discriminator loss: 0.513516, acc: 0.796875] [adversarial loss: 1.557692, acc: 0.000000]\n",
      "61: [discriminator loss: 0.391946, acc: 0.726562] [adversarial loss: 3.231759, acc: 0.000000]\n",
      "62: [discriminator loss: 0.251312, acc: 0.945312] [adversarial loss: 2.104098, acc: 0.000000]\n",
      "63: [discriminator loss: 0.226069, acc: 0.992188] [adversarial loss: 2.580988, acc: 0.000000]\n",
      "64: [discriminator loss: 0.250001, acc: 0.968750] [adversarial loss: 1.945804, acc: 0.000000]\n",
      "65: [discriminator loss: 0.300907, acc: 0.992188] [adversarial loss: 2.537770, acc: 0.000000]\n",
      "66: [discriminator loss: 0.331031, acc: 0.906250] [adversarial loss: 1.022023, acc: 0.000000]\n",
      "67: [discriminator loss: 0.439351, acc: 0.593750] [adversarial loss: 3.760748, acc: 0.000000]\n",
      "68: [discriminator loss: 0.512402, acc: 0.757812] [adversarial loss: 0.640222, acc: 0.703125]\n",
      "69: [discriminator loss: 0.563448, acc: 0.500000] [adversarial loss: 3.734414, acc: 0.000000]\n",
      "70: [discriminator loss: 0.543165, acc: 0.750000] [adversarial loss: 0.860832, acc: 0.015625]\n",
      "71: [discriminator loss: 0.398599, acc: 0.742188] [adversarial loss: 2.510411, acc: 0.000000]\n",
      "72: [discriminator loss: 0.352084, acc: 0.882812] [adversarial loss: 1.032317, acc: 0.000000]\n",
      "73: [discriminator loss: 0.297819, acc: 1.000000] [adversarial loss: 2.356084, acc: 0.000000]\n",
      "74: [discriminator loss: 0.298941, acc: 0.875000] [adversarial loss: 0.934605, acc: 0.000000]\n",
      "75: [discriminator loss: 0.293689, acc: 0.984375] [adversarial loss: 2.609392, acc: 0.000000]\n",
      "76: [discriminator loss: 0.303732, acc: 0.890625] [adversarial loss: 0.899191, acc: 0.000000]\n",
      "77: [discriminator loss: 0.299457, acc: 0.976562] [adversarial loss: 2.887585, acc: 0.000000]\n",
      "78: [discriminator loss: 0.311985, acc: 0.898438] [adversarial loss: 1.122864, acc: 0.000000]\n",
      "79: [discriminator loss: 0.260901, acc: 1.000000] [adversarial loss: 2.721476, acc: 0.000000]\n",
      "80: [discriminator loss: 0.359878, acc: 0.867188] [adversarial loss: 0.837723, acc: 0.046875]\n",
      "81: [discriminator loss: 0.442143, acc: 0.507812] [adversarial loss: 3.128797, acc: 0.000000]\n",
      "82: [discriminator loss: 0.419255, acc: 0.835938] [adversarial loss: 1.100014, acc: 0.000000]\n",
      "83: [discriminator loss: 0.345723, acc: 0.867188] [adversarial loss: 2.663933, acc: 0.000000]\n",
      "84: [discriminator loss: 0.312494, acc: 0.921875] [adversarial loss: 1.225544, acc: 0.000000]\n",
      "85: [discriminator loss: 0.283835, acc: 0.992188] [adversarial loss: 2.486336, acc: 0.000000]\n",
      "86: [discriminator loss: 0.255970, acc: 0.945312] [adversarial loss: 1.342548, acc: 0.000000]\n",
      "87: [discriminator loss: 0.285991, acc: 1.000000] [adversarial loss: 2.812911, acc: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88: [discriminator loss: 0.384047, acc: 0.882812] [adversarial loss: 0.541381, acc: 0.953125]\n",
      "89: [discriminator loss: 0.680269, acc: 0.500000] [adversarial loss: 4.420385, acc: 0.000000]\n",
      "90: [discriminator loss: 1.258720, acc: 0.570312] [adversarial loss: 0.474724, acc: 1.000000]\n",
      "91: [discriminator loss: 0.788282, acc: 0.500000] [adversarial loss: 2.205306, acc: 0.000000]\n",
      "92: [discriminator loss: 0.390835, acc: 0.843750] [adversarial loss: 0.939355, acc: 0.046875]\n",
      "93: [discriminator loss: 0.346713, acc: 0.914062] [adversarial loss: 1.949677, acc: 0.000000]\n",
      "94: [discriminator loss: 0.297949, acc: 0.937500] [adversarial loss: 1.121472, acc: 0.000000]\n",
      "95: [discriminator loss: 0.286829, acc: 0.976562] [adversarial loss: 1.690777, acc: 0.000000]\n",
      "96: [discriminator loss: 0.276471, acc: 0.945312] [adversarial loss: 1.159191, acc: 0.000000]\n",
      "97: [discriminator loss: 0.285608, acc: 1.000000] [adversarial loss: 2.205151, acc: 0.000000]\n",
      "98: [discriminator loss: 0.468078, acc: 0.804688] [adversarial loss: 0.414156, acc: 1.000000]\n",
      "99: [discriminator loss: 0.637387, acc: 0.500000] [adversarial loss: 2.533545, acc: 0.000000]\n",
      "100: [discriminator loss: 0.502113, acc: 0.789062] [adversarial loss: 0.641578, acc: 0.750000]\n",
      "101: [discriminator loss: 0.483470, acc: 0.500000] [adversarial loss: 2.137849, acc: 0.000000]\n",
      "102: [discriminator loss: 0.406283, acc: 0.828125] [adversarial loss: 0.559774, acc: 0.984375]\n",
      "103: [discriminator loss: 0.520167, acc: 0.500000] [adversarial loss: 2.786364, acc: 0.000000]\n",
      "104: [discriminator loss: 0.599664, acc: 0.726562] [adversarial loss: 0.463726, acc: 1.000000]\n",
      "105: [discriminator loss: 0.582435, acc: 0.500000] [adversarial loss: 2.538253, acc: 0.000000]\n",
      "106: [discriminator loss: 0.509236, acc: 0.742188] [adversarial loss: 0.449094, acc: 1.000000]\n",
      "107: [discriminator loss: 0.600934, acc: 0.500000] [adversarial loss: 2.231051, acc: 0.000000]\n",
      "108: [discriminator loss: 0.352298, acc: 0.882812] [adversarial loss: 0.940233, acc: 0.000000]\n",
      "109: [discriminator loss: 0.409480, acc: 0.718750] [adversarial loss: 2.081811, acc: 0.000000]\n",
      "110: [discriminator loss: 0.443855, acc: 0.851562] [adversarial loss: 0.663108, acc: 0.625000]\n",
      "111: [discriminator loss: 0.545709, acc: 0.500000] [adversarial loss: 2.839432, acc: 0.000000]\n",
      "112: [discriminator loss: 0.728068, acc: 0.718750] [adversarial loss: 0.390558, acc: 1.000000]\n",
      "113: [discriminator loss: 0.746674, acc: 0.500000] [adversarial loss: 1.932679, acc: 0.000000]\n",
      "114: [discriminator loss: 0.397301, acc: 0.921875] [adversarial loss: 0.927599, acc: 0.078125]\n",
      "115: [discriminator loss: 0.439376, acc: 0.656250] [adversarial loss: 1.852172, acc: 0.000000]\n",
      "116: [discriminator loss: 0.448208, acc: 0.867188] [adversarial loss: 0.764057, acc: 0.375000]\n",
      "117: [discriminator loss: 0.461233, acc: 0.562500] [adversarial loss: 2.244757, acc: 0.000000]\n",
      "118: [discriminator loss: 0.508670, acc: 0.796875] [adversarial loss: 0.540389, acc: 0.906250]\n",
      "119: [discriminator loss: 0.659204, acc: 0.500000] [adversarial loss: 2.640209, acc: 0.000000]\n",
      "120: [discriminator loss: 0.830877, acc: 0.585938] [adversarial loss: 0.248841, acc: 1.000000]\n",
      "121: [discriminator loss: 0.987235, acc: 0.500000] [adversarial loss: 1.782702, acc: 0.000000]\n",
      "122: [discriminator loss: 0.572058, acc: 0.804688] [adversarial loss: 0.594643, acc: 0.890625]\n",
      "123: [discriminator loss: 0.595191, acc: 0.500000] [adversarial loss: 2.248193, acc: 0.000000]\n",
      "124: [discriminator loss: 0.606379, acc: 0.679688] [adversarial loss: 0.456853, acc: 0.984375]\n",
      "125: [discriminator loss: 0.655234, acc: 0.500000] [adversarial loss: 2.156824, acc: 0.000000]\n",
      "126: [discriminator loss: 0.516625, acc: 0.781250] [adversarial loss: 0.589965, acc: 0.890625]\n",
      "127: [discriminator loss: 0.538600, acc: 0.500000] [adversarial loss: 2.177419, acc: 0.000000]\n",
      "128: [discriminator loss: 0.517231, acc: 0.750000] [adversarial loss: 0.576129, acc: 0.875000]\n",
      "129: [discriminator loss: 0.554767, acc: 0.500000] [adversarial loss: 2.623669, acc: 0.000000]\n",
      "130: [discriminator loss: 0.549821, acc: 0.679688] [adversarial loss: 0.509726, acc: 0.921875]\n",
      "131: [discriminator loss: 0.609443, acc: 0.492188] [adversarial loss: 2.492851, acc: 0.000000]\n",
      "132: [discriminator loss: 0.541711, acc: 0.710938] [adversarial loss: 0.474576, acc: 0.984375]\n",
      "133: [discriminator loss: 0.664021, acc: 0.500000] [adversarial loss: 2.533811, acc: 0.000000]\n",
      "134: [discriminator loss: 0.661232, acc: 0.640625] [adversarial loss: 0.404433, acc: 1.000000]\n",
      "135: [discriminator loss: 0.716563, acc: 0.500000] [adversarial loss: 2.109377, acc: 0.000000]\n",
      "136: [discriminator loss: 0.591932, acc: 0.695312] [adversarial loss: 0.504706, acc: 0.968750]\n",
      "137: [discriminator loss: 0.654211, acc: 0.500000] [adversarial loss: 2.327036, acc: 0.000000]\n",
      "138: [discriminator loss: 0.587045, acc: 0.656250] [adversarial loss: 0.613618, acc: 0.703125]\n",
      "139: [discriminator loss: 0.554682, acc: 0.500000] [adversarial loss: 2.216980, acc: 0.000000]\n",
      "140: [discriminator loss: 0.479965, acc: 0.726562] [adversarial loss: 0.741537, acc: 0.312500]\n",
      "141: [discriminator loss: 0.487065, acc: 0.601562] [adversarial loss: 2.230974, acc: 0.000000]\n",
      "142: [discriminator loss: 0.521899, acc: 0.726562] [adversarial loss: 0.595962, acc: 0.765625]\n",
      "143: [discriminator loss: 0.525242, acc: 0.507812] [adversarial loss: 2.467711, acc: 0.000000]\n",
      "144: [discriminator loss: 0.425809, acc: 0.804688] [adversarial loss: 0.721829, acc: 0.437500]\n",
      "145: [discriminator loss: 0.475486, acc: 0.640625] [adversarial loss: 2.300470, acc: 0.000000]\n",
      "146: [discriminator loss: 0.495114, acc: 0.742188] [adversarial loss: 0.550615, acc: 0.781250]\n",
      "147: [discriminator loss: 0.631014, acc: 0.515625] [adversarial loss: 2.600543, acc: 0.000000]\n",
      "148: [discriminator loss: 0.695664, acc: 0.632812] [adversarial loss: 0.328175, acc: 1.000000]\n",
      "149: [discriminator loss: 0.778108, acc: 0.500000] [adversarial loss: 1.818915, acc: 0.000000]\n",
      "150: [discriminator loss: 0.461559, acc: 0.781250] [adversarial loss: 0.768859, acc: 0.296875]\n",
      "151: [discriminator loss: 0.556109, acc: 0.625000] [adversarial loss: 2.004484, acc: 0.000000]\n",
      "152: [discriminator loss: 0.567605, acc: 0.687500] [adversarial loss: 0.451530, acc: 0.968750]\n",
      "153: [discriminator loss: 0.641942, acc: 0.500000] [adversarial loss: 2.538427, acc: 0.000000]\n",
      "154: [discriminator loss: 0.647823, acc: 0.601562] [adversarial loss: 0.411034, acc: 0.968750]\n",
      "155: [discriminator loss: 0.683141, acc: 0.500000] [adversarial loss: 2.209292, acc: 0.000000]\n",
      "156: [discriminator loss: 0.531266, acc: 0.695312] [adversarial loss: 0.493623, acc: 0.906250]\n",
      "157: [discriminator loss: 0.617001, acc: 0.515625] [adversarial loss: 2.272400, acc: 0.000000]\n",
      "158: [discriminator loss: 0.573524, acc: 0.664062] [adversarial loss: 0.568164, acc: 0.750000]\n",
      "159: [discriminator loss: 0.516951, acc: 0.562500] [adversarial loss: 2.052939, acc: 0.000000]\n",
      "160: [discriminator loss: 0.513392, acc: 0.718750] [adversarial loss: 0.582472, acc: 0.750000]\n",
      "161: [discriminator loss: 0.515203, acc: 0.570312] [adversarial loss: 2.226446, acc: 0.000000]\n",
      "162: [discriminator loss: 0.526352, acc: 0.703125] [adversarial loss: 0.485330, acc: 0.859375]\n",
      "163: [discriminator loss: 0.594562, acc: 0.562500] [adversarial loss: 2.156678, acc: 0.000000]\n",
      "164: [discriminator loss: 0.558260, acc: 0.734375] [adversarial loss: 0.457498, acc: 0.875000]\n",
      "165: [discriminator loss: 0.613405, acc: 0.523438] [adversarial loss: 2.207731, acc: 0.000000]\n",
      "166: [discriminator loss: 0.624247, acc: 0.609375] [adversarial loss: 0.316053, acc: 1.000000]\n",
      "167: [discriminator loss: 0.734459, acc: 0.507812] [adversarial loss: 2.200086, acc: 0.000000]\n",
      "168: [discriminator loss: 0.597021, acc: 0.648438] [adversarial loss: 0.432192, acc: 0.968750]\n",
      "169: [discriminator loss: 0.627926, acc: 0.578125] [adversarial loss: 1.899569, acc: 0.000000]\n",
      "170: [discriminator loss: 0.579313, acc: 0.640625] [adversarial loss: 0.416391, acc: 0.890625]\n",
      "171: [discriminator loss: 0.624985, acc: 0.531250] [adversarial loss: 1.976020, acc: 0.000000]\n",
      "172: [discriminator loss: 0.597393, acc: 0.640625] [adversarial loss: 0.462093, acc: 0.937500]\n",
      "173: [discriminator loss: 0.560390, acc: 0.585938] [adversarial loss: 1.958555, acc: 0.000000]\n",
      "174: [discriminator loss: 0.547438, acc: 0.703125] [adversarial loss: 0.479741, acc: 0.875000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175: [discriminator loss: 0.549565, acc: 0.632812] [adversarial loss: 1.703334, acc: 0.015625]\n",
      "176: [discriminator loss: 0.491939, acc: 0.750000] [adversarial loss: 0.586812, acc: 0.703125]\n",
      "177: [discriminator loss: 0.543696, acc: 0.695312] [adversarial loss: 1.893637, acc: 0.000000]\n",
      "178: [discriminator loss: 0.542015, acc: 0.734375] [adversarial loss: 0.357144, acc: 0.953125]\n",
      "179: [discriminator loss: 0.536165, acc: 0.640625] [adversarial loss: 1.609533, acc: 0.015625]\n",
      "180: [discriminator loss: 0.515463, acc: 0.726562] [adversarial loss: 0.552346, acc: 0.734375]\n",
      "181: [discriminator loss: 0.545691, acc: 0.656250] [adversarial loss: 1.848547, acc: 0.000000]\n",
      "182: [discriminator loss: 0.645091, acc: 0.554688] [adversarial loss: 0.152213, acc: 1.000000]\n",
      "183: [discriminator loss: 0.727324, acc: 0.515625] [adversarial loss: 1.498623, acc: 0.015625]\n",
      "184: [discriminator loss: 0.531910, acc: 0.687500] [adversarial loss: 0.307370, acc: 1.000000]\n",
      "185: [discriminator loss: 0.673567, acc: 0.562500] [adversarial loss: 1.794633, acc: 0.000000]\n",
      "186: [discriminator loss: 0.549482, acc: 0.648438] [adversarial loss: 0.309916, acc: 0.968750]\n",
      "187: [discriminator loss: 0.716457, acc: 0.554688] [adversarial loss: 1.879588, acc: 0.000000]\n",
      "188: [discriminator loss: 0.589248, acc: 0.617188] [adversarial loss: 0.262756, acc: 1.000000]\n",
      "189: [discriminator loss: 0.659431, acc: 0.562500] [adversarial loss: 1.687567, acc: 0.015625]\n",
      "190: [discriminator loss: 0.656075, acc: 0.562500] [adversarial loss: 0.270974, acc: 1.000000]\n",
      "191: [discriminator loss: 0.758082, acc: 0.515625] [adversarial loss: 2.311716, acc: 0.000000]\n",
      "192: [discriminator loss: 0.766355, acc: 0.523438] [adversarial loss: 0.285303, acc: 0.968750]\n",
      "193: [discriminator loss: 0.705941, acc: 0.546875] [adversarial loss: 1.816344, acc: 0.000000]\n",
      "194: [discriminator loss: 0.644424, acc: 0.570312] [adversarial loss: 0.339816, acc: 0.953125]\n",
      "195: [discriminator loss: 0.707464, acc: 0.546875] [adversarial loss: 1.922979, acc: 0.000000]\n",
      "196: [discriminator loss: 0.676193, acc: 0.523438] [adversarial loss: 0.281578, acc: 1.000000]\n",
      "197: [discriminator loss: 0.694713, acc: 0.562500] [adversarial loss: 1.635199, acc: 0.000000]\n",
      "198: [discriminator loss: 0.676172, acc: 0.578125] [adversarial loss: 0.348624, acc: 0.984375]\n",
      "199: [discriminator loss: 0.818410, acc: 0.484375] [adversarial loss: 1.916863, acc: 0.000000]\n",
      "200: [discriminator loss: 0.741282, acc: 0.546875] [adversarial loss: 0.350420, acc: 0.984375]\n",
      "201: [discriminator loss: 0.708947, acc: 0.554688] [adversarial loss: 1.821628, acc: 0.000000]\n",
      "202: [discriminator loss: 0.620556, acc: 0.570312] [adversarial loss: 0.407291, acc: 0.968750]\n",
      "203: [discriminator loss: 0.627650, acc: 0.578125] [adversarial loss: 1.485803, acc: 0.000000]\n",
      "204: [discriminator loss: 0.572195, acc: 0.671875] [adversarial loss: 0.358174, acc: 1.000000]\n",
      "205: [discriminator loss: 0.619288, acc: 0.593750] [adversarial loss: 1.410485, acc: 0.015625]\n",
      "206: [discriminator loss: 0.602403, acc: 0.640625] [adversarial loss: 0.325165, acc: 1.000000]\n",
      "207: [discriminator loss: 0.662489, acc: 0.539062] [adversarial loss: 1.767780, acc: 0.000000]\n",
      "208: [discriminator loss: 0.699716, acc: 0.523438] [adversarial loss: 0.314471, acc: 0.984375]\n",
      "209: [discriminator loss: 0.770470, acc: 0.500000] [adversarial loss: 2.001726, acc: 0.000000]\n",
      "210: [discriminator loss: 0.753766, acc: 0.546875] [adversarial loss: 0.320492, acc: 1.000000]\n",
      "211: [discriminator loss: 0.783959, acc: 0.500000] [adversarial loss: 2.005532, acc: 0.000000]\n",
      "212: [discriminator loss: 0.747901, acc: 0.531250] [adversarial loss: 0.303218, acc: 1.000000]\n",
      "213: [discriminator loss: 0.690446, acc: 0.546875] [adversarial loss: 1.431588, acc: 0.000000]\n",
      "214: [discriminator loss: 0.636793, acc: 0.617188] [adversarial loss: 0.485324, acc: 0.828125]\n",
      "215: [discriminator loss: 0.650812, acc: 0.546875] [adversarial loss: 1.656420, acc: 0.000000]\n",
      "216: [discriminator loss: 0.661359, acc: 0.601562] [adversarial loss: 0.440164, acc: 0.984375]\n",
      "217: [discriminator loss: 0.662005, acc: 0.562500] [adversarial loss: 1.797949, acc: 0.000000]\n",
      "218: [discriminator loss: 0.648671, acc: 0.546875] [adversarial loss: 0.432353, acc: 0.984375]\n",
      "219: [discriminator loss: 0.700707, acc: 0.523438] [adversarial loss: 2.111223, acc: 0.000000]\n",
      "220: [discriminator loss: 0.741386, acc: 0.507812] [adversarial loss: 0.347381, acc: 0.984375]\n",
      "221: [discriminator loss: 0.688964, acc: 0.554688] [adversarial loss: 1.672629, acc: 0.000000]\n",
      "222: [discriminator loss: 0.637150, acc: 0.585938] [adversarial loss: 0.442487, acc: 0.906250]\n",
      "223: [discriminator loss: 0.686007, acc: 0.562500] [adversarial loss: 1.545057, acc: 0.000000]\n",
      "224: [discriminator loss: 0.626668, acc: 0.578125] [adversarial loss: 0.404403, acc: 1.000000]\n",
      "225: [discriminator loss: 0.677846, acc: 0.539062] [adversarial loss: 1.659224, acc: 0.000000]\n",
      "226: [discriminator loss: 0.678083, acc: 0.562500] [adversarial loss: 0.397385, acc: 1.000000]\n",
      "227: [discriminator loss: 0.655234, acc: 0.554688] [adversarial loss: 1.538478, acc: 0.000000]\n",
      "228: [discriminator loss: 0.618753, acc: 0.632812] [adversarial loss: 0.607550, acc: 0.687500]\n",
      "229: [discriminator loss: 0.640192, acc: 0.562500] [adversarial loss: 1.592277, acc: 0.000000]\n",
      "230: [discriminator loss: 0.580840, acc: 0.656250] [adversarial loss: 0.551461, acc: 0.734375]\n",
      "231: [discriminator loss: 0.606997, acc: 0.671875] [adversarial loss: 1.588418, acc: 0.000000]\n",
      "232: [discriminator loss: 0.611263, acc: 0.640625] [adversarial loss: 0.357359, acc: 1.000000]\n",
      "233: [discriminator loss: 0.685520, acc: 0.523438] [adversarial loss: 1.739630, acc: 0.000000]\n",
      "234: [discriminator loss: 0.632504, acc: 0.601562] [adversarial loss: 0.478435, acc: 0.890625]\n",
      "235: [discriminator loss: 0.605289, acc: 0.625000] [adversarial loss: 1.308188, acc: 0.000000]\n",
      "236: [discriminator loss: 0.647081, acc: 0.664062] [adversarial loss: 0.525596, acc: 0.843750]\n",
      "237: [discriminator loss: 0.633950, acc: 0.554688] [adversarial loss: 1.963248, acc: 0.000000]\n",
      "238: [discriminator loss: 0.662012, acc: 0.570312] [adversarial loss: 0.431769, acc: 0.890625]\n",
      "239: [discriminator loss: 0.643991, acc: 0.554688] [adversarial loss: 1.703234, acc: 0.000000]\n",
      "240: [discriminator loss: 0.649850, acc: 0.601562] [adversarial loss: 0.440027, acc: 0.953125]\n",
      "241: [discriminator loss: 0.632291, acc: 0.593750] [adversarial loss: 1.352661, acc: 0.000000]\n",
      "242: [discriminator loss: 0.595893, acc: 0.687500] [adversarial loss: 0.471347, acc: 0.906250]\n",
      "243: [discriminator loss: 0.615285, acc: 0.554688] [adversarial loss: 1.490457, acc: 0.000000]\n",
      "244: [discriminator loss: 0.622269, acc: 0.671875] [adversarial loss: 0.490355, acc: 0.875000]\n",
      "245: [discriminator loss: 0.702644, acc: 0.531250] [adversarial loss: 1.697173, acc: 0.000000]\n",
      "246: [discriminator loss: 0.657079, acc: 0.539062] [adversarial loss: 0.356601, acc: 0.953125]\n",
      "247: [discriminator loss: 0.731299, acc: 0.531250] [adversarial loss: 1.737447, acc: 0.000000]\n",
      "248: [discriminator loss: 0.718025, acc: 0.539062] [adversarial loss: 0.401587, acc: 0.937500]\n",
      "249: [discriminator loss: 0.646918, acc: 0.562500] [adversarial loss: 1.303180, acc: 0.000000]\n",
      "250: [discriminator loss: 0.606664, acc: 0.625000] [adversarial loss: 0.470646, acc: 0.875000]\n",
      "251: [discriminator loss: 0.623533, acc: 0.601562] [adversarial loss: 1.515684, acc: 0.000000]\n",
      "252: [discriminator loss: 0.652704, acc: 0.562500] [adversarial loss: 0.379329, acc: 0.953125]\n",
      "253: [discriminator loss: 0.729752, acc: 0.515625] [adversarial loss: 1.723318, acc: 0.000000]\n",
      "254: [discriminator loss: 0.674446, acc: 0.531250] [adversarial loss: 0.439067, acc: 0.968750]\n",
      "255: [discriminator loss: 0.675316, acc: 0.507812] [adversarial loss: 1.483681, acc: 0.000000]\n",
      "256: [discriminator loss: 0.640724, acc: 0.593750] [adversarial loss: 0.518665, acc: 0.843750]\n",
      "257: [discriminator loss: 0.621094, acc: 0.562500] [adversarial loss: 1.606438, acc: 0.000000]\n",
      "258: [discriminator loss: 0.598466, acc: 0.539062] [adversarial loss: 0.501982, acc: 0.875000]\n",
      "259: [discriminator loss: 0.606589, acc: 0.593750] [adversarial loss: 1.578808, acc: 0.000000]\n",
      "260: [discriminator loss: 0.598685, acc: 0.601562] [adversarial loss: 0.508174, acc: 0.921875]\n",
      "261: [discriminator loss: 0.635317, acc: 0.523438] [adversarial loss: 1.629441, acc: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262: [discriminator loss: 0.642164, acc: 0.546875] [adversarial loss: 0.425057, acc: 0.953125]\n",
      "263: [discriminator loss: 0.654329, acc: 0.531250] [adversarial loss: 1.596988, acc: 0.000000]\n",
      "264: [discriminator loss: 0.643955, acc: 0.554688] [adversarial loss: 0.478160, acc: 0.890625]\n",
      "265: [discriminator loss: 0.652950, acc: 0.515625] [adversarial loss: 1.510000, acc: 0.000000]\n",
      "266: [discriminator loss: 0.615617, acc: 0.593750] [adversarial loss: 0.482773, acc: 0.890625]\n",
      "267: [discriminator loss: 0.629391, acc: 0.515625] [adversarial loss: 1.529294, acc: 0.000000]\n",
      "268: [discriminator loss: 0.594705, acc: 0.617188] [adversarial loss: 0.507298, acc: 0.859375]\n",
      "269: [discriminator loss: 0.642002, acc: 0.546875] [adversarial loss: 1.579219, acc: 0.000000]\n",
      "270: [discriminator loss: 0.630565, acc: 0.585938] [adversarial loss: 0.464558, acc: 0.890625]\n",
      "271: [discriminator loss: 0.647349, acc: 0.546875] [adversarial loss: 1.746850, acc: 0.000000]\n",
      "272: [discriminator loss: 0.670524, acc: 0.531250] [adversarial loss: 0.481593, acc: 0.921875]\n",
      "273: [discriminator loss: 0.660048, acc: 0.546875] [adversarial loss: 1.747917, acc: 0.000000]\n",
      "274: [discriminator loss: 0.691358, acc: 0.531250] [adversarial loss: 0.524769, acc: 0.859375]\n",
      "275: [discriminator loss: 0.647387, acc: 0.531250] [adversarial loss: 1.602506, acc: 0.000000]\n",
      "276: [discriminator loss: 0.654295, acc: 0.531250] [adversarial loss: 0.524908, acc: 0.843750]\n",
      "277: [discriminator loss: 0.589023, acc: 0.601562] [adversarial loss: 1.428018, acc: 0.000000]\n",
      "278: [discriminator loss: 0.571979, acc: 0.671875] [adversarial loss: 0.604919, acc: 0.765625]\n",
      "279: [discriminator loss: 0.580328, acc: 0.601562] [adversarial loss: 1.692309, acc: 0.000000]\n",
      "280: [discriminator loss: 0.661616, acc: 0.554688] [adversarial loss: 0.433663, acc: 0.968750]\n",
      "281: [discriminator loss: 0.647792, acc: 0.500000] [adversarial loss: 1.576539, acc: 0.000000]\n",
      "282: [discriminator loss: 0.632057, acc: 0.570312] [adversarial loss: 0.519549, acc: 0.859375]\n",
      "283: [discriminator loss: 0.631833, acc: 0.562500] [adversarial loss: 1.388398, acc: 0.000000]\n",
      "284: [discriminator loss: 0.601647, acc: 0.648438] [adversarial loss: 0.552112, acc: 0.828125]\n",
      "285: [discriminator loss: 0.625961, acc: 0.546875] [adversarial loss: 1.284891, acc: 0.015625]\n",
      "286: [discriminator loss: 0.564927, acc: 0.664062] [adversarial loss: 0.595700, acc: 0.750000]\n",
      "287: [discriminator loss: 0.597398, acc: 0.593750] [adversarial loss: 1.442801, acc: 0.015625]\n",
      "288: [discriminator loss: 0.616562, acc: 0.648438] [adversarial loss: 0.493104, acc: 0.906250]\n",
      "289: [discriminator loss: 0.599225, acc: 0.539062] [adversarial loss: 1.566578, acc: 0.000000]\n",
      "290: [discriminator loss: 0.644135, acc: 0.617188] [adversarial loss: 0.472193, acc: 0.937500]\n",
      "291: [discriminator loss: 0.640746, acc: 0.523438] [adversarial loss: 1.342618, acc: 0.000000]\n",
      "292: [discriminator loss: 0.607000, acc: 0.617188] [adversarial loss: 0.533018, acc: 0.843750]\n",
      "293: [discriminator loss: 0.663679, acc: 0.515625] [adversarial loss: 1.595926, acc: 0.000000]\n",
      "294: [discriminator loss: 0.684198, acc: 0.554688] [adversarial loss: 0.423996, acc: 1.000000]\n",
      "295: [discriminator loss: 0.690180, acc: 0.484375] [adversarial loss: 1.561828, acc: 0.000000]\n",
      "296: [discriminator loss: 0.656915, acc: 0.578125] [adversarial loss: 0.554956, acc: 0.890625]\n",
      "297: [discriminator loss: 0.621377, acc: 0.570312] [adversarial loss: 1.252608, acc: 0.000000]\n",
      "298: [discriminator loss: 0.596960, acc: 0.671875] [adversarial loss: 0.603558, acc: 0.718750]\n",
      "299: [discriminator loss: 0.600203, acc: 0.562500] [adversarial loss: 1.555800, acc: 0.000000]\n",
      "300: [discriminator loss: 0.660977, acc: 0.625000] [adversarial loss: 0.490525, acc: 0.875000]\n",
      "301: [discriminator loss: 0.667675, acc: 0.515625] [adversarial loss: 1.456248, acc: 0.000000]\n",
      "302: [discriminator loss: 0.674339, acc: 0.531250] [adversarial loss: 0.457233, acc: 0.906250]\n",
      "303: [discriminator loss: 0.665435, acc: 0.531250] [adversarial loss: 1.249824, acc: 0.000000]\n",
      "304: [discriminator loss: 0.644016, acc: 0.585938] [adversarial loss: 0.585428, acc: 0.796875]\n",
      "305: [discriminator loss: 0.617215, acc: 0.570312] [adversarial loss: 1.275048, acc: 0.000000]\n",
      "306: [discriminator loss: 0.603514, acc: 0.640625] [adversarial loss: 0.572945, acc: 0.765625]\n",
      "307: [discriminator loss: 0.636307, acc: 0.562500] [adversarial loss: 1.392167, acc: 0.000000]\n",
      "308: [discriminator loss: 0.626579, acc: 0.617188] [adversarial loss: 0.526395, acc: 0.812500]\n",
      "309: [discriminator loss: 0.638281, acc: 0.570312] [adversarial loss: 1.548274, acc: 0.000000]\n",
      "310: [discriminator loss: 0.719160, acc: 0.562500] [adversarial loss: 0.440197, acc: 0.953125]\n",
      "311: [discriminator loss: 0.689330, acc: 0.515625] [adversarial loss: 1.620566, acc: 0.000000]\n",
      "312: [discriminator loss: 0.729155, acc: 0.515625] [adversarial loss: 0.462310, acc: 0.953125]\n",
      "313: [discriminator loss: 0.662046, acc: 0.500000] [adversarial loss: 1.330585, acc: 0.000000]\n",
      "314: [discriminator loss: 0.672991, acc: 0.562500] [adversarial loss: 0.527038, acc: 0.859375]\n",
      "315: [discriminator loss: 0.648138, acc: 0.539062] [adversarial loss: 1.221876, acc: 0.031250]\n",
      "316: [discriminator loss: 0.588171, acc: 0.671875] [adversarial loss: 0.637372, acc: 0.640625]\n",
      "317: [discriminator loss: 0.601820, acc: 0.632812] [adversarial loss: 1.132735, acc: 0.015625]\n",
      "318: [discriminator loss: 0.601699, acc: 0.695312] [adversarial loss: 0.624882, acc: 0.671875]\n",
      "319: [discriminator loss: 0.608971, acc: 0.648438] [adversarial loss: 1.251722, acc: 0.031250]\n",
      "320: [discriminator loss: 0.669061, acc: 0.601562] [adversarial loss: 0.441735, acc: 0.921875]\n",
      "321: [discriminator loss: 0.681052, acc: 0.500000] [adversarial loss: 1.334674, acc: 0.000000]\n",
      "322: [discriminator loss: 0.664815, acc: 0.562500] [adversarial loss: 0.445960, acc: 0.953125]\n",
      "323: [discriminator loss: 0.699159, acc: 0.523438] [adversarial loss: 1.199409, acc: 0.000000]\n",
      "324: [discriminator loss: 0.616294, acc: 0.601562] [adversarial loss: 0.477018, acc: 0.937500]\n",
      "325: [discriminator loss: 0.635950, acc: 0.539062] [adversarial loss: 1.326826, acc: 0.000000]\n",
      "326: [discriminator loss: 0.661936, acc: 0.531250] [adversarial loss: 0.479331, acc: 0.921875]\n",
      "327: [discriminator loss: 0.661598, acc: 0.531250] [adversarial loss: 1.299033, acc: 0.000000]\n",
      "328: [discriminator loss: 0.650540, acc: 0.546875] [adversarial loss: 0.528070, acc: 0.859375]\n",
      "329: [discriminator loss: 0.630716, acc: 0.562500] [adversarial loss: 1.269175, acc: 0.000000]\n",
      "330: [discriminator loss: 0.629951, acc: 0.656250] [adversarial loss: 0.530034, acc: 0.859375]\n",
      "331: [discriminator loss: 0.661189, acc: 0.554688] [adversarial loss: 1.294893, acc: 0.000000]\n",
      "332: [discriminator loss: 0.620739, acc: 0.617188] [adversarial loss: 0.511379, acc: 0.937500]\n",
      "333: [discriminator loss: 0.620937, acc: 0.554688] [adversarial loss: 1.447004, acc: 0.000000]\n",
      "334: [discriminator loss: 0.638412, acc: 0.539062] [adversarial loss: 0.460636, acc: 0.968750]\n",
      "335: [discriminator loss: 0.649557, acc: 0.531250] [adversarial loss: 1.305792, acc: 0.000000]\n",
      "336: [discriminator loss: 0.630181, acc: 0.578125] [adversarial loss: 0.493335, acc: 0.906250]\n",
      "337: [discriminator loss: 0.663817, acc: 0.554688] [adversarial loss: 1.084609, acc: 0.000000]\n",
      "338: [discriminator loss: 0.614266, acc: 0.679688] [adversarial loss: 0.631667, acc: 0.640625]\n",
      "339: [discriminator loss: 0.589771, acc: 0.695312] [adversarial loss: 0.996476, acc: 0.062500]\n",
      "340: [discriminator loss: 0.559409, acc: 0.781250] [adversarial loss: 0.684006, acc: 0.546875]\n",
      "341: [discriminator loss: 0.604170, acc: 0.656250] [adversarial loss: 1.142077, acc: 0.000000]\n",
      "342: [discriminator loss: 0.603494, acc: 0.687500] [adversarial loss: 0.456205, acc: 0.968750]\n",
      "343: [discriminator loss: 0.638982, acc: 0.546875] [adversarial loss: 1.622555, acc: 0.000000]\n",
      "344: [discriminator loss: 0.694452, acc: 0.515625] [adversarial loss: 0.403720, acc: 0.937500]\n",
      "345: [discriminator loss: 0.678014, acc: 0.492188] [adversarial loss: 1.479794, acc: 0.000000]\n",
      "346: [discriminator loss: 0.711119, acc: 0.523438] [adversarial loss: 0.470682, acc: 0.906250]\n",
      "347: [discriminator loss: 0.674575, acc: 0.507812] [adversarial loss: 1.332046, acc: 0.000000]\n",
      "348: [discriminator loss: 0.657705, acc: 0.578125] [adversarial loss: 0.648777, acc: 0.609375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349: [discriminator loss: 0.592311, acc: 0.593750] [adversarial loss: 1.230728, acc: 0.000000]\n",
      "350: [discriminator loss: 0.633005, acc: 0.687500] [adversarial loss: 0.596964, acc: 0.781250]\n",
      "351: [discriminator loss: 0.593898, acc: 0.656250] [adversarial loss: 1.092799, acc: 0.046875]\n",
      "352: [discriminator loss: 0.596446, acc: 0.695312] [adversarial loss: 0.559665, acc: 0.781250]\n",
      "353: [discriminator loss: 0.579518, acc: 0.679688] [adversarial loss: 1.038680, acc: 0.015625]\n",
      "354: [discriminator loss: 0.583420, acc: 0.703125] [adversarial loss: 0.544318, acc: 0.812500]\n",
      "355: [discriminator loss: 0.633822, acc: 0.617188] [adversarial loss: 1.357352, acc: 0.000000]\n",
      "356: [discriminator loss: 0.655637, acc: 0.546875] [adversarial loss: 0.457695, acc: 0.937500]\n",
      "357: [discriminator loss: 0.691292, acc: 0.554688] [adversarial loss: 1.569403, acc: 0.000000]\n",
      "358: [discriminator loss: 0.715053, acc: 0.515625] [adversarial loss: 0.449372, acc: 0.890625]\n",
      "359: [discriminator loss: 0.662587, acc: 0.546875] [adversarial loss: 1.357885, acc: 0.000000]\n",
      "360: [discriminator loss: 0.638244, acc: 0.585938] [adversarial loss: 0.556401, acc: 0.765625]\n",
      "361: [discriminator loss: 0.649655, acc: 0.539062] [adversarial loss: 1.227156, acc: 0.015625]\n",
      "362: [discriminator loss: 0.631700, acc: 0.625000] [adversarial loss: 0.568034, acc: 0.781250]\n",
      "363: [discriminator loss: 0.645360, acc: 0.554688] [adversarial loss: 1.056861, acc: 0.093750]\n",
      "364: [discriminator loss: 0.613178, acc: 0.664062] [adversarial loss: 0.612005, acc: 0.703125]\n",
      "365: [discriminator loss: 0.633108, acc: 0.585938] [adversarial loss: 1.139159, acc: 0.015625]\n",
      "366: [discriminator loss: 0.622899, acc: 0.625000] [adversarial loss: 0.513536, acc: 0.859375]\n",
      "367: [discriminator loss: 0.613727, acc: 0.578125] [adversarial loss: 1.234346, acc: 0.000000]\n",
      "368: [discriminator loss: 0.624058, acc: 0.625000] [adversarial loss: 0.558748, acc: 0.875000]\n",
      "369: [discriminator loss: 0.611925, acc: 0.601562] [adversarial loss: 1.277074, acc: 0.000000]\n",
      "370: [discriminator loss: 0.666270, acc: 0.570312] [adversarial loss: 0.499002, acc: 0.906250]\n",
      "371: [discriminator loss: 0.695124, acc: 0.523438] [adversarial loss: 1.441627, acc: 0.000000]\n",
      "372: [discriminator loss: 0.714132, acc: 0.523438] [adversarial loss: 0.433815, acc: 0.953125]\n",
      "373: [discriminator loss: 0.680374, acc: 0.531250] [adversarial loss: 1.226171, acc: 0.000000]\n",
      "374: [discriminator loss: 0.672740, acc: 0.531250] [adversarial loss: 0.602838, acc: 0.703125]\n",
      "375: [discriminator loss: 0.634508, acc: 0.593750] [adversarial loss: 1.141631, acc: 0.031250]\n",
      "376: [discriminator loss: 0.651926, acc: 0.593750] [adversarial loss: 0.533656, acc: 0.812500]\n",
      "377: [discriminator loss: 0.635355, acc: 0.570312] [adversarial loss: 1.298485, acc: 0.000000]\n",
      "378: [discriminator loss: 0.640840, acc: 0.585938] [adversarial loss: 0.575968, acc: 0.781250]\n",
      "379: [discriminator loss: 0.593072, acc: 0.671875] [adversarial loss: 1.106749, acc: 0.015625]\n",
      "380: [discriminator loss: 0.613876, acc: 0.632812] [adversarial loss: 0.514717, acc: 0.906250]\n",
      "381: [discriminator loss: 0.688828, acc: 0.523438] [adversarial loss: 1.269919, acc: 0.000000]\n",
      "382: [discriminator loss: 0.680688, acc: 0.515625] [adversarial loss: 0.500349, acc: 0.875000]\n",
      "383: [discriminator loss: 0.656452, acc: 0.554688] [adversarial loss: 1.261893, acc: 0.000000]\n",
      "384: [discriminator loss: 0.633073, acc: 0.554688] [adversarial loss: 0.547393, acc: 0.843750]\n",
      "385: [discriminator loss: 0.636206, acc: 0.585938] [adversarial loss: 1.268362, acc: 0.000000]\n",
      "386: [discriminator loss: 0.631694, acc: 0.562500] [adversarial loss: 0.490735, acc: 0.937500]\n",
      "387: [discriminator loss: 0.609162, acc: 0.593750] [adversarial loss: 1.210991, acc: 0.015625]\n",
      "388: [discriminator loss: 0.642047, acc: 0.609375] [adversarial loss: 0.515105, acc: 0.921875]\n",
      "389: [discriminator loss: 0.634353, acc: 0.578125] [adversarial loss: 1.156787, acc: 0.015625]\n",
      "390: [discriminator loss: 0.598288, acc: 0.632812] [adversarial loss: 0.586997, acc: 0.750000]\n",
      "391: [discriminator loss: 0.663824, acc: 0.539062] [adversarial loss: 1.190960, acc: 0.000000]\n",
      "392: [discriminator loss: 0.622766, acc: 0.648438] [adversarial loss: 0.600886, acc: 0.781250]\n",
      "393: [discriminator loss: 0.667288, acc: 0.562500] [adversarial loss: 1.264475, acc: 0.015625]\n",
      "394: [discriminator loss: 0.662801, acc: 0.570312] [adversarial loss: 0.468305, acc: 0.953125]\n",
      "395: [discriminator loss: 0.705263, acc: 0.507812] [adversarial loss: 1.398204, acc: 0.000000]\n",
      "396: [discriminator loss: 0.693078, acc: 0.515625] [adversarial loss: 0.512468, acc: 0.812500]\n",
      "397: [discriminator loss: 0.692634, acc: 0.507812] [adversarial loss: 1.360353, acc: 0.000000]\n",
      "398: [discriminator loss: 0.678961, acc: 0.554688] [adversarial loss: 0.523424, acc: 0.859375]\n",
      "399: [discriminator loss: 0.689169, acc: 0.539062] [adversarial loss: 1.229833, acc: 0.000000]\n",
      "400: [discriminator loss: 0.661290, acc: 0.601562] [adversarial loss: 0.599511, acc: 0.750000]\n",
      "401: [discriminator loss: 0.626394, acc: 0.585938] [adversarial loss: 1.260430, acc: 0.015625]\n",
      "402: [discriminator loss: 0.664350, acc: 0.570312] [adversarial loss: 0.446944, acc: 0.984375]\n",
      "403: [discriminator loss: 0.693828, acc: 0.507812] [adversarial loss: 1.235131, acc: 0.000000]\n",
      "404: [discriminator loss: 0.659892, acc: 0.562500] [adversarial loss: 0.547504, acc: 0.843750]\n",
      "405: [discriminator loss: 0.624660, acc: 0.593750] [adversarial loss: 0.968028, acc: 0.062500]\n",
      "406: [discriminator loss: 0.581774, acc: 0.742188] [adversarial loss: 0.606462, acc: 0.718750]\n",
      "407: [discriminator loss: 0.603977, acc: 0.671875] [adversarial loss: 1.134513, acc: 0.015625]\n",
      "408: [discriminator loss: 0.638788, acc: 0.578125] [adversarial loss: 0.563159, acc: 0.765625]\n",
      "409: [discriminator loss: 0.640847, acc: 0.585938] [adversarial loss: 1.480000, acc: 0.000000]\n",
      "410: [discriminator loss: 0.743647, acc: 0.523438] [adversarial loss: 0.397127, acc: 0.953125]\n",
      "411: [discriminator loss: 0.723879, acc: 0.515625] [adversarial loss: 1.419904, acc: 0.000000]\n",
      "412: [discriminator loss: 0.725826, acc: 0.523438] [adversarial loss: 0.597215, acc: 0.765625]\n",
      "413: [discriminator loss: 0.664298, acc: 0.468750] [adversarial loss: 1.137203, acc: 0.015625]\n",
      "414: [discriminator loss: 0.623915, acc: 0.664062] [adversarial loss: 0.648995, acc: 0.609375]\n",
      "415: [discriminator loss: 0.579855, acc: 0.679688] [adversarial loss: 0.998545, acc: 0.109375]\n",
      "416: [discriminator loss: 0.619216, acc: 0.664062] [adversarial loss: 0.544127, acc: 0.812500]\n",
      "417: [discriminator loss: 0.601837, acc: 0.625000] [adversarial loss: 0.907902, acc: 0.093750]\n",
      "418: [discriminator loss: 0.608404, acc: 0.726562] [adversarial loss: 0.672525, acc: 0.625000]\n",
      "419: [discriminator loss: 0.631653, acc: 0.632812] [adversarial loss: 1.094181, acc: 0.000000]\n",
      "420: [discriminator loss: 0.603232, acc: 0.625000] [adversarial loss: 0.535965, acc: 0.875000]\n",
      "421: [discriminator loss: 0.614182, acc: 0.625000] [adversarial loss: 1.213171, acc: 0.031250]\n",
      "422: [discriminator loss: 0.649346, acc: 0.554688] [adversarial loss: 0.462162, acc: 0.968750]\n",
      "423: [discriminator loss: 0.667832, acc: 0.523438] [adversarial loss: 1.616489, acc: 0.000000]\n",
      "424: [discriminator loss: 0.760226, acc: 0.507812] [adversarial loss: 0.415798, acc: 0.968750]\n",
      "425: [discriminator loss: 0.732086, acc: 0.515625] [adversarial loss: 1.482193, acc: 0.000000]\n",
      "426: [discriminator loss: 0.703056, acc: 0.523438] [adversarial loss: 0.524063, acc: 0.890625]\n",
      "427: [discriminator loss: 0.674595, acc: 0.570312] [adversarial loss: 1.238953, acc: 0.000000]\n",
      "428: [discriminator loss: 0.623742, acc: 0.625000] [adversarial loss: 0.621564, acc: 0.703125]\n",
      "429: [discriminator loss: 0.615726, acc: 0.585938] [adversarial loss: 1.162595, acc: 0.000000]\n",
      "430: [discriminator loss: 0.654388, acc: 0.617188] [adversarial loss: 0.563544, acc: 0.812500]\n",
      "431: [discriminator loss: 0.611882, acc: 0.570312] [adversarial loss: 1.119609, acc: 0.046875]\n",
      "432: [discriminator loss: 0.632886, acc: 0.648438] [adversarial loss: 0.550976, acc: 0.875000]\n",
      "433: [discriminator loss: 0.650006, acc: 0.554688] [adversarial loss: 1.234757, acc: 0.000000]\n",
      "434: [discriminator loss: 0.666676, acc: 0.585938] [adversarial loss: 0.556941, acc: 0.734375]\n",
      "435: [discriminator loss: 0.638755, acc: 0.570312] [adversarial loss: 1.140929, acc: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436: [discriminator loss: 0.646096, acc: 0.585938] [adversarial loss: 0.567950, acc: 0.734375]\n",
      "437: [discriminator loss: 0.636374, acc: 0.578125] [adversarial loss: 1.291237, acc: 0.000000]\n",
      "438: [discriminator loss: 0.624100, acc: 0.609375] [adversarial loss: 0.546267, acc: 0.781250]\n",
      "439: [discriminator loss: 0.629453, acc: 0.578125] [adversarial loss: 1.184627, acc: 0.015625]\n",
      "440: [discriminator loss: 0.646192, acc: 0.562500] [adversarial loss: 0.512832, acc: 0.890625]\n",
      "441: [discriminator loss: 0.669427, acc: 0.539062] [adversarial loss: 1.276497, acc: 0.000000]\n",
      "442: [discriminator loss: 0.685447, acc: 0.562500] [adversarial loss: 0.427809, acc: 0.968750]\n",
      "443: [discriminator loss: 0.699482, acc: 0.523438] [adversarial loss: 1.430298, acc: 0.000000]\n",
      "444: [discriminator loss: 0.711513, acc: 0.546875] [adversarial loss: 0.505471, acc: 0.875000]\n",
      "445: [discriminator loss: 0.672040, acc: 0.539062] [adversarial loss: 1.456771, acc: 0.000000]\n",
      "446: [discriminator loss: 0.709272, acc: 0.546875] [adversarial loss: 0.491177, acc: 0.890625]\n",
      "447: [discriminator loss: 0.651158, acc: 0.523438] [adversarial loss: 1.157121, acc: 0.000000]\n",
      "448: [discriminator loss: 0.630433, acc: 0.570312] [adversarial loss: 0.646942, acc: 0.578125]\n",
      "449: [discriminator loss: 0.595368, acc: 0.664062] [adversarial loss: 0.926865, acc: 0.125000]\n",
      "450: [discriminator loss: 0.646529, acc: 0.632812] [adversarial loss: 0.612791, acc: 0.718750]\n",
      "451: [discriminator loss: 0.609715, acc: 0.648438] [adversarial loss: 1.085745, acc: 0.031250]\n",
      "452: [discriminator loss: 0.597059, acc: 0.656250] [adversarial loss: 0.539832, acc: 0.843750]\n",
      "453: [discriminator loss: 0.649639, acc: 0.546875] [adversarial loss: 1.417442, acc: 0.000000]\n",
      "454: [discriminator loss: 0.638391, acc: 0.601562] [adversarial loss: 0.525382, acc: 0.843750]\n",
      "455: [discriminator loss: 0.652250, acc: 0.531250] [adversarial loss: 1.491066, acc: 0.000000]\n",
      "456: [discriminator loss: 0.661098, acc: 0.601562] [adversarial loss: 0.524747, acc: 0.875000]\n",
      "457: [discriminator loss: 0.642129, acc: 0.562500] [adversarial loss: 1.208367, acc: 0.000000]\n",
      "458: [discriminator loss: 0.595735, acc: 0.656250] [adversarial loss: 0.620481, acc: 0.656250]\n",
      "459: [discriminator loss: 0.636914, acc: 0.554688] [adversarial loss: 1.143952, acc: 0.000000]\n",
      "460: [discriminator loss: 0.603283, acc: 0.671875] [adversarial loss: 0.574534, acc: 0.718750]\n",
      "461: [discriminator loss: 0.642643, acc: 0.578125] [adversarial loss: 1.250455, acc: 0.000000]\n",
      "462: [discriminator loss: 0.621186, acc: 0.625000] [adversarial loss: 0.477883, acc: 0.906250]\n",
      "463: [discriminator loss: 0.689745, acc: 0.515625] [adversarial loss: 1.556610, acc: 0.000000]\n",
      "464: [discriminator loss: 0.733051, acc: 0.546875] [adversarial loss: 0.480081, acc: 0.937500]\n",
      "465: [discriminator loss: 0.655096, acc: 0.531250] [adversarial loss: 1.348346, acc: 0.000000]\n",
      "466: [discriminator loss: 0.671786, acc: 0.601562] [adversarial loss: 0.517626, acc: 0.843750]\n",
      "467: [discriminator loss: 0.643529, acc: 0.601562] [adversarial loss: 0.973187, acc: 0.062500]\n",
      "468: [discriminator loss: 0.587193, acc: 0.656250] [adversarial loss: 0.683218, acc: 0.593750]\n",
      "469: [discriminator loss: 0.575444, acc: 0.726562] [adversarial loss: 0.908432, acc: 0.171875]\n",
      "470: [discriminator loss: 0.604156, acc: 0.695312] [adversarial loss: 0.737626, acc: 0.421875]\n",
      "471: [discriminator loss: 0.614107, acc: 0.671875] [adversarial loss: 1.023151, acc: 0.093750]\n",
      "472: [discriminator loss: 0.635621, acc: 0.648438] [adversarial loss: 0.638474, acc: 0.671875]\n",
      "473: [discriminator loss: 0.626710, acc: 0.664062] [adversarial loss: 1.402598, acc: 0.000000]\n",
      "474: [discriminator loss: 0.652809, acc: 0.601562] [adversarial loss: 0.464973, acc: 0.921875]\n",
      "475: [discriminator loss: 0.662365, acc: 0.523438] [adversarial loss: 1.532520, acc: 0.000000]\n",
      "476: [discriminator loss: 0.749039, acc: 0.523438] [adversarial loss: 0.441041, acc: 0.953125]\n",
      "477: [discriminator loss: 0.680090, acc: 0.531250] [adversarial loss: 1.490146, acc: 0.000000]\n",
      "478: [discriminator loss: 0.678198, acc: 0.578125] [adversarial loss: 0.569103, acc: 0.765625]\n",
      "479: [discriminator loss: 0.627870, acc: 0.570312] [adversarial loss: 1.029190, acc: 0.046875]\n",
      "480: [discriminator loss: 0.615058, acc: 0.640625] [adversarial loss: 0.621544, acc: 0.703125]\n",
      "481: [discriminator loss: 0.646022, acc: 0.585938] [adversarial loss: 1.063616, acc: 0.046875]\n",
      "482: [discriminator loss: 0.627927, acc: 0.640625] [adversarial loss: 0.659923, acc: 0.562500]\n",
      "483: [discriminator loss: 0.635256, acc: 0.625000] [adversarial loss: 1.129076, acc: 0.078125]\n",
      "484: [discriminator loss: 0.673769, acc: 0.546875] [adversarial loss: 0.542317, acc: 0.828125]\n",
      "485: [discriminator loss: 0.617644, acc: 0.601562] [adversarial loss: 1.214799, acc: 0.015625]\n",
      "486: [discriminator loss: 0.613173, acc: 0.632812] [adversarial loss: 0.561381, acc: 0.796875]\n",
      "487: [discriminator loss: 0.623099, acc: 0.593750] [adversarial loss: 1.329827, acc: 0.000000]\n",
      "488: [discriminator loss: 0.645649, acc: 0.609375] [adversarial loss: 0.477535, acc: 0.921875]\n",
      "489: [discriminator loss: 0.694544, acc: 0.546875] [adversarial loss: 1.349364, acc: 0.015625]\n",
      "490: [discriminator loss: 0.701884, acc: 0.554688] [adversarial loss: 0.507703, acc: 0.890625]\n",
      "491: [discriminator loss: 0.706399, acc: 0.523438] [adversarial loss: 1.619407, acc: 0.000000]\n",
      "492: [discriminator loss: 0.689070, acc: 0.578125] [adversarial loss: 0.549725, acc: 0.812500]\n",
      "493: [discriminator loss: 0.650954, acc: 0.539062] [adversarial loss: 1.109464, acc: 0.015625]\n",
      "494: [discriminator loss: 0.648786, acc: 0.632812] [adversarial loss: 0.551618, acc: 0.812500]\n",
      "495: [discriminator loss: 0.627931, acc: 0.625000] [adversarial loss: 0.945210, acc: 0.125000]\n",
      "496: [discriminator loss: 0.593267, acc: 0.718750] [adversarial loss: 0.714244, acc: 0.468750]\n",
      "497: [discriminator loss: 0.606959, acc: 0.718750] [adversarial loss: 0.932861, acc: 0.125000]\n",
      "498: [discriminator loss: 0.610335, acc: 0.718750] [adversarial loss: 0.674000, acc: 0.578125]\n",
      "499: [discriminator loss: 0.639253, acc: 0.570312] [adversarial loss: 1.192987, acc: 0.000000]\n",
      "cgan_mnist  labels for generated images:  [0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5]\n",
      "500: [discriminator loss: 0.628334, acc: 0.632812] [adversarial loss: 0.572258, acc: 0.765625]\n",
      "501: [discriminator loss: 0.649718, acc: 0.531250] [adversarial loss: 1.701553, acc: 0.000000]\n",
      "502: [discriminator loss: 0.741939, acc: 0.578125] [adversarial loss: 0.493569, acc: 0.875000]\n",
      "503: [discriminator loss: 0.679302, acc: 0.546875] [adversarial loss: 1.184982, acc: 0.015625]\n",
      "504: [discriminator loss: 0.647324, acc: 0.554688] [adversarial loss: 0.544499, acc: 0.812500]\n",
      "505: [discriminator loss: 0.642674, acc: 0.593750] [adversarial loss: 1.235597, acc: 0.031250]\n",
      "506: [discriminator loss: 0.656058, acc: 0.593750] [adversarial loss: 0.538235, acc: 0.859375]\n",
      "507: [discriminator loss: 0.624761, acc: 0.617188] [adversarial loss: 1.172204, acc: 0.031250]\n",
      "508: [discriminator loss: 0.650943, acc: 0.578125] [adversarial loss: 0.574681, acc: 0.765625]\n",
      "509: [discriminator loss: 0.640919, acc: 0.617188] [adversarial loss: 1.196264, acc: 0.015625]\n",
      "510: [discriminator loss: 0.617835, acc: 0.640625] [adversarial loss: 0.600482, acc: 0.734375]\n",
      "511: [discriminator loss: 0.630309, acc: 0.609375] [adversarial loss: 1.218437, acc: 0.000000]\n",
      "512: [discriminator loss: 0.642479, acc: 0.617188] [adversarial loss: 0.518346, acc: 0.859375]\n",
      "513: [discriminator loss: 0.651576, acc: 0.570312] [adversarial loss: 1.395456, acc: 0.000000]\n",
      "514: [discriminator loss: 0.689976, acc: 0.562500] [adversarial loss: 0.527104, acc: 0.812500]\n",
      "515: [discriminator loss: 0.698064, acc: 0.507812] [adversarial loss: 1.384390, acc: 0.000000]\n",
      "516: [discriminator loss: 0.666439, acc: 0.593750] [adversarial loss: 0.533663, acc: 0.843750]\n",
      "517: [discriminator loss: 0.675208, acc: 0.546875] [adversarial loss: 1.072846, acc: 0.078125]\n",
      "518: [discriminator loss: 0.624554, acc: 0.625000] [adversarial loss: 0.639652, acc: 0.640625]\n",
      "519: [discriminator loss: 0.629791, acc: 0.585938] [adversarial loss: 1.149204, acc: 0.046875]\n",
      "520: [discriminator loss: 0.622610, acc: 0.656250] [adversarial loss: 0.586086, acc: 0.781250]\n",
      "521: [discriminator loss: 0.666296, acc: 0.554688] [adversarial loss: 1.171654, acc: 0.031250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522: [discriminator loss: 0.631379, acc: 0.632812] [adversarial loss: 0.624012, acc: 0.640625]\n",
      "523: [discriminator loss: 0.601596, acc: 0.625000] [adversarial loss: 0.942782, acc: 0.046875]\n",
      "524: [discriminator loss: 0.589044, acc: 0.742188] [adversarial loss: 0.699059, acc: 0.500000]\n",
      "525: [discriminator loss: 0.613598, acc: 0.679688] [adversarial loss: 0.870588, acc: 0.218750]\n",
      "526: [discriminator loss: 0.599517, acc: 0.695312] [adversarial loss: 0.773313, acc: 0.328125]\n",
      "527: [discriminator loss: 0.634890, acc: 0.609375] [adversarial loss: 1.022208, acc: 0.093750]\n",
      "528: [discriminator loss: 0.583328, acc: 0.703125] [adversarial loss: 0.685906, acc: 0.562500]\n",
      "529: [discriminator loss: 0.611116, acc: 0.648438] [adversarial loss: 1.267606, acc: 0.015625]\n",
      "530: [discriminator loss: 0.651864, acc: 0.617188] [adversarial loss: 0.432843, acc: 0.921875]\n",
      "531: [discriminator loss: 0.778065, acc: 0.468750] [adversarial loss: 1.898060, acc: 0.000000]\n",
      "532: [discriminator loss: 0.856667, acc: 0.523438] [adversarial loss: 0.543716, acc: 0.812500]\n",
      "533: [discriminator loss: 0.672663, acc: 0.554688] [adversarial loss: 1.056757, acc: 0.078125]\n",
      "534: [discriminator loss: 0.674173, acc: 0.570312] [adversarial loss: 0.615019, acc: 0.718750]\n",
      "535: [discriminator loss: 0.625785, acc: 0.562500] [adversarial loss: 1.083546, acc: 0.046875]\n",
      "536: [discriminator loss: 0.660637, acc: 0.570312] [adversarial loss: 0.632588, acc: 0.640625]\n",
      "537: [discriminator loss: 0.634579, acc: 0.601562] [adversarial loss: 1.078650, acc: 0.046875]\n",
      "538: [discriminator loss: 0.620340, acc: 0.632812] [adversarial loss: 0.645119, acc: 0.671875]\n",
      "539: [discriminator loss: 0.628634, acc: 0.601562] [adversarial loss: 0.906975, acc: 0.125000]\n",
      "540: [discriminator loss: 0.630363, acc: 0.609375] [adversarial loss: 0.743964, acc: 0.390625]\n",
      "541: [discriminator loss: 0.644617, acc: 0.640625] [adversarial loss: 1.038687, acc: 0.046875]\n",
      "542: [discriminator loss: 0.635899, acc: 0.679688] [adversarial loss: 0.689933, acc: 0.500000]\n",
      "543: [discriminator loss: 0.612128, acc: 0.625000] [adversarial loss: 1.198260, acc: 0.015625]\n",
      "544: [discriminator loss: 0.622533, acc: 0.625000] [adversarial loss: 0.529018, acc: 0.812500]\n",
      "545: [discriminator loss: 0.635356, acc: 0.578125] [adversarial loss: 1.206319, acc: 0.000000]\n",
      "546: [discriminator loss: 0.679782, acc: 0.523438] [adversarial loss: 0.443247, acc: 0.937500]\n",
      "547: [discriminator loss: 0.661941, acc: 0.585938] [adversarial loss: 1.194892, acc: 0.031250]\n",
      "548: [discriminator loss: 0.642728, acc: 0.609375] [adversarial loss: 0.551415, acc: 0.859375]\n",
      "549: [discriminator loss: 0.660888, acc: 0.570312] [adversarial loss: 1.343125, acc: 0.000000]\n",
      "550: [discriminator loss: 0.636372, acc: 0.593750] [adversarial loss: 0.518551, acc: 0.890625]\n",
      "551: [discriminator loss: 0.644947, acc: 0.546875] [adversarial loss: 1.331491, acc: 0.000000]\n",
      "552: [discriminator loss: 0.656849, acc: 0.593750] [adversarial loss: 0.602975, acc: 0.750000]\n",
      "553: [discriminator loss: 0.635678, acc: 0.578125] [adversarial loss: 1.058224, acc: 0.062500]\n",
      "554: [discriminator loss: 0.612579, acc: 0.664062] [adversarial loss: 0.595845, acc: 0.703125]\n",
      "555: [discriminator loss: 0.642723, acc: 0.562500] [adversarial loss: 1.103939, acc: 0.015625]\n",
      "556: [discriminator loss: 0.638828, acc: 0.632812] [adversarial loss: 0.631261, acc: 0.656250]\n",
      "557: [discriminator loss: 0.624927, acc: 0.578125] [adversarial loss: 1.218011, acc: 0.015625]\n",
      "558: [discriminator loss: 0.638055, acc: 0.601562] [adversarial loss: 0.595154, acc: 0.687500]\n",
      "559: [discriminator loss: 0.637601, acc: 0.546875] [adversarial loss: 1.259285, acc: 0.000000]\n",
      "560: [discriminator loss: 0.617521, acc: 0.625000] [adversarial loss: 0.539725, acc: 0.796875]\n",
      "561: [discriminator loss: 0.619247, acc: 0.570312] [adversarial loss: 1.144155, acc: 0.031250]\n",
      "562: [discriminator loss: 0.620594, acc: 0.640625] [adversarial loss: 0.646687, acc: 0.687500]\n",
      "563: [discriminator loss: 0.636996, acc: 0.625000] [adversarial loss: 1.077695, acc: 0.109375]\n",
      "564: [discriminator loss: 0.585141, acc: 0.726562] [adversarial loss: 0.842355, acc: 0.250000]\n",
      "565: [discriminator loss: 0.653298, acc: 0.632812] [adversarial loss: 0.971682, acc: 0.187500]\n",
      "566: [discriminator loss: 0.583901, acc: 0.710938] [adversarial loss: 0.653349, acc: 0.609375]\n",
      "567: [discriminator loss: 0.604953, acc: 0.695312] [adversarial loss: 0.922399, acc: 0.250000]\n",
      "568: [discriminator loss: 0.560657, acc: 0.750000] [adversarial loss: 0.848513, acc: 0.234375]\n",
      "569: [discriminator loss: 0.595807, acc: 0.687500] [adversarial loss: 0.966135, acc: 0.187500]\n",
      "570: [discriminator loss: 0.590144, acc: 0.695312] [adversarial loss: 0.548891, acc: 0.781250]\n",
      "571: [discriminator loss: 0.618837, acc: 0.585938] [adversarial loss: 1.777004, acc: 0.000000]\n",
      "572: [discriminator loss: 0.745137, acc: 0.531250] [adversarial loss: 0.354060, acc: 0.968750]\n",
      "573: [discriminator loss: 0.699178, acc: 0.523438] [adversarial loss: 1.318217, acc: 0.000000]\n",
      "574: [discriminator loss: 0.667871, acc: 0.593750] [adversarial loss: 0.520928, acc: 0.859375]\n",
      "575: [discriminator loss: 0.684881, acc: 0.531250] [adversarial loss: 1.390455, acc: 0.000000]\n",
      "576: [discriminator loss: 0.649194, acc: 0.578125] [adversarial loss: 0.599111, acc: 0.718750]\n",
      "577: [discriminator loss: 0.620120, acc: 0.601562] [adversarial loss: 1.222616, acc: 0.000000]\n",
      "578: [discriminator loss: 0.627587, acc: 0.609375] [adversarial loss: 0.711215, acc: 0.468750]\n",
      "579: [discriminator loss: 0.604377, acc: 0.664062] [adversarial loss: 0.941586, acc: 0.093750]\n",
      "580: [discriminator loss: 0.579349, acc: 0.710938] [adversarial loss: 0.837387, acc: 0.203125]\n",
      "581: [discriminator loss: 0.612946, acc: 0.617188] [adversarial loss: 1.050570, acc: 0.046875]\n",
      "582: [discriminator loss: 0.602658, acc: 0.679688] [adversarial loss: 0.778106, acc: 0.343750]\n",
      "583: [discriminator loss: 0.592543, acc: 0.703125] [adversarial loss: 0.986906, acc: 0.078125]\n",
      "584: [discriminator loss: 0.622967, acc: 0.656250] [adversarial loss: 0.615434, acc: 0.703125]\n",
      "585: [discriminator loss: 0.637543, acc: 0.593750] [adversarial loss: 1.252558, acc: 0.015625]\n",
      "586: [discriminator loss: 0.661697, acc: 0.554688] [adversarial loss: 0.440642, acc: 0.921875]\n",
      "587: [discriminator loss: 0.690950, acc: 0.523438] [adversarial loss: 1.639650, acc: 0.031250]\n",
      "588: [discriminator loss: 0.694683, acc: 0.593750] [adversarial loss: 0.475540, acc: 0.875000]\n",
      "589: [discriminator loss: 0.655642, acc: 0.515625] [adversarial loss: 1.402008, acc: 0.031250]\n",
      "590: [discriminator loss: 0.630129, acc: 0.632812] [adversarial loss: 0.698247, acc: 0.531250]\n",
      "591: [discriminator loss: 0.609564, acc: 0.593750] [adversarial loss: 1.094656, acc: 0.046875]\n",
      "592: [discriminator loss: 0.611669, acc: 0.679688] [adversarial loss: 0.694319, acc: 0.500000]\n",
      "593: [discriminator loss: 0.588805, acc: 0.742188] [adversarial loss: 0.952823, acc: 0.156250]\n",
      "594: [discriminator loss: 0.568336, acc: 0.710938] [adversarial loss: 0.810918, acc: 0.312500]\n",
      "595: [discriminator loss: 0.596254, acc: 0.773438] [adversarial loss: 0.800532, acc: 0.296875]\n",
      "596: [discriminator loss: 0.589943, acc: 0.734375] [adversarial loss: 1.060262, acc: 0.140625]\n",
      "597: [discriminator loss: 0.628319, acc: 0.664062] [adversarial loss: 0.565044, acc: 0.781250]\n",
      "598: [discriminator loss: 0.624945, acc: 0.601562] [adversarial loss: 1.603027, acc: 0.015625]\n",
      "599: [discriminator loss: 0.705328, acc: 0.578125] [adversarial loss: 0.421460, acc: 0.984375]\n",
      "600: [discriminator loss: 0.713621, acc: 0.531250] [adversarial loss: 1.594416, acc: 0.000000]\n",
      "601: [discriminator loss: 0.668799, acc: 0.593750] [adversarial loss: 0.609495, acc: 0.687500]\n",
      "602: [discriminator loss: 0.649207, acc: 0.625000] [adversarial loss: 1.136339, acc: 0.031250]\n",
      "603: [discriminator loss: 0.608182, acc: 0.671875] [adversarial loss: 0.811580, acc: 0.359375]\n",
      "604: [discriminator loss: 0.567491, acc: 0.742188] [adversarial loss: 0.844279, acc: 0.359375]\n",
      "605: [discriminator loss: 0.617234, acc: 0.648438] [adversarial loss: 0.961364, acc: 0.109375]\n",
      "606: [discriminator loss: 0.569750, acc: 0.765625] [adversarial loss: 0.864991, acc: 0.187500]\n",
      "607: [discriminator loss: 0.572085, acc: 0.750000] [adversarial loss: 0.851868, acc: 0.328125]\n",
      "608: [discriminator loss: 0.573287, acc: 0.664062] [adversarial loss: 1.035469, acc: 0.093750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "609: [discriminator loss: 0.582208, acc: 0.703125] [adversarial loss: 0.622135, acc: 0.671875]\n",
      "610: [discriminator loss: 0.620346, acc: 0.601562] [adversarial loss: 1.583388, acc: 0.000000]\n",
      "611: [discriminator loss: 0.694215, acc: 0.578125] [adversarial loss: 0.413305, acc: 0.921875]\n",
      "612: [discriminator loss: 0.722900, acc: 0.523438] [adversarial loss: 1.673454, acc: 0.000000]\n",
      "613: [discriminator loss: 0.811627, acc: 0.492188] [adversarial loss: 0.497019, acc: 0.843750]\n",
      "614: [discriminator loss: 0.645385, acc: 0.539062] [adversarial loss: 1.151358, acc: 0.031250]\n",
      "615: [discriminator loss: 0.632740, acc: 0.648438] [adversarial loss: 0.683454, acc: 0.546875]\n",
      "616: [discriminator loss: 0.627823, acc: 0.632812] [adversarial loss: 1.128609, acc: 0.031250]\n",
      "617: [discriminator loss: 0.613337, acc: 0.601562] [adversarial loss: 0.613416, acc: 0.718750]\n",
      "618: [discriminator loss: 0.604541, acc: 0.601562] [adversarial loss: 0.977944, acc: 0.109375]\n",
      "619: [discriminator loss: 0.608318, acc: 0.664062] [adversarial loss: 0.701602, acc: 0.531250]\n",
      "620: [discriminator loss: 0.596943, acc: 0.718750] [adversarial loss: 0.887575, acc: 0.265625]\n",
      "621: [discriminator loss: 0.551567, acc: 0.820312] [adversarial loss: 0.984862, acc: 0.156250]\n",
      "622: [discriminator loss: 0.613137, acc: 0.648438] [adversarial loss: 0.681693, acc: 0.593750]\n",
      "623: [discriminator loss: 0.617756, acc: 0.625000] [adversarial loss: 1.298203, acc: 0.031250]\n",
      "624: [discriminator loss: 0.620402, acc: 0.640625] [adversarial loss: 0.495338, acc: 0.875000]\n",
      "625: [discriminator loss: 0.658733, acc: 0.554688] [adversarial loss: 1.656329, acc: 0.000000]\n",
      "626: [discriminator loss: 0.696540, acc: 0.562500] [adversarial loss: 0.540283, acc: 0.750000]\n",
      "627: [discriminator loss: 0.643232, acc: 0.617188] [adversarial loss: 1.213878, acc: 0.031250]\n",
      "628: [discriminator loss: 0.630931, acc: 0.617188] [adversarial loss: 0.575737, acc: 0.750000]\n",
      "629: [discriminator loss: 0.621299, acc: 0.601562] [adversarial loss: 1.302624, acc: 0.078125]\n",
      "630: [discriminator loss: 0.599346, acc: 0.703125] [adversarial loss: 0.642751, acc: 0.609375]\n",
      "631: [discriminator loss: 0.588157, acc: 0.664062] [adversarial loss: 1.330698, acc: 0.078125]\n",
      "632: [discriminator loss: 0.617367, acc: 0.671875] [adversarial loss: 0.524780, acc: 0.796875]\n",
      "633: [discriminator loss: 0.694355, acc: 0.562500] [adversarial loss: 1.432711, acc: 0.015625]\n",
      "634: [discriminator loss: 0.678432, acc: 0.609375] [adversarial loss: 0.536134, acc: 0.812500]\n",
      "635: [discriminator loss: 0.610754, acc: 0.609375] [adversarial loss: 1.092569, acc: 0.078125]\n",
      "636: [discriminator loss: 0.548392, acc: 0.750000] [adversarial loss: 0.832970, acc: 0.218750]\n",
      "637: [discriminator loss: 0.579801, acc: 0.703125] [adversarial loss: 1.075497, acc: 0.062500]\n",
      "638: [discriminator loss: 0.604907, acc: 0.687500] [adversarial loss: 0.797187, acc: 0.359375]\n",
      "639: [discriminator loss: 0.587991, acc: 0.726562] [adversarial loss: 1.015641, acc: 0.171875]\n",
      "640: [discriminator loss: 0.598477, acc: 0.664062] [adversarial loss: 0.668355, acc: 0.609375]\n",
      "641: [discriminator loss: 0.593772, acc: 0.687500] [adversarial loss: 1.434232, acc: 0.015625]\n",
      "642: [discriminator loss: 0.636388, acc: 0.609375] [adversarial loss: 0.502672, acc: 0.890625]\n",
      "643: [discriminator loss: 0.633518, acc: 0.601562] [adversarial loss: 1.536375, acc: 0.015625]\n",
      "644: [discriminator loss: 0.661106, acc: 0.585938] [adversarial loss: 0.557539, acc: 0.812500]\n",
      "645: [discriminator loss: 0.639242, acc: 0.578125] [adversarial loss: 1.320766, acc: 0.046875]\n",
      "646: [discriminator loss: 0.657599, acc: 0.593750] [adversarial loss: 0.496758, acc: 0.812500]\n",
      "647: [discriminator loss: 0.713272, acc: 0.515625] [adversarial loss: 1.309930, acc: 0.015625]\n",
      "648: [discriminator loss: 0.637421, acc: 0.617188] [adversarial loss: 0.660253, acc: 0.593750]\n",
      "649: [discriminator loss: 0.638174, acc: 0.656250] [adversarial loss: 1.174065, acc: 0.046875]\n",
      "650: [discriminator loss: 0.621080, acc: 0.632812] [adversarial loss: 0.580673, acc: 0.703125]\n",
      "651: [discriminator loss: 0.587661, acc: 0.640625] [adversarial loss: 1.124796, acc: 0.046875]\n",
      "652: [discriminator loss: 0.607186, acc: 0.664062] [adversarial loss: 0.598172, acc: 0.656250]\n",
      "653: [discriminator loss: 0.621653, acc: 0.570312] [adversarial loss: 1.132297, acc: 0.000000]\n",
      "654: [discriminator loss: 0.577293, acc: 0.710938] [adversarial loss: 0.608212, acc: 0.718750]\n",
      "655: [discriminator loss: 0.600318, acc: 0.664062] [adversarial loss: 1.250832, acc: 0.015625]\n",
      "656: [discriminator loss: 0.640718, acc: 0.640625] [adversarial loss: 0.478897, acc: 0.843750]\n",
      "657: [discriminator loss: 0.638563, acc: 0.554688] [adversarial loss: 1.546938, acc: 0.000000]\n",
      "658: [discriminator loss: 0.657750, acc: 0.562500] [adversarial loss: 0.638197, acc: 0.640625]\n",
      "659: [discriminator loss: 0.605571, acc: 0.640625] [adversarial loss: 1.341099, acc: 0.000000]\n",
      "660: [discriminator loss: 0.650885, acc: 0.593750] [adversarial loss: 0.615064, acc: 0.734375]\n",
      "661: [discriminator loss: 0.621546, acc: 0.632812] [adversarial loss: 1.135194, acc: 0.031250]\n",
      "662: [discriminator loss: 0.582658, acc: 0.710938] [adversarial loss: 0.765262, acc: 0.406250]\n",
      "663: [discriminator loss: 0.590721, acc: 0.703125] [adversarial loss: 0.920048, acc: 0.265625]\n",
      "664: [discriminator loss: 0.563167, acc: 0.718750] [adversarial loss: 0.686352, acc: 0.578125]\n",
      "665: [discriminator loss: 0.566278, acc: 0.703125] [adversarial loss: 1.344511, acc: 0.046875]\n",
      "666: [discriminator loss: 0.671593, acc: 0.562500] [adversarial loss: 0.436867, acc: 0.906250]\n",
      "667: [discriminator loss: 0.664185, acc: 0.531250] [adversarial loss: 1.397505, acc: 0.015625]\n",
      "668: [discriminator loss: 0.670828, acc: 0.554688] [adversarial loss: 0.642435, acc: 0.640625]\n",
      "669: [discriminator loss: 0.636331, acc: 0.562500] [adversarial loss: 1.218938, acc: 0.031250]\n",
      "670: [discriminator loss: 0.601604, acc: 0.640625] [adversarial loss: 0.658969, acc: 0.546875]\n",
      "671: [discriminator loss: 0.602527, acc: 0.656250] [adversarial loss: 1.298077, acc: 0.046875]\n",
      "672: [discriminator loss: 0.612336, acc: 0.640625] [adversarial loss: 0.690325, acc: 0.609375]\n",
      "673: [discriminator loss: 0.598478, acc: 0.656250] [adversarial loss: 1.135554, acc: 0.046875]\n",
      "674: [discriminator loss: 0.629261, acc: 0.664062] [adversarial loss: 0.605710, acc: 0.703125]\n",
      "675: [discriminator loss: 0.623467, acc: 0.632812] [adversarial loss: 1.104347, acc: 0.046875]\n",
      "676: [discriminator loss: 0.589467, acc: 0.656250] [adversarial loss: 0.604239, acc: 0.671875]\n",
      "677: [discriminator loss: 0.622970, acc: 0.625000] [adversarial loss: 1.360881, acc: 0.015625]\n",
      "678: [discriminator loss: 0.618612, acc: 0.609375] [adversarial loss: 0.722063, acc: 0.468750]\n",
      "679: [discriminator loss: 0.614962, acc: 0.640625] [adversarial loss: 1.367185, acc: 0.031250]\n",
      "680: [discriminator loss: 0.635021, acc: 0.625000] [adversarial loss: 0.633913, acc: 0.562500]\n",
      "681: [discriminator loss: 0.592694, acc: 0.695312] [adversarial loss: 1.545468, acc: 0.031250]\n",
      "682: [discriminator loss: 0.702729, acc: 0.562500] [adversarial loss: 0.454350, acc: 0.906250]\n",
      "683: [discriminator loss: 0.620699, acc: 0.632812] [adversarial loss: 1.201988, acc: 0.078125]\n",
      "684: [discriminator loss: 0.623803, acc: 0.671875] [adversarial loss: 0.699850, acc: 0.578125]\n",
      "685: [discriminator loss: 0.613421, acc: 0.640625] [adversarial loss: 1.095945, acc: 0.062500]\n",
      "686: [discriminator loss: 0.600585, acc: 0.648438] [adversarial loss: 0.673077, acc: 0.687500]\n",
      "687: [discriminator loss: 0.551001, acc: 0.671875] [adversarial loss: 1.330825, acc: 0.015625]\n",
      "688: [discriminator loss: 0.635725, acc: 0.585938] [adversarial loss: 0.706447, acc: 0.484375]\n",
      "689: [discriminator loss: 0.657296, acc: 0.578125] [adversarial loss: 1.330539, acc: 0.046875]\n",
      "690: [discriminator loss: 0.648257, acc: 0.539062] [adversarial loss: 0.564605, acc: 0.796875]\n",
      "691: [discriminator loss: 0.611369, acc: 0.625000] [adversarial loss: 1.352591, acc: 0.062500]\n",
      "692: [discriminator loss: 0.675541, acc: 0.609375] [adversarial loss: 0.489912, acc: 0.875000]\n",
      "693: [discriminator loss: 0.634503, acc: 0.585938] [adversarial loss: 1.462615, acc: 0.015625]\n",
      "694: [discriminator loss: 0.611944, acc: 0.640625] [adversarial loss: 0.656906, acc: 0.625000]\n",
      "695: [discriminator loss: 0.638779, acc: 0.578125] [adversarial loss: 1.107997, acc: 0.078125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696: [discriminator loss: 0.595864, acc: 0.703125] [adversarial loss: 0.702100, acc: 0.484375]\n",
      "697: [discriminator loss: 0.628143, acc: 0.664062] [adversarial loss: 1.181496, acc: 0.062500]\n",
      "698: [discriminator loss: 0.628382, acc: 0.648438] [adversarial loss: 0.586484, acc: 0.750000]\n",
      "699: [discriminator loss: 0.579772, acc: 0.664062] [adversarial loss: 1.272405, acc: 0.015625]\n",
      "700: [discriminator loss: 0.606274, acc: 0.648438] [adversarial loss: 0.662028, acc: 0.625000]\n",
      "701: [discriminator loss: 0.624447, acc: 0.625000] [adversarial loss: 1.309610, acc: 0.015625]\n",
      "702: [discriminator loss: 0.616635, acc: 0.648438] [adversarial loss: 0.662924, acc: 0.609375]\n",
      "703: [discriminator loss: 0.602608, acc: 0.648438] [adversarial loss: 1.217567, acc: 0.078125]\n",
      "704: [discriminator loss: 0.617403, acc: 0.640625] [adversarial loss: 0.671887, acc: 0.593750]\n",
      "705: [discriminator loss: 0.585635, acc: 0.609375] [adversarial loss: 1.424228, acc: 0.015625]\n",
      "706: [discriminator loss: 0.642026, acc: 0.617188] [adversarial loss: 0.480767, acc: 0.843750]\n",
      "707: [discriminator loss: 0.669208, acc: 0.593750] [adversarial loss: 1.507248, acc: 0.000000]\n",
      "708: [discriminator loss: 0.666715, acc: 0.546875] [adversarial loss: 0.665585, acc: 0.609375]\n",
      "709: [discriminator loss: 0.617105, acc: 0.601562] [adversarial loss: 1.098264, acc: 0.171875]\n",
      "710: [discriminator loss: 0.580807, acc: 0.703125] [adversarial loss: 0.727687, acc: 0.546875]\n",
      "711: [discriminator loss: 0.579130, acc: 0.695312] [adversarial loss: 1.147878, acc: 0.078125]\n",
      "712: [discriminator loss: 0.568338, acc: 0.734375] [adversarial loss: 0.756371, acc: 0.468750]\n",
      "713: [discriminator loss: 0.664341, acc: 0.601562] [adversarial loss: 1.360597, acc: 0.031250]\n",
      "714: [discriminator loss: 0.662685, acc: 0.601562] [adversarial loss: 0.683073, acc: 0.578125]\n",
      "715: [discriminator loss: 0.635640, acc: 0.640625] [adversarial loss: 1.535754, acc: 0.015625]\n",
      "716: [discriminator loss: 0.629259, acc: 0.632812] [adversarial loss: 0.565045, acc: 0.781250]\n",
      "717: [discriminator loss: 0.621265, acc: 0.562500] [adversarial loss: 1.317165, acc: 0.031250]\n",
      "718: [discriminator loss: 0.687988, acc: 0.554688] [adversarial loss: 0.562121, acc: 0.765625]\n",
      "719: [discriminator loss: 0.615287, acc: 0.601562] [adversarial loss: 1.196121, acc: 0.078125]\n",
      "720: [discriminator loss: 0.571511, acc: 0.703125] [adversarial loss: 0.669584, acc: 0.625000]\n",
      "721: [discriminator loss: 0.586358, acc: 0.671875] [adversarial loss: 1.022834, acc: 0.203125]\n",
      "722: [discriminator loss: 0.602914, acc: 0.656250] [adversarial loss: 0.641935, acc: 0.656250]\n",
      "723: [discriminator loss: 0.615485, acc: 0.648438] [adversarial loss: 1.249403, acc: 0.031250]\n",
      "724: [discriminator loss: 0.594923, acc: 0.632812] [adversarial loss: 0.582389, acc: 0.734375]\n",
      "725: [discriminator loss: 0.642888, acc: 0.554688] [adversarial loss: 1.386622, acc: 0.031250]\n",
      "726: [discriminator loss: 0.629802, acc: 0.601562] [adversarial loss: 0.632397, acc: 0.656250]\n",
      "727: [discriminator loss: 0.647316, acc: 0.585938] [adversarial loss: 1.315185, acc: 0.062500]\n",
      "728: [discriminator loss: 0.580530, acc: 0.625000] [adversarial loss: 0.719356, acc: 0.484375]\n",
      "729: [discriminator loss: 0.621074, acc: 0.625000] [adversarial loss: 1.233436, acc: 0.078125]\n",
      "730: [discriminator loss: 0.599607, acc: 0.625000] [adversarial loss: 0.721726, acc: 0.484375]\n",
      "731: [discriminator loss: 0.609746, acc: 0.656250] [adversarial loss: 1.059209, acc: 0.062500]\n",
      "732: [discriminator loss: 0.585811, acc: 0.734375] [adversarial loss: 0.767406, acc: 0.437500]\n",
      "733: [discriminator loss: 0.593445, acc: 0.742188] [adversarial loss: 1.099696, acc: 0.046875]\n",
      "734: [discriminator loss: 0.622241, acc: 0.617188] [adversarial loss: 0.696185, acc: 0.562500]\n",
      "735: [discriminator loss: 0.583174, acc: 0.656250] [adversarial loss: 1.481436, acc: 0.015625]\n",
      "736: [discriminator loss: 0.646741, acc: 0.625000] [adversarial loss: 0.496529, acc: 0.859375]\n",
      "737: [discriminator loss: 0.725102, acc: 0.531250] [adversarial loss: 1.908565, acc: 0.000000]\n",
      "738: [discriminator loss: 0.694588, acc: 0.578125] [adversarial loss: 0.645763, acc: 0.718750]\n",
      "739: [discriminator loss: 0.592931, acc: 0.648438] [adversarial loss: 1.155123, acc: 0.031250]\n",
      "740: [discriminator loss: 0.574991, acc: 0.726562] [adversarial loss: 0.796011, acc: 0.328125]\n",
      "741: [discriminator loss: 0.529890, acc: 0.796875] [adversarial loss: 0.975781, acc: 0.234375]\n",
      "742: [discriminator loss: 0.565626, acc: 0.734375] [adversarial loss: 0.825333, acc: 0.390625]\n",
      "743: [discriminator loss: 0.580780, acc: 0.687500] [adversarial loss: 1.004880, acc: 0.125000]\n",
      "744: [discriminator loss: 0.588961, acc: 0.718750] [adversarial loss: 0.754504, acc: 0.359375]\n",
      "745: [discriminator loss: 0.562201, acc: 0.773438] [adversarial loss: 1.063461, acc: 0.203125]\n",
      "746: [discriminator loss: 0.573400, acc: 0.656250] [adversarial loss: 0.590028, acc: 0.703125]\n",
      "747: [discriminator loss: 0.674524, acc: 0.593750] [adversarial loss: 1.474508, acc: 0.000000]\n",
      "748: [discriminator loss: 0.638394, acc: 0.562500] [adversarial loss: 0.535837, acc: 0.843750]\n",
      "749: [discriminator loss: 0.651612, acc: 0.570312] [adversarial loss: 1.292027, acc: 0.031250]\n",
      "750: [discriminator loss: 0.631906, acc: 0.625000] [adversarial loss: 0.772595, acc: 0.437500]\n",
      "751: [discriminator loss: 0.627764, acc: 0.585938] [adversarial loss: 1.336166, acc: 0.015625]\n",
      "752: [discriminator loss: 0.610465, acc: 0.617188] [adversarial loss: 0.573806, acc: 0.671875]\n",
      "753: [discriminator loss: 0.644087, acc: 0.617188] [adversarial loss: 1.521766, acc: 0.015625]\n",
      "754: [discriminator loss: 0.629837, acc: 0.625000] [adversarial loss: 0.663740, acc: 0.562500]\n",
      "755: [discriminator loss: 0.590976, acc: 0.710938] [adversarial loss: 1.255905, acc: 0.062500]\n",
      "756: [discriminator loss: 0.585076, acc: 0.687500] [adversarial loss: 0.743012, acc: 0.468750]\n",
      "757: [discriminator loss: 0.597810, acc: 0.656250] [adversarial loss: 1.411602, acc: 0.093750]\n",
      "758: [discriminator loss: 0.695261, acc: 0.585938] [adversarial loss: 0.585222, acc: 0.703125]\n",
      "759: [discriminator loss: 0.629614, acc: 0.609375] [adversarial loss: 1.355955, acc: 0.031250]\n",
      "760: [discriminator loss: 0.619123, acc: 0.648438] [adversarial loss: 0.746421, acc: 0.484375]\n",
      "761: [discriminator loss: 0.612322, acc: 0.664062] [adversarial loss: 1.295384, acc: 0.031250]\n",
      "762: [discriminator loss: 0.637198, acc: 0.601562] [adversarial loss: 0.551565, acc: 0.765625]\n",
      "763: [discriminator loss: 0.632950, acc: 0.578125] [adversarial loss: 1.434617, acc: 0.015625]\n",
      "764: [discriminator loss: 0.666978, acc: 0.585938] [adversarial loss: 0.539355, acc: 0.812500]\n",
      "765: [discriminator loss: 0.606520, acc: 0.648438] [adversarial loss: 1.261105, acc: 0.093750]\n",
      "766: [discriminator loss: 0.652408, acc: 0.601562] [adversarial loss: 0.613255, acc: 0.609375]\n",
      "767: [discriminator loss: 0.608066, acc: 0.632812] [adversarial loss: 1.195947, acc: 0.062500]\n",
      "768: [discriminator loss: 0.616910, acc: 0.664062] [adversarial loss: 0.667278, acc: 0.578125]\n",
      "769: [discriminator loss: 0.611124, acc: 0.648438] [adversarial loss: 1.330747, acc: 0.109375]\n",
      "770: [discriminator loss: 0.635756, acc: 0.609375] [adversarial loss: 0.631952, acc: 0.546875]\n",
      "771: [discriminator loss: 0.644555, acc: 0.578125] [adversarial loss: 1.241471, acc: 0.015625]\n",
      "772: [discriminator loss: 0.622321, acc: 0.640625] [adversarial loss: 0.731842, acc: 0.500000]\n",
      "773: [discriminator loss: 0.651479, acc: 0.585938] [adversarial loss: 1.280122, acc: 0.000000]\n",
      "774: [discriminator loss: 0.668479, acc: 0.570312] [adversarial loss: 0.694225, acc: 0.515625]\n",
      "775: [discriminator loss: 0.630584, acc: 0.593750] [adversarial loss: 1.408087, acc: 0.000000]\n",
      "776: [discriminator loss: 0.646505, acc: 0.648438] [adversarial loss: 0.589268, acc: 0.718750]\n",
      "777: [discriminator loss: 0.601026, acc: 0.625000] [adversarial loss: 1.245458, acc: 0.078125]\n",
      "778: [discriminator loss: 0.601973, acc: 0.671875] [adversarial loss: 0.836192, acc: 0.390625]\n",
      "779: [discriminator loss: 0.579943, acc: 0.781250] [adversarial loss: 0.731006, acc: 0.500000]\n",
      "780: [discriminator loss: 0.551980, acc: 0.781250] [adversarial loss: 0.849981, acc: 0.359375]\n",
      "781: [discriminator loss: 0.662064, acc: 0.664062] [adversarial loss: 0.971455, acc: 0.234375]\n",
      "782: [discriminator loss: 0.590995, acc: 0.718750] [adversarial loss: 0.759605, acc: 0.515625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783: [discriminator loss: 0.610609, acc: 0.687500] [adversarial loss: 1.092016, acc: 0.093750]\n",
      "784: [discriminator loss: 0.574660, acc: 0.765625] [adversarial loss: 0.525226, acc: 0.765625]\n",
      "785: [discriminator loss: 0.678431, acc: 0.625000] [adversarial loss: 1.582955, acc: 0.031250]\n",
      "786: [discriminator loss: 0.680688, acc: 0.562500] [adversarial loss: 0.506569, acc: 0.828125]\n",
      "787: [discriminator loss: 0.635031, acc: 0.546875] [adversarial loss: 1.404669, acc: 0.015625]\n",
      "788: [discriminator loss: 0.611308, acc: 0.632812] [adversarial loss: 0.569838, acc: 0.750000]\n",
      "789: [discriminator loss: 0.659628, acc: 0.632812] [adversarial loss: 1.238244, acc: 0.015625]\n",
      "790: [discriminator loss: 0.646594, acc: 0.617188] [adversarial loss: 0.568121, acc: 0.765625]\n",
      "791: [discriminator loss: 0.674642, acc: 0.570312] [adversarial loss: 1.415069, acc: 0.000000]\n",
      "792: [discriminator loss: 0.649874, acc: 0.554688] [adversarial loss: 0.628720, acc: 0.734375]\n",
      "793: [discriminator loss: 0.601737, acc: 0.648438] [adversarial loss: 1.194353, acc: 0.093750]\n",
      "794: [discriminator loss: 0.620114, acc: 0.625000] [adversarial loss: 0.689151, acc: 0.515625]\n",
      "795: [discriminator loss: 0.585516, acc: 0.664062] [adversarial loss: 0.994883, acc: 0.203125]\n",
      "796: [discriminator loss: 0.562118, acc: 0.734375] [adversarial loss: 0.804930, acc: 0.390625]\n",
      "797: [discriminator loss: 0.614779, acc: 0.656250] [adversarial loss: 0.994550, acc: 0.140625]\n",
      "798: [discriminator loss: 0.636458, acc: 0.609375] [adversarial loss: 0.637305, acc: 0.656250]\n",
      "799: [discriminator loss: 0.589732, acc: 0.679688] [adversarial loss: 1.334448, acc: 0.093750]\n",
      "800: [discriminator loss: 0.644377, acc: 0.632812] [adversarial loss: 0.577878, acc: 0.671875]\n",
      "801: [discriminator loss: 0.665650, acc: 0.570312] [adversarial loss: 1.518778, acc: 0.031250]\n",
      "802: [discriminator loss: 0.679433, acc: 0.562500] [adversarial loss: 0.545909, acc: 0.703125]\n",
      "803: [discriminator loss: 0.649800, acc: 0.578125] [adversarial loss: 1.411215, acc: 0.000000]\n",
      "804: [discriminator loss: 0.654662, acc: 0.578125] [adversarial loss: 0.747498, acc: 0.437500]\n",
      "805: [discriminator loss: 0.581847, acc: 0.718750] [adversarial loss: 1.169614, acc: 0.078125]\n",
      "806: [discriminator loss: 0.598323, acc: 0.656250] [adversarial loss: 0.609031, acc: 0.656250]\n",
      "807: [discriminator loss: 0.637271, acc: 0.640625] [adversarial loss: 1.187659, acc: 0.109375]\n",
      "808: [discriminator loss: 0.609964, acc: 0.640625] [adversarial loss: 0.618252, acc: 0.656250]\n",
      "809: [discriminator loss: 0.600753, acc: 0.609375] [adversarial loss: 1.340459, acc: 0.046875]\n",
      "810: [discriminator loss: 0.644003, acc: 0.593750] [adversarial loss: 0.603960, acc: 0.687500]\n",
      "811: [discriminator loss: 0.688362, acc: 0.585938] [adversarial loss: 1.298504, acc: 0.015625]\n",
      "812: [discriminator loss: 0.608582, acc: 0.632812] [adversarial loss: 0.542999, acc: 0.781250]\n",
      "813: [discriminator loss: 0.656977, acc: 0.562500] [adversarial loss: 1.211927, acc: 0.062500]\n",
      "814: [discriminator loss: 0.614456, acc: 0.648438] [adversarial loss: 0.592814, acc: 0.656250]\n",
      "815: [discriminator loss: 0.661692, acc: 0.601562] [adversarial loss: 1.221441, acc: 0.078125]\n",
      "816: [discriminator loss: 0.596659, acc: 0.648438] [adversarial loss: 0.658444, acc: 0.625000]\n",
      "817: [discriminator loss: 0.631860, acc: 0.625000] [adversarial loss: 1.322062, acc: 0.031250]\n",
      "818: [discriminator loss: 0.616342, acc: 0.625000] [adversarial loss: 0.588726, acc: 0.671875]\n",
      "819: [discriminator loss: 0.624874, acc: 0.625000] [adversarial loss: 1.167950, acc: 0.046875]\n",
      "820: [discriminator loss: 0.591283, acc: 0.679688] [adversarial loss: 0.712465, acc: 0.578125]\n",
      "821: [discriminator loss: 0.576994, acc: 0.710938] [adversarial loss: 1.155843, acc: 0.078125]\n",
      "822: [discriminator loss: 0.626099, acc: 0.640625] [adversarial loss: 0.652408, acc: 0.640625]\n",
      "823: [discriminator loss: 0.644722, acc: 0.625000] [adversarial loss: 1.564128, acc: 0.046875]\n",
      "824: [discriminator loss: 0.689614, acc: 0.539062] [adversarial loss: 0.539851, acc: 0.765625]\n",
      "825: [discriminator loss: 0.655741, acc: 0.625000] [adversarial loss: 1.425175, acc: 0.031250]\n",
      "826: [discriminator loss: 0.608830, acc: 0.648438] [adversarial loss: 0.622276, acc: 0.625000]\n",
      "827: [discriminator loss: 0.611602, acc: 0.664062] [adversarial loss: 1.165542, acc: 0.203125]\n",
      "828: [discriminator loss: 0.601399, acc: 0.640625] [adversarial loss: 0.666847, acc: 0.656250]\n",
      "829: [discriminator loss: 0.634188, acc: 0.601562] [adversarial loss: 1.094092, acc: 0.156250]\n",
      "830: [discriminator loss: 0.597943, acc: 0.703125] [adversarial loss: 0.662404, acc: 0.609375]\n",
      "831: [discriminator loss: 0.613537, acc: 0.632812] [adversarial loss: 1.340335, acc: 0.046875]\n",
      "832: [discriminator loss: 0.579803, acc: 0.710938] [adversarial loss: 0.625453, acc: 0.656250]\n",
      "833: [discriminator loss: 0.602011, acc: 0.640625] [adversarial loss: 1.060242, acc: 0.156250]\n",
      "834: [discriminator loss: 0.581332, acc: 0.710938] [adversarial loss: 0.675884, acc: 0.531250]\n",
      "835: [discriminator loss: 0.632573, acc: 0.648438] [adversarial loss: 1.280241, acc: 0.093750]\n",
      "836: [discriminator loss: 0.683380, acc: 0.585938] [adversarial loss: 0.568299, acc: 0.703125]\n",
      "837: [discriminator loss: 0.674190, acc: 0.593750] [adversarial loss: 1.487956, acc: 0.046875]\n",
      "838: [discriminator loss: 0.752720, acc: 0.531250] [adversarial loss: 0.615772, acc: 0.625000]\n",
      "839: [discriminator loss: 0.656233, acc: 0.593750] [adversarial loss: 1.153988, acc: 0.046875]\n",
      "840: [discriminator loss: 0.626655, acc: 0.593750] [adversarial loss: 0.675188, acc: 0.609375]\n",
      "841: [discriminator loss: 0.589685, acc: 0.718750] [adversarial loss: 1.015373, acc: 0.234375]\n",
      "842: [discriminator loss: 0.579782, acc: 0.703125] [adversarial loss: 0.741746, acc: 0.500000]\n",
      "843: [discriminator loss: 0.590015, acc: 0.726562] [adversarial loss: 1.129724, acc: 0.171875]\n",
      "844: [discriminator loss: 0.564570, acc: 0.718750] [adversarial loss: 0.682990, acc: 0.593750]\n",
      "845: [discriminator loss: 0.589160, acc: 0.703125] [adversarial loss: 1.021728, acc: 0.093750]\n",
      "846: [discriminator loss: 0.586831, acc: 0.710938] [adversarial loss: 0.739761, acc: 0.578125]\n",
      "847: [discriminator loss: 0.588374, acc: 0.671875] [adversarial loss: 1.043079, acc: 0.156250]\n",
      "848: [discriminator loss: 0.629499, acc: 0.617188] [adversarial loss: 0.603849, acc: 0.671875]\n",
      "849: [discriminator loss: 0.622957, acc: 0.593750] [adversarial loss: 1.658737, acc: 0.015625]\n",
      "850: [discriminator loss: 0.786255, acc: 0.507812] [adversarial loss: 0.602498, acc: 0.734375]\n",
      "851: [discriminator loss: 0.654271, acc: 0.609375] [adversarial loss: 1.306434, acc: 0.000000]\n",
      "852: [discriminator loss: 0.632266, acc: 0.632812] [adversarial loss: 0.615908, acc: 0.609375]\n",
      "853: [discriminator loss: 0.624386, acc: 0.625000] [adversarial loss: 1.268678, acc: 0.078125]\n",
      "854: [discriminator loss: 0.644455, acc: 0.585938] [adversarial loss: 0.830330, acc: 0.437500]\n",
      "855: [discriminator loss: 0.600370, acc: 0.679688] [adversarial loss: 1.034309, acc: 0.125000]\n",
      "856: [discriminator loss: 0.559894, acc: 0.703125] [adversarial loss: 0.869289, acc: 0.250000]\n",
      "857: [discriminator loss: 0.598408, acc: 0.734375] [adversarial loss: 0.917101, acc: 0.203125]\n",
      "858: [discriminator loss: 0.591591, acc: 0.734375] [adversarial loss: 0.760179, acc: 0.500000]\n",
      "859: [discriminator loss: 0.615326, acc: 0.679688] [adversarial loss: 1.198721, acc: 0.109375]\n",
      "860: [discriminator loss: 0.625713, acc: 0.656250] [adversarial loss: 0.640676, acc: 0.562500]\n",
      "861: [discriminator loss: 0.644374, acc: 0.632812] [adversarial loss: 1.393407, acc: 0.015625]\n",
      "862: [discriminator loss: 0.647835, acc: 0.601562] [adversarial loss: 0.543242, acc: 0.765625]\n",
      "863: [discriminator loss: 0.653138, acc: 0.617188] [adversarial loss: 1.289206, acc: 0.062500]\n",
      "864: [discriminator loss: 0.630470, acc: 0.593750] [adversarial loss: 0.601662, acc: 0.703125]\n",
      "865: [discriminator loss: 0.655560, acc: 0.554688] [adversarial loss: 1.246394, acc: 0.078125]\n",
      "866: [discriminator loss: 0.625613, acc: 0.625000] [adversarial loss: 0.689023, acc: 0.531250]\n",
      "867: [discriminator loss: 0.587913, acc: 0.671875] [adversarial loss: 1.207495, acc: 0.171875]\n",
      "868: [discriminator loss: 0.636424, acc: 0.601562] [adversarial loss: 0.667897, acc: 0.593750]\n",
      "869: [discriminator loss: 0.617057, acc: 0.640625] [adversarial loss: 1.236625, acc: 0.109375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "870: [discriminator loss: 0.600375, acc: 0.609375] [adversarial loss: 0.746213, acc: 0.562500]\n",
      "871: [discriminator loss: 0.600219, acc: 0.632812] [adversarial loss: 1.245679, acc: 0.062500]\n",
      "872: [discriminator loss: 0.638031, acc: 0.656250] [adversarial loss: 0.728080, acc: 0.468750]\n",
      "873: [discriminator loss: 0.596547, acc: 0.656250] [adversarial loss: 0.959030, acc: 0.203125]\n",
      "874: [discriminator loss: 0.572341, acc: 0.703125] [adversarial loss: 0.712547, acc: 0.484375]\n",
      "875: [discriminator loss: 0.562000, acc: 0.695312] [adversarial loss: 1.109985, acc: 0.156250]\n",
      "876: [discriminator loss: 0.597403, acc: 0.703125] [adversarial loss: 0.720955, acc: 0.546875]\n",
      "877: [discriminator loss: 0.587662, acc: 0.679688] [adversarial loss: 1.197391, acc: 0.046875]\n",
      "878: [discriminator loss: 0.627333, acc: 0.640625] [adversarial loss: 0.620023, acc: 0.687500]\n",
      "879: [discriminator loss: 0.621225, acc: 0.632812] [adversarial loss: 1.594458, acc: 0.000000]\n",
      "880: [discriminator loss: 0.733688, acc: 0.539062] [adversarial loss: 0.488225, acc: 0.828125]\n",
      "881: [discriminator loss: 0.719055, acc: 0.585938] [adversarial loss: 1.458559, acc: 0.046875]\n",
      "882: [discriminator loss: 0.675242, acc: 0.570312] [adversarial loss: 0.647172, acc: 0.578125]\n",
      "883: [discriminator loss: 0.656981, acc: 0.546875] [adversarial loss: 1.120728, acc: 0.062500]\n",
      "884: [discriminator loss: 0.596994, acc: 0.640625] [adversarial loss: 0.727832, acc: 0.515625]\n",
      "885: [discriminator loss: 0.630239, acc: 0.632812] [adversarial loss: 1.354587, acc: 0.078125]\n",
      "886: [discriminator loss: 0.663322, acc: 0.617188] [adversarial loss: 0.740062, acc: 0.437500]\n",
      "887: [discriminator loss: 0.611687, acc: 0.640625] [adversarial loss: 1.319264, acc: 0.078125]\n",
      "888: [discriminator loss: 0.603852, acc: 0.679688] [adversarial loss: 0.664957, acc: 0.578125]\n",
      "889: [discriminator loss: 0.636955, acc: 0.656250] [adversarial loss: 1.116728, acc: 0.093750]\n",
      "890: [discriminator loss: 0.614608, acc: 0.664062] [adversarial loss: 0.547846, acc: 0.765625]\n",
      "891: [discriminator loss: 0.624626, acc: 0.585938] [adversarial loss: 1.297947, acc: 0.031250]\n",
      "892: [discriminator loss: 0.657736, acc: 0.578125] [adversarial loss: 0.690919, acc: 0.515625]\n",
      "893: [discriminator loss: 0.671775, acc: 0.609375] [adversarial loss: 1.357705, acc: 0.031250]\n",
      "894: [discriminator loss: 0.648154, acc: 0.617188] [adversarial loss: 0.639190, acc: 0.625000]\n",
      "895: [discriminator loss: 0.592407, acc: 0.687500] [adversarial loss: 1.067755, acc: 0.171875]\n",
      "896: [discriminator loss: 0.589323, acc: 0.679688] [adversarial loss: 0.673631, acc: 0.500000]\n",
      "897: [discriminator loss: 0.617847, acc: 0.671875] [adversarial loss: 1.130790, acc: 0.078125]\n",
      "898: [discriminator loss: 0.596273, acc: 0.640625] [adversarial loss: 0.618813, acc: 0.609375]\n",
      "899: [discriminator loss: 0.620908, acc: 0.671875] [adversarial loss: 1.252977, acc: 0.156250]\n",
      "900: [discriminator loss: 0.591064, acc: 0.671875] [adversarial loss: 0.676721, acc: 0.546875]\n",
      "901: [discriminator loss: 0.621067, acc: 0.632812] [adversarial loss: 1.070060, acc: 0.156250]\n",
      "902: [discriminator loss: 0.615325, acc: 0.664062] [adversarial loss: 0.800476, acc: 0.468750]\n",
      "903: [discriminator loss: 0.574634, acc: 0.765625] [adversarial loss: 1.027950, acc: 0.156250]\n",
      "904: [discriminator loss: 0.581844, acc: 0.695312] [adversarial loss: 0.862251, acc: 0.390625]\n",
      "905: [discriminator loss: 0.582387, acc: 0.671875] [adversarial loss: 1.107076, acc: 0.125000]\n",
      "906: [discriminator loss: 0.578570, acc: 0.773438] [adversarial loss: 0.716582, acc: 0.546875]\n",
      "907: [discriminator loss: 0.632896, acc: 0.632812] [adversarial loss: 1.388195, acc: 0.015625]\n",
      "908: [discriminator loss: 0.650096, acc: 0.593750] [adversarial loss: 0.573177, acc: 0.703125]\n",
      "909: [discriminator loss: 0.631480, acc: 0.632812] [adversarial loss: 1.520232, acc: 0.000000]\n",
      "910: [discriminator loss: 0.707253, acc: 0.546875] [adversarial loss: 0.481102, acc: 0.906250]\n",
      "911: [discriminator loss: 0.672536, acc: 0.585938] [adversarial loss: 1.379430, acc: 0.031250]\n",
      "912: [discriminator loss: 0.633775, acc: 0.609375] [adversarial loss: 0.849500, acc: 0.390625]\n",
      "913: [discriminator loss: 0.596816, acc: 0.679688] [adversarial loss: 1.046573, acc: 0.234375]\n",
      "914: [discriminator loss: 0.620655, acc: 0.664062] [adversarial loss: 0.674128, acc: 0.625000]\n",
      "915: [discriminator loss: 0.640728, acc: 0.585938] [adversarial loss: 1.266538, acc: 0.078125]\n",
      "916: [discriminator loss: 0.594205, acc: 0.664062] [adversarial loss: 0.651641, acc: 0.609375]\n",
      "917: [discriminator loss: 0.625411, acc: 0.640625] [adversarial loss: 1.118122, acc: 0.078125]\n",
      "918: [discriminator loss: 0.582933, acc: 0.710938] [adversarial loss: 0.724953, acc: 0.546875]\n",
      "919: [discriminator loss: 0.607800, acc: 0.671875] [adversarial loss: 1.141781, acc: 0.140625]\n",
      "920: [discriminator loss: 0.658558, acc: 0.585938] [adversarial loss: 0.698841, acc: 0.468750]\n",
      "921: [discriminator loss: 0.613978, acc: 0.648438] [adversarial loss: 1.341247, acc: 0.015625]\n",
      "922: [discriminator loss: 0.597436, acc: 0.648438] [adversarial loss: 0.634941, acc: 0.625000]\n",
      "923: [discriminator loss: 0.597071, acc: 0.687500] [adversarial loss: 1.194274, acc: 0.062500]\n",
      "924: [discriminator loss: 0.620939, acc: 0.640625] [adversarial loss: 0.733361, acc: 0.468750]\n",
      "925: [discriminator loss: 0.610211, acc: 0.640625] [adversarial loss: 1.249637, acc: 0.062500]\n",
      "926: [discriminator loss: 0.621545, acc: 0.593750] [adversarial loss: 0.573117, acc: 0.703125]\n",
      "927: [discriminator loss: 0.624164, acc: 0.640625] [adversarial loss: 1.298474, acc: 0.015625]\n",
      "928: [discriminator loss: 0.636054, acc: 0.617188] [adversarial loss: 0.579994, acc: 0.765625]\n",
      "929: [discriminator loss: 0.648075, acc: 0.601562] [adversarial loss: 1.216732, acc: 0.062500]\n",
      "930: [discriminator loss: 0.626741, acc: 0.640625] [adversarial loss: 0.653344, acc: 0.593750]\n",
      "931: [discriminator loss: 0.596039, acc: 0.664062] [adversarial loss: 1.256925, acc: 0.046875]\n",
      "932: [discriminator loss: 0.646535, acc: 0.625000] [adversarial loss: 0.685124, acc: 0.515625]\n",
      "933: [discriminator loss: 0.625990, acc: 0.609375] [adversarial loss: 1.278986, acc: 0.109375]\n",
      "934: [discriminator loss: 0.601280, acc: 0.656250] [adversarial loss: 0.834590, acc: 0.328125]\n",
      "935: [discriminator loss: 0.587291, acc: 0.703125] [adversarial loss: 1.408653, acc: 0.062500]\n",
      "936: [discriminator loss: 0.674427, acc: 0.609375] [adversarial loss: 0.552906, acc: 0.765625]\n",
      "937: [discriminator loss: 0.648045, acc: 0.601562] [adversarial loss: 1.400060, acc: 0.000000]\n",
      "938: [discriminator loss: 0.640406, acc: 0.609375] [adversarial loss: 0.698579, acc: 0.578125]\n",
      "939: [discriminator loss: 0.656257, acc: 0.617188] [adversarial loss: 1.230814, acc: 0.125000]\n",
      "940: [discriminator loss: 0.605003, acc: 0.664062] [adversarial loss: 0.701182, acc: 0.546875]\n",
      "941: [discriminator loss: 0.611592, acc: 0.687500] [adversarial loss: 1.140877, acc: 0.109375]\n",
      "942: [discriminator loss: 0.587636, acc: 0.679688] [adversarial loss: 0.774611, acc: 0.453125]\n",
      "943: [discriminator loss: 0.627608, acc: 0.656250] [adversarial loss: 1.155810, acc: 0.062500]\n",
      "944: [discriminator loss: 0.624173, acc: 0.632812] [adversarial loss: 0.697330, acc: 0.609375]\n",
      "945: [discriminator loss: 0.610190, acc: 0.664062] [adversarial loss: 1.338528, acc: 0.046875]\n",
      "946: [discriminator loss: 0.608666, acc: 0.632812] [adversarial loss: 0.577053, acc: 0.734375]\n",
      "947: [discriminator loss: 0.620434, acc: 0.601562] [adversarial loss: 1.298737, acc: 0.093750]\n",
      "948: [discriminator loss: 0.628437, acc: 0.632812] [adversarial loss: 0.595934, acc: 0.718750]\n",
      "949: [discriminator loss: 0.612016, acc: 0.617188] [adversarial loss: 1.272761, acc: 0.046875]\n",
      "950: [discriminator loss: 0.609768, acc: 0.632812] [adversarial loss: 0.723706, acc: 0.515625]\n",
      "951: [discriminator loss: 0.607749, acc: 0.648438] [adversarial loss: 1.138539, acc: 0.140625]\n",
      "952: [discriminator loss: 0.567251, acc: 0.687500] [adversarial loss: 0.862043, acc: 0.296875]\n",
      "953: [discriminator loss: 0.583619, acc: 0.679688] [adversarial loss: 0.894371, acc: 0.359375]\n",
      "954: [discriminator loss: 0.611125, acc: 0.656250] [adversarial loss: 1.071841, acc: 0.093750]\n",
      "955: [discriminator loss: 0.612950, acc: 0.640625] [adversarial loss: 0.483338, acc: 0.812500]\n",
      "956: [discriminator loss: 0.691366, acc: 0.562500] [adversarial loss: 1.602384, acc: 0.046875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "957: [discriminator loss: 0.739243, acc: 0.507812] [adversarial loss: 0.499383, acc: 0.843750]\n",
      "958: [discriminator loss: 0.642574, acc: 0.625000] [adversarial loss: 1.210492, acc: 0.109375]\n",
      "959: [discriminator loss: 0.620452, acc: 0.664062] [adversarial loss: 0.678369, acc: 0.578125]\n",
      "960: [discriminator loss: 0.629706, acc: 0.617188] [adversarial loss: 1.252714, acc: 0.062500]\n",
      "961: [discriminator loss: 0.584070, acc: 0.671875] [adversarial loss: 0.674226, acc: 0.562500]\n",
      "962: [discriminator loss: 0.673715, acc: 0.585938] [adversarial loss: 1.127228, acc: 0.125000]\n",
      "963: [discriminator loss: 0.675192, acc: 0.609375] [adversarial loss: 0.871054, acc: 0.359375]\n",
      "964: [discriminator loss: 0.560284, acc: 0.757812] [adversarial loss: 1.031655, acc: 0.156250]\n",
      "965: [discriminator loss: 0.577681, acc: 0.734375] [adversarial loss: 0.862916, acc: 0.343750]\n",
      "966: [discriminator loss: 0.541509, acc: 0.750000] [adversarial loss: 1.008661, acc: 0.203125]\n",
      "967: [discriminator loss: 0.596593, acc: 0.687500] [adversarial loss: 0.667347, acc: 0.593750]\n",
      "968: [discriminator loss: 0.613329, acc: 0.640625] [adversarial loss: 1.341026, acc: 0.015625]\n",
      "969: [discriminator loss: 0.660467, acc: 0.601562] [adversarial loss: 0.519363, acc: 0.734375]\n",
      "970: [discriminator loss: 0.692490, acc: 0.570312] [adversarial loss: 1.512067, acc: 0.062500]\n",
      "971: [discriminator loss: 0.701970, acc: 0.593750] [adversarial loss: 0.650712, acc: 0.562500]\n",
      "972: [discriminator loss: 0.635236, acc: 0.648438] [adversarial loss: 1.290743, acc: 0.078125]\n",
      "973: [discriminator loss: 0.597099, acc: 0.703125] [adversarial loss: 0.645615, acc: 0.625000]\n",
      "974: [discriminator loss: 0.593735, acc: 0.656250] [adversarial loss: 1.100119, acc: 0.140625]\n",
      "975: [discriminator loss: 0.607917, acc: 0.703125] [adversarial loss: 0.740835, acc: 0.531250]\n",
      "976: [discriminator loss: 0.580661, acc: 0.726562] [adversarial loss: 1.175897, acc: 0.093750]\n",
      "977: [discriminator loss: 0.634416, acc: 0.601562] [adversarial loss: 0.498962, acc: 0.843750]\n",
      "978: [discriminator loss: 0.673791, acc: 0.578125] [adversarial loss: 1.534499, acc: 0.078125]\n",
      "979: [discriminator loss: 0.690689, acc: 0.617188] [adversarial loss: 0.594350, acc: 0.687500]\n",
      "980: [discriminator loss: 0.621343, acc: 0.625000] [adversarial loss: 1.195378, acc: 0.093750]\n",
      "981: [discriminator loss: 0.616788, acc: 0.695312] [adversarial loss: 0.816256, acc: 0.453125]\n",
      "982: [discriminator loss: 0.602007, acc: 0.703125] [adversarial loss: 0.943346, acc: 0.250000]\n",
      "983: [discriminator loss: 0.599330, acc: 0.687500] [adversarial loss: 0.894232, acc: 0.281250]\n",
      "984: [discriminator loss: 0.565751, acc: 0.710938] [adversarial loss: 1.029924, acc: 0.281250]\n",
      "985: [discriminator loss: 0.643428, acc: 0.601562] [adversarial loss: 0.990295, acc: 0.187500]\n",
      "986: [discriminator loss: 0.627395, acc: 0.640625] [adversarial loss: 0.762187, acc: 0.406250]\n",
      "987: [discriminator loss: 0.621801, acc: 0.671875] [adversarial loss: 0.995717, acc: 0.171875]\n",
      "988: [discriminator loss: 0.590688, acc: 0.671875] [adversarial loss: 0.782971, acc: 0.437500]\n",
      "989: [discriminator loss: 0.586188, acc: 0.695312] [adversarial loss: 1.333720, acc: 0.093750]\n",
      "990: [discriminator loss: 0.656590, acc: 0.585938] [adversarial loss: 0.592516, acc: 0.734375]\n",
      "991: [discriminator loss: 0.689318, acc: 0.546875] [adversarial loss: 1.594889, acc: 0.046875]\n",
      "992: [discriminator loss: 0.764899, acc: 0.531250] [adversarial loss: 0.607293, acc: 0.671875]\n",
      "993: [discriminator loss: 0.684484, acc: 0.523438] [adversarial loss: 1.211492, acc: 0.078125]\n",
      "994: [discriminator loss: 0.608170, acc: 0.656250] [adversarial loss: 0.720704, acc: 0.562500]\n",
      "995: [discriminator loss: 0.596719, acc: 0.671875] [adversarial loss: 1.061655, acc: 0.156250]\n",
      "996: [discriminator loss: 0.569222, acc: 0.718750] [adversarial loss: 0.714704, acc: 0.515625]\n",
      "997: [discriminator loss: 0.636186, acc: 0.632812] [adversarial loss: 1.340699, acc: 0.109375]\n",
      "998: [discriminator loss: 0.690481, acc: 0.632812] [adversarial loss: 0.609382, acc: 0.625000]\n",
      "999: [discriminator loss: 0.648656, acc: 0.632812] [adversarial loss: 1.140935, acc: 0.046875]\n",
      "cgan_mnist  labels for generated images:  [0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5]\n",
      "1000: [discriminator loss: 0.641692, acc: 0.601562] [adversarial loss: 0.699725, acc: 0.546875]\n",
      "1001: [discriminator loss: 0.614745, acc: 0.679688] [adversarial loss: 1.059851, acc: 0.125000]\n",
      "1002: [discriminator loss: 0.612904, acc: 0.648438] [adversarial loss: 0.795269, acc: 0.390625]\n",
      "1003: [discriminator loss: 0.581823, acc: 0.710938] [adversarial loss: 1.000736, acc: 0.218750]\n",
      "1004: [discriminator loss: 0.590216, acc: 0.664062] [adversarial loss: 0.832377, acc: 0.312500]\n",
      "1005: [discriminator loss: 0.629582, acc: 0.671875] [adversarial loss: 0.995039, acc: 0.234375]\n",
      "1006: [discriminator loss: 0.600768, acc: 0.687500] [adversarial loss: 0.885006, acc: 0.296875]\n",
      "1007: [discriminator loss: 0.608177, acc: 0.679688] [adversarial loss: 0.922750, acc: 0.218750]\n",
      "1008: [discriminator loss: 0.599519, acc: 0.671875] [adversarial loss: 0.826607, acc: 0.328125]\n",
      "1009: [discriminator loss: 0.574013, acc: 0.773438] [adversarial loss: 0.951459, acc: 0.234375]\n",
      "1010: [discriminator loss: 0.594796, acc: 0.750000] [adversarial loss: 0.773249, acc: 0.468750]\n",
      "1011: [discriminator loss: 0.577376, acc: 0.703125] [adversarial loss: 1.368174, acc: 0.062500]\n",
      "1012: [discriminator loss: 0.569767, acc: 0.664062] [adversarial loss: 0.708734, acc: 0.562500]\n",
      "1013: [discriminator loss: 0.645328, acc: 0.656250] [adversarial loss: 1.707606, acc: 0.015625]\n",
      "1014: [discriminator loss: 0.786361, acc: 0.554688] [adversarial loss: 0.437711, acc: 0.843750]\n",
      "1015: [discriminator loss: 0.757109, acc: 0.570312] [adversarial loss: 1.333857, acc: 0.109375]\n",
      "1016: [discriminator loss: 0.739030, acc: 0.562500] [adversarial loss: 0.617343, acc: 0.640625]\n",
      "1017: [discriminator loss: 0.629601, acc: 0.632812] [adversarial loss: 1.058547, acc: 0.125000]\n",
      "1018: [discriminator loss: 0.602838, acc: 0.695312] [adversarial loss: 0.833946, acc: 0.312500]\n",
      "1019: [discriminator loss: 0.631201, acc: 0.640625] [adversarial loss: 0.876206, acc: 0.281250]\n",
      "1020: [discriminator loss: 0.620357, acc: 0.648438] [adversarial loss: 0.948610, acc: 0.250000]\n",
      "1021: [discriminator loss: 0.623252, acc: 0.695312] [adversarial loss: 0.884847, acc: 0.312500]\n",
      "1022: [discriminator loss: 0.587959, acc: 0.726562] [adversarial loss: 0.899419, acc: 0.359375]\n",
      "1023: [discriminator loss: 0.610202, acc: 0.640625] [adversarial loss: 0.928558, acc: 0.265625]\n",
      "1024: [discriminator loss: 0.584578, acc: 0.695312] [adversarial loss: 1.079731, acc: 0.234375]\n",
      "1025: [discriminator loss: 0.620671, acc: 0.671875] [adversarial loss: 0.729916, acc: 0.500000]\n",
      "1026: [discriminator loss: 0.625733, acc: 0.671875] [adversarial loss: 1.368241, acc: 0.046875]\n",
      "1027: [discriminator loss: 0.605533, acc: 0.648438] [adversarial loss: 0.636421, acc: 0.656250]\n",
      "1028: [discriminator loss: 0.652098, acc: 0.593750] [adversarial loss: 1.485397, acc: 0.046875]\n",
      "1029: [discriminator loss: 0.706283, acc: 0.609375] [adversarial loss: 0.516617, acc: 0.859375]\n",
      "1030: [discriminator loss: 0.674424, acc: 0.609375] [adversarial loss: 1.257794, acc: 0.015625]\n",
      "1031: [discriminator loss: 0.623821, acc: 0.632812] [adversarial loss: 0.658799, acc: 0.609375]\n",
      "1032: [discriminator loss: 0.691700, acc: 0.570312] [adversarial loss: 1.320380, acc: 0.093750]\n",
      "1033: [discriminator loss: 0.691942, acc: 0.570312] [adversarial loss: 0.600696, acc: 0.718750]\n",
      "1034: [discriminator loss: 0.666200, acc: 0.609375] [adversarial loss: 1.255836, acc: 0.046875]\n",
      "1035: [discriminator loss: 0.669605, acc: 0.539062] [adversarial loss: 0.683515, acc: 0.578125]\n",
      "1036: [discriminator loss: 0.637657, acc: 0.640625] [adversarial loss: 0.963957, acc: 0.171875]\n",
      "1037: [discriminator loss: 0.659460, acc: 0.562500] [adversarial loss: 0.909924, acc: 0.250000]\n",
      "1038: [discriminator loss: 0.596099, acc: 0.664062] [adversarial loss: 0.970728, acc: 0.187500]\n",
      "1039: [discriminator loss: 0.651153, acc: 0.578125] [adversarial loss: 0.855578, acc: 0.343750]\n",
      "1040: [discriminator loss: 0.657611, acc: 0.617188] [adversarial loss: 1.002980, acc: 0.234375]\n",
      "1041: [discriminator loss: 0.581405, acc: 0.726562] [adversarial loss: 0.733523, acc: 0.453125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1042: [discriminator loss: 0.631306, acc: 0.632812] [adversarial loss: 1.101032, acc: 0.265625]\n",
      "1043: [discriminator loss: 0.641372, acc: 0.632812] [adversarial loss: 0.655983, acc: 0.609375]\n",
      "1044: [discriminator loss: 0.633323, acc: 0.609375] [adversarial loss: 1.185338, acc: 0.078125]\n",
      "1045: [discriminator loss: 0.662469, acc: 0.562500] [adversarial loss: 0.573548, acc: 0.718750]\n",
      "1046: [discriminator loss: 0.675925, acc: 0.570312] [adversarial loss: 1.404621, acc: 0.078125]\n",
      "1047: [discriminator loss: 0.668987, acc: 0.632812] [adversarial loss: 0.606909, acc: 0.609375]\n",
      "1048: [discriminator loss: 0.690109, acc: 0.523438] [adversarial loss: 1.158571, acc: 0.078125]\n",
      "1049: [discriminator loss: 0.674228, acc: 0.617188] [adversarial loss: 0.667036, acc: 0.593750]\n",
      "1050: [discriminator loss: 0.605461, acc: 0.687500] [adversarial loss: 1.122470, acc: 0.109375]\n",
      "1051: [discriminator loss: 0.640689, acc: 0.632812] [adversarial loss: 0.682128, acc: 0.546875]\n",
      "1052: [discriminator loss: 0.621409, acc: 0.593750] [adversarial loss: 1.246800, acc: 0.109375]\n",
      "1053: [discriminator loss: 0.637453, acc: 0.656250] [adversarial loss: 0.584082, acc: 0.687500]\n",
      "1054: [discriminator loss: 0.643329, acc: 0.648438] [adversarial loss: 1.362136, acc: 0.031250]\n",
      "1055: [discriminator loss: 0.710020, acc: 0.562500] [adversarial loss: 0.642705, acc: 0.593750]\n",
      "1056: [discriminator loss: 0.677266, acc: 0.554688] [adversarial loss: 1.312878, acc: 0.046875]\n",
      "1057: [discriminator loss: 0.673751, acc: 0.531250] [adversarial loss: 0.616916, acc: 0.687500]\n",
      "1058: [discriminator loss: 0.641490, acc: 0.609375] [adversarial loss: 1.222271, acc: 0.078125]\n",
      "1059: [discriminator loss: 0.617073, acc: 0.671875] [adversarial loss: 0.741925, acc: 0.468750]\n",
      "1060: [discriminator loss: 0.642076, acc: 0.648438] [adversarial loss: 1.125919, acc: 0.062500]\n",
      "1061: [discriminator loss: 0.605134, acc: 0.648438] [adversarial loss: 0.650235, acc: 0.546875]\n",
      "1062: [discriminator loss: 0.604497, acc: 0.585938] [adversarial loss: 1.288664, acc: 0.078125]\n",
      "1063: [discriminator loss: 0.701067, acc: 0.593750] [adversarial loss: 0.614151, acc: 0.703125]\n",
      "1064: [discriminator loss: 0.641314, acc: 0.632812] [adversarial loss: 1.074531, acc: 0.109375]\n",
      "1065: [discriminator loss: 0.632260, acc: 0.656250] [adversarial loss: 0.730349, acc: 0.500000]\n",
      "1066: [discriminator loss: 0.717025, acc: 0.578125] [adversarial loss: 1.124482, acc: 0.125000]\n",
      "1067: [discriminator loss: 0.690570, acc: 0.593750] [adversarial loss: 0.707611, acc: 0.515625]\n",
      "1068: [discriminator loss: 0.579711, acc: 0.695312] [adversarial loss: 1.022343, acc: 0.218750]\n",
      "1069: [discriminator loss: 0.653233, acc: 0.617188] [adversarial loss: 0.829866, acc: 0.343750]\n",
      "1070: [discriminator loss: 0.636999, acc: 0.632812] [adversarial loss: 1.027537, acc: 0.156250]\n",
      "1071: [discriminator loss: 0.631239, acc: 0.648438] [adversarial loss: 0.875980, acc: 0.343750]\n",
      "1072: [discriminator loss: 0.648407, acc: 0.679688] [adversarial loss: 1.009827, acc: 0.109375]\n",
      "1073: [discriminator loss: 0.599059, acc: 0.625000] [adversarial loss: 0.809225, acc: 0.437500]\n",
      "1074: [discriminator loss: 0.651956, acc: 0.640625] [adversarial loss: 0.719114, acc: 0.484375]\n",
      "1075: [discriminator loss: 0.702794, acc: 0.515625] [adversarial loss: 1.099592, acc: 0.156250]\n",
      "1076: [discriminator loss: 0.657272, acc: 0.625000] [adversarial loss: 0.508214, acc: 0.828125]\n",
      "1077: [discriminator loss: 0.688886, acc: 0.562500] [adversarial loss: 1.314397, acc: 0.078125]\n",
      "1078: [discriminator loss: 0.699324, acc: 0.562500] [adversarial loss: 0.544655, acc: 0.781250]\n",
      "1079: [discriminator loss: 0.729690, acc: 0.492188] [adversarial loss: 1.101086, acc: 0.109375]\n",
      "1080: [discriminator loss: 0.638225, acc: 0.578125] [adversarial loss: 0.552838, acc: 0.734375]\n",
      "1081: [discriminator loss: 0.682494, acc: 0.593750] [adversarial loss: 1.195046, acc: 0.109375]\n",
      "1082: [discriminator loss: 0.686405, acc: 0.562500] [adversarial loss: 0.646136, acc: 0.562500]\n",
      "1083: [discriminator loss: 0.652997, acc: 0.562500] [adversarial loss: 1.264436, acc: 0.046875]\n",
      "1084: [discriminator loss: 0.695265, acc: 0.539062] [adversarial loss: 0.613994, acc: 0.687500]\n",
      "1085: [discriminator loss: 0.647382, acc: 0.679688] [adversarial loss: 1.201720, acc: 0.078125]\n",
      "1086: [discriminator loss: 0.638277, acc: 0.671875] [adversarial loss: 0.737240, acc: 0.421875]\n",
      "1087: [discriminator loss: 0.617600, acc: 0.710938] [adversarial loss: 0.853932, acc: 0.265625]\n",
      "1088: [discriminator loss: 0.644212, acc: 0.664062] [adversarial loss: 0.843698, acc: 0.375000]\n",
      "1089: [discriminator loss: 0.638363, acc: 0.664062] [adversarial loss: 0.718308, acc: 0.453125]\n",
      "1090: [discriminator loss: 0.617982, acc: 0.648438] [adversarial loss: 1.012786, acc: 0.187500]\n",
      "1091: [discriminator loss: 0.650843, acc: 0.593750] [adversarial loss: 0.735615, acc: 0.468750]\n",
      "1092: [discriminator loss: 0.627182, acc: 0.664062] [adversarial loss: 0.906090, acc: 0.265625]\n",
      "1093: [discriminator loss: 0.627682, acc: 0.656250] [adversarial loss: 0.930885, acc: 0.218750]\n",
      "1094: [discriminator loss: 0.626078, acc: 0.664062] [adversarial loss: 0.806275, acc: 0.359375]\n",
      "1095: [discriminator loss: 0.643871, acc: 0.656250] [adversarial loss: 1.338194, acc: 0.078125]\n",
      "1096: [discriminator loss: 0.708112, acc: 0.601562] [adversarial loss: 0.522487, acc: 0.796875]\n",
      "1097: [discriminator loss: 0.703481, acc: 0.523438] [adversarial loss: 1.367740, acc: 0.000000]\n",
      "1098: [discriminator loss: 0.759664, acc: 0.570312] [adversarial loss: 0.653252, acc: 0.578125]\n",
      "1099: [discriminator loss: 0.670454, acc: 0.578125] [adversarial loss: 1.131840, acc: 0.187500]\n",
      "1100: [discriminator loss: 0.748789, acc: 0.531250] [adversarial loss: 0.598924, acc: 0.703125]\n",
      "1101: [discriminator loss: 0.645517, acc: 0.578125] [adversarial loss: 1.006836, acc: 0.156250]\n",
      "1102: [discriminator loss: 0.663170, acc: 0.640625] [adversarial loss: 0.629966, acc: 0.640625]\n",
      "1103: [discriminator loss: 0.642369, acc: 0.609375] [adversarial loss: 1.015990, acc: 0.140625]\n",
      "1104: [discriminator loss: 0.717561, acc: 0.476562] [adversarial loss: 0.736549, acc: 0.468750]\n",
      "1105: [discriminator loss: 0.619870, acc: 0.656250] [adversarial loss: 0.871857, acc: 0.312500]\n",
      "1106: [discriminator loss: 0.658151, acc: 0.609375] [adversarial loss: 0.848422, acc: 0.234375]\n",
      "1107: [discriminator loss: 0.617348, acc: 0.695312] [adversarial loss: 0.874189, acc: 0.281250]\n",
      "1108: [discriminator loss: 0.581240, acc: 0.710938] [adversarial loss: 0.915020, acc: 0.296875]\n",
      "1109: [discriminator loss: 0.658072, acc: 0.617188] [adversarial loss: 0.774800, acc: 0.406250]\n",
      "1110: [discriminator loss: 0.566721, acc: 0.750000] [adversarial loss: 0.925082, acc: 0.234375]\n",
      "1111: [discriminator loss: 0.629012, acc: 0.617188] [adversarial loss: 0.730990, acc: 0.484375]\n",
      "1112: [discriminator loss: 0.597619, acc: 0.671875] [adversarial loss: 0.947541, acc: 0.171875]\n",
      "1113: [discriminator loss: 0.593820, acc: 0.750000] [adversarial loss: 0.599334, acc: 0.640625]\n",
      "1114: [discriminator loss: 0.615608, acc: 0.632812] [adversarial loss: 1.277874, acc: 0.093750]\n",
      "1115: [discriminator loss: 0.712411, acc: 0.546875] [adversarial loss: 0.551410, acc: 0.843750]\n",
      "1116: [discriminator loss: 0.702564, acc: 0.531250] [adversarial loss: 1.386494, acc: 0.015625]\n",
      "1117: [discriminator loss: 0.684732, acc: 0.593750] [adversarial loss: 0.619454, acc: 0.656250]\n",
      "1118: [discriminator loss: 0.660015, acc: 0.578125] [adversarial loss: 1.256493, acc: 0.078125]\n",
      "1119: [discriminator loss: 0.735238, acc: 0.531250] [adversarial loss: 0.614637, acc: 0.640625]\n",
      "1120: [discriminator loss: 0.717718, acc: 0.570312] [adversarial loss: 1.152908, acc: 0.046875]\n",
      "1121: [discriminator loss: 0.677631, acc: 0.585938] [adversarial loss: 0.626345, acc: 0.609375]\n",
      "1122: [discriminator loss: 0.701860, acc: 0.523438] [adversarial loss: 1.093987, acc: 0.140625]\n",
      "1123: [discriminator loss: 0.651004, acc: 0.609375] [adversarial loss: 0.704710, acc: 0.531250]\n",
      "1124: [discriminator loss: 0.683817, acc: 0.546875] [adversarial loss: 0.966427, acc: 0.156250]\n",
      "1125: [discriminator loss: 0.590831, acc: 0.718750] [adversarial loss: 0.773497, acc: 0.406250]\n",
      "1126: [discriminator loss: 0.603254, acc: 0.656250] [adversarial loss: 0.790305, acc: 0.406250]\n",
      "1127: [discriminator loss: 0.643389, acc: 0.593750] [adversarial loss: 0.859638, acc: 0.296875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1128: [discriminator loss: 0.606476, acc: 0.687500] [adversarial loss: 0.790519, acc: 0.328125]\n",
      "1129: [discriminator loss: 0.616247, acc: 0.671875] [adversarial loss: 0.894705, acc: 0.250000]\n",
      "1130: [discriminator loss: 0.640455, acc: 0.625000] [adversarial loss: 0.830218, acc: 0.453125]\n",
      "1131: [discriminator loss: 0.582758, acc: 0.726562] [adversarial loss: 1.033438, acc: 0.203125]\n",
      "1132: [discriminator loss: 0.633744, acc: 0.625000] [adversarial loss: 0.669681, acc: 0.546875]\n",
      "1133: [discriminator loss: 0.671780, acc: 0.617188] [adversarial loss: 1.214523, acc: 0.109375]\n",
      "1134: [discriminator loss: 0.652386, acc: 0.617188] [adversarial loss: 0.586625, acc: 0.671875]\n",
      "1135: [discriminator loss: 0.696722, acc: 0.554688] [adversarial loss: 1.174874, acc: 0.171875]\n",
      "1136: [discriminator loss: 0.691520, acc: 0.546875] [adversarial loss: 0.606617, acc: 0.671875]\n",
      "1137: [discriminator loss: 0.658306, acc: 0.570312] [adversarial loss: 1.236569, acc: 0.078125]\n",
      "1138: [discriminator loss: 0.684204, acc: 0.601562] [adversarial loss: 0.608598, acc: 0.640625]\n",
      "1139: [discriminator loss: 0.662036, acc: 0.609375] [adversarial loss: 1.099049, acc: 0.140625]\n",
      "1140: [discriminator loss: 0.734212, acc: 0.531250] [adversarial loss: 0.733006, acc: 0.515625]\n",
      "1141: [discriminator loss: 0.653113, acc: 0.617188] [adversarial loss: 0.900960, acc: 0.250000]\n",
      "1142: [discriminator loss: 0.674950, acc: 0.539062] [adversarial loss: 0.756530, acc: 0.390625]\n",
      "1143: [discriminator loss: 0.596991, acc: 0.695312] [adversarial loss: 1.066466, acc: 0.187500]\n",
      "1144: [discriminator loss: 0.711693, acc: 0.578125] [adversarial loss: 0.770333, acc: 0.484375]\n",
      "1145: [discriminator loss: 0.656053, acc: 0.578125] [adversarial loss: 1.089029, acc: 0.156250]\n",
      "1146: [discriminator loss: 0.633512, acc: 0.648438] [adversarial loss: 0.607814, acc: 0.703125]\n",
      "1147: [discriminator loss: 0.642456, acc: 0.632812] [adversarial loss: 1.171365, acc: 0.031250]\n",
      "1148: [discriminator loss: 0.664719, acc: 0.585938] [adversarial loss: 0.693347, acc: 0.531250]\n",
      "1149: [discriminator loss: 0.650761, acc: 0.601562] [adversarial loss: 0.992408, acc: 0.171875]\n",
      "1150: [discriminator loss: 0.615075, acc: 0.648438] [adversarial loss: 0.704679, acc: 0.500000]\n",
      "1151: [discriminator loss: 0.618856, acc: 0.625000] [adversarial loss: 1.057134, acc: 0.156250]\n",
      "1152: [discriminator loss: 0.632083, acc: 0.632812] [adversarial loss: 0.611302, acc: 0.703125]\n",
      "1153: [discriminator loss: 0.644681, acc: 0.632812] [adversarial loss: 1.221834, acc: 0.015625]\n",
      "1154: [discriminator loss: 0.684630, acc: 0.578125] [adversarial loss: 0.486859, acc: 0.843750]\n",
      "1155: [discriminator loss: 0.693905, acc: 0.500000] [adversarial loss: 1.296606, acc: 0.015625]\n",
      "1156: [discriminator loss: 0.747323, acc: 0.531250] [adversarial loss: 0.586533, acc: 0.734375]\n",
      "1157: [discriminator loss: 0.670499, acc: 0.609375] [adversarial loss: 1.205265, acc: 0.109375]\n",
      "1158: [discriminator loss: 0.638372, acc: 0.578125] [adversarial loss: 0.735375, acc: 0.468750]\n",
      "1159: [discriminator loss: 0.642312, acc: 0.609375] [adversarial loss: 1.051130, acc: 0.062500]\n",
      "1160: [discriminator loss: 0.627702, acc: 0.617188] [adversarial loss: 0.726127, acc: 0.562500]\n",
      "1161: [discriminator loss: 0.636572, acc: 0.640625] [adversarial loss: 1.002091, acc: 0.203125]\n",
      "1162: [discriminator loss: 0.639238, acc: 0.632812] [adversarial loss: 0.650435, acc: 0.609375]\n",
      "1163: [discriminator loss: 0.630010, acc: 0.601562] [adversarial loss: 1.034649, acc: 0.125000]\n",
      "1164: [discriminator loss: 0.631367, acc: 0.687500] [adversarial loss: 0.702641, acc: 0.546875]\n",
      "1165: [discriminator loss: 0.674316, acc: 0.546875] [adversarial loss: 0.930347, acc: 0.203125]\n",
      "1166: [discriminator loss: 0.650743, acc: 0.609375] [adversarial loss: 0.704900, acc: 0.468750]\n",
      "1167: [discriminator loss: 0.638950, acc: 0.585938] [adversarial loss: 0.938036, acc: 0.171875]\n",
      "1168: [discriminator loss: 0.635168, acc: 0.617188] [adversarial loss: 0.728057, acc: 0.421875]\n",
      "1169: [discriminator loss: 0.655164, acc: 0.617188] [adversarial loss: 1.181730, acc: 0.046875]\n",
      "1170: [discriminator loss: 0.648849, acc: 0.593750] [adversarial loss: 0.672926, acc: 0.500000]\n",
      "1171: [discriminator loss: 0.679201, acc: 0.562500] [adversarial loss: 1.131594, acc: 0.140625]\n",
      "1172: [discriminator loss: 0.720060, acc: 0.515625] [adversarial loss: 0.635536, acc: 0.687500]\n",
      "1173: [discriminator loss: 0.692355, acc: 0.554688] [adversarial loss: 1.068900, acc: 0.171875]\n",
      "1174: [discriminator loss: 0.636685, acc: 0.625000] [adversarial loss: 0.715113, acc: 0.515625]\n",
      "1175: [discriminator loss: 0.678086, acc: 0.578125] [adversarial loss: 1.024859, acc: 0.203125]\n",
      "1176: [discriminator loss: 0.660417, acc: 0.617188] [adversarial loss: 0.713764, acc: 0.515625]\n",
      "1177: [discriminator loss: 0.610340, acc: 0.695312] [adversarial loss: 0.907313, acc: 0.203125]\n",
      "1178: [discriminator loss: 0.589939, acc: 0.718750] [adversarial loss: 0.806142, acc: 0.406250]\n",
      "1179: [discriminator loss: 0.660494, acc: 0.562500] [adversarial loss: 0.795173, acc: 0.390625]\n",
      "1180: [discriminator loss: 0.630817, acc: 0.671875] [adversarial loss: 0.767970, acc: 0.390625]\n",
      "1181: [discriminator loss: 0.653152, acc: 0.578125] [adversarial loss: 0.866856, acc: 0.359375]\n",
      "1182: [discriminator loss: 0.653783, acc: 0.617188] [adversarial loss: 0.807704, acc: 0.328125]\n",
      "1183: [discriminator loss: 0.620457, acc: 0.664062] [adversarial loss: 1.053815, acc: 0.171875]\n",
      "1184: [discriminator loss: 0.706083, acc: 0.515625] [adversarial loss: 0.649849, acc: 0.562500]\n",
      "1185: [discriminator loss: 0.679533, acc: 0.585938] [adversarial loss: 1.314601, acc: 0.062500]\n",
      "1186: [discriminator loss: 0.715501, acc: 0.601562] [adversarial loss: 0.541672, acc: 0.812500]\n",
      "1187: [discriminator loss: 0.696916, acc: 0.554688] [adversarial loss: 1.201308, acc: 0.031250]\n",
      "1188: [discriminator loss: 0.679510, acc: 0.601562] [adversarial loss: 0.592751, acc: 0.656250]\n",
      "1189: [discriminator loss: 0.662438, acc: 0.585938] [adversarial loss: 1.206190, acc: 0.062500]\n",
      "1190: [discriminator loss: 0.719466, acc: 0.593750] [adversarial loss: 0.595301, acc: 0.687500]\n",
      "1191: [discriminator loss: 0.662691, acc: 0.570312] [adversarial loss: 1.078117, acc: 0.187500]\n",
      "1192: [discriminator loss: 0.659747, acc: 0.593750] [adversarial loss: 0.579980, acc: 0.750000]\n",
      "1193: [discriminator loss: 0.653760, acc: 0.632812] [adversarial loss: 0.990519, acc: 0.062500]\n",
      "1194: [discriminator loss: 0.680522, acc: 0.617188] [adversarial loss: 0.665740, acc: 0.609375]\n",
      "1195: [discriminator loss: 0.708129, acc: 0.562500] [adversarial loss: 1.108991, acc: 0.156250]\n",
      "1196: [discriminator loss: 0.672370, acc: 0.507812] [adversarial loss: 0.764284, acc: 0.437500]\n",
      "1197: [discriminator loss: 0.651933, acc: 0.609375] [adversarial loss: 0.986742, acc: 0.078125]\n",
      "1198: [discriminator loss: 0.663889, acc: 0.578125] [adversarial loss: 0.699339, acc: 0.515625]\n",
      "1199: [discriminator loss: 0.664707, acc: 0.585938] [adversarial loss: 0.951898, acc: 0.125000]\n",
      "1200: [discriminator loss: 0.603854, acc: 0.687500] [adversarial loss: 0.777427, acc: 0.343750]\n",
      "1201: [discriminator loss: 0.654070, acc: 0.625000] [adversarial loss: 0.852838, acc: 0.296875]\n",
      "1202: [discriminator loss: 0.621691, acc: 0.656250] [adversarial loss: 0.807650, acc: 0.421875]\n",
      "1203: [discriminator loss: 0.661184, acc: 0.539062] [adversarial loss: 0.849348, acc: 0.281250]\n",
      "1204: [discriminator loss: 0.612885, acc: 0.664062] [adversarial loss: 0.802416, acc: 0.421875]\n",
      "1205: [discriminator loss: 0.662867, acc: 0.593750] [adversarial loss: 0.745138, acc: 0.531250]\n",
      "1206: [discriminator loss: 0.668576, acc: 0.585938] [adversarial loss: 1.063712, acc: 0.140625]\n",
      "1207: [discriminator loss: 0.676093, acc: 0.617188] [adversarial loss: 0.725610, acc: 0.484375]\n",
      "1208: [discriminator loss: 0.675236, acc: 0.546875] [adversarial loss: 1.041869, acc: 0.109375]\n",
      "1209: [discriminator loss: 0.637619, acc: 0.632812] [adversarial loss: 0.557121, acc: 0.750000]\n",
      "1210: [discriminator loss: 0.723938, acc: 0.570312] [adversarial loss: 1.349351, acc: 0.015625]\n",
      "1211: [discriminator loss: 0.720724, acc: 0.539062] [adversarial loss: 0.522997, acc: 0.843750]\n",
      "1212: [discriminator loss: 0.736341, acc: 0.515625] [adversarial loss: 1.344774, acc: 0.015625]\n",
      "1213: [discriminator loss: 0.694924, acc: 0.531250] [adversarial loss: 0.581439, acc: 0.734375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1214: [discriminator loss: 0.730822, acc: 0.523438] [adversarial loss: 1.004028, acc: 0.093750]\n",
      "1215: [discriminator loss: 0.638094, acc: 0.664062] [adversarial loss: 0.672843, acc: 0.578125]\n",
      "1216: [discriminator loss: 0.715575, acc: 0.601562] [adversarial loss: 0.952519, acc: 0.156250]\n",
      "1217: [discriminator loss: 0.639518, acc: 0.687500] [adversarial loss: 0.719276, acc: 0.437500]\n",
      "1218: [discriminator loss: 0.636467, acc: 0.632812] [adversarial loss: 0.897310, acc: 0.171875]\n",
      "1219: [discriminator loss: 0.630076, acc: 0.593750] [adversarial loss: 0.793843, acc: 0.343750]\n",
      "1220: [discriminator loss: 0.683357, acc: 0.593750] [adversarial loss: 0.826646, acc: 0.250000]\n",
      "1221: [discriminator loss: 0.667326, acc: 0.640625] [adversarial loss: 0.778121, acc: 0.359375]\n",
      "1222: [discriminator loss: 0.654706, acc: 0.625000] [adversarial loss: 0.958089, acc: 0.125000]\n",
      "1223: [discriminator loss: 0.654001, acc: 0.593750] [adversarial loss: 0.714249, acc: 0.468750]\n",
      "1224: [discriminator loss: 0.672869, acc: 0.531250] [adversarial loss: 1.059463, acc: 0.125000]\n",
      "1225: [discriminator loss: 0.607432, acc: 0.679688] [adversarial loss: 0.666295, acc: 0.609375]\n",
      "1226: [discriminator loss: 0.645457, acc: 0.632812] [adversarial loss: 1.081433, acc: 0.203125]\n",
      "1227: [discriminator loss: 0.708878, acc: 0.554688] [adversarial loss: 0.698595, acc: 0.546875]\n",
      "1228: [discriminator loss: 0.660026, acc: 0.632812] [adversarial loss: 1.200533, acc: 0.015625]\n",
      "1229: [discriminator loss: 0.640338, acc: 0.585938] [adversarial loss: 0.589202, acc: 0.781250]\n",
      "1230: [discriminator loss: 0.679327, acc: 0.593750] [adversarial loss: 1.080314, acc: 0.031250]\n",
      "1231: [discriminator loss: 0.665165, acc: 0.609375] [adversarial loss: 0.570110, acc: 0.718750]\n",
      "1232: [discriminator loss: 0.672707, acc: 0.562500] [adversarial loss: 1.005411, acc: 0.156250]\n",
      "1233: [discriminator loss: 0.663574, acc: 0.539062] [adversarial loss: 0.552729, acc: 0.843750]\n",
      "1234: [discriminator loss: 0.653996, acc: 0.562500] [adversarial loss: 1.078508, acc: 0.078125]\n",
      "1235: [discriminator loss: 0.656594, acc: 0.562500] [adversarial loss: 0.593994, acc: 0.687500]\n",
      "1236: [discriminator loss: 0.670574, acc: 0.562500] [adversarial loss: 1.111178, acc: 0.140625]\n",
      "1237: [discriminator loss: 0.678341, acc: 0.554688] [adversarial loss: 0.617627, acc: 0.625000]\n",
      "1238: [discriminator loss: 0.699872, acc: 0.570312] [adversarial loss: 1.031942, acc: 0.140625]\n",
      "1239: [discriminator loss: 0.643301, acc: 0.617188] [adversarial loss: 0.698315, acc: 0.531250]\n",
      "1240: [discriminator loss: 0.651617, acc: 0.578125] [adversarial loss: 1.043601, acc: 0.171875]\n",
      "1241: [discriminator loss: 0.638891, acc: 0.617188] [adversarial loss: 0.694340, acc: 0.531250]\n",
      "1242: [discriminator loss: 0.664119, acc: 0.593750] [adversarial loss: 0.982357, acc: 0.125000]\n",
      "1243: [discriminator loss: 0.621704, acc: 0.687500] [adversarial loss: 0.724114, acc: 0.406250]\n",
      "1244: [discriminator loss: 0.652969, acc: 0.648438] [adversarial loss: 1.017986, acc: 0.234375]\n",
      "1245: [discriminator loss: 0.630381, acc: 0.679688] [adversarial loss: 0.731334, acc: 0.484375]\n",
      "1246: [discriminator loss: 0.665827, acc: 0.562500] [adversarial loss: 0.827343, acc: 0.343750]\n",
      "1247: [discriminator loss: 0.619526, acc: 0.664062] [adversarial loss: 0.751623, acc: 0.453125]\n",
      "1248: [discriminator loss: 0.669126, acc: 0.617188] [adversarial loss: 0.969878, acc: 0.093750]\n",
      "1249: [discriminator loss: 0.640556, acc: 0.632812] [adversarial loss: 0.688135, acc: 0.593750]\n",
      "1250: [discriminator loss: 0.652923, acc: 0.562500] [adversarial loss: 1.047376, acc: 0.156250]\n",
      "1251: [discriminator loss: 0.662315, acc: 0.578125] [adversarial loss: 0.665517, acc: 0.578125]\n",
      "1252: [discriminator loss: 0.682046, acc: 0.515625] [adversarial loss: 1.585027, acc: 0.000000]\n",
      "1253: [discriminator loss: 0.774801, acc: 0.562500] [adversarial loss: 0.593064, acc: 0.750000]\n",
      "1254: [discriminator loss: 0.677496, acc: 0.585938] [adversarial loss: 1.059245, acc: 0.109375]\n",
      "1255: [discriminator loss: 0.641807, acc: 0.593750] [adversarial loss: 0.630830, acc: 0.671875]\n",
      "1256: [discriminator loss: 0.687937, acc: 0.554688] [adversarial loss: 0.964498, acc: 0.234375]\n",
      "1257: [discriminator loss: 0.645945, acc: 0.625000] [adversarial loss: 0.845803, acc: 0.343750]\n",
      "1258: [discriminator loss: 0.668472, acc: 0.617188] [adversarial loss: 0.800046, acc: 0.343750]\n",
      "1259: [discriminator loss: 0.679641, acc: 0.554688] [adversarial loss: 1.060871, acc: 0.171875]\n",
      "1260: [discriminator loss: 0.719203, acc: 0.531250] [adversarial loss: 0.618092, acc: 0.687500]\n",
      "1261: [discriminator loss: 0.661108, acc: 0.601562] [adversarial loss: 0.987720, acc: 0.203125]\n",
      "1262: [discriminator loss: 0.630027, acc: 0.648438] [adversarial loss: 0.690163, acc: 0.531250]\n",
      "1263: [discriminator loss: 0.621106, acc: 0.679688] [adversarial loss: 0.953974, acc: 0.218750]\n",
      "1264: [discriminator loss: 0.641209, acc: 0.648438] [adversarial loss: 0.798998, acc: 0.406250]\n",
      "1265: [discriminator loss: 0.658997, acc: 0.617188] [adversarial loss: 1.018780, acc: 0.093750]\n",
      "1266: [discriminator loss: 0.673690, acc: 0.593750] [adversarial loss: 0.695289, acc: 0.500000]\n",
      "1267: [discriminator loss: 0.653359, acc: 0.562500] [adversarial loss: 0.918216, acc: 0.218750]\n",
      "1268: [discriminator loss: 0.644024, acc: 0.671875] [adversarial loss: 0.688303, acc: 0.593750]\n",
      "1269: [discriminator loss: 0.657951, acc: 0.609375] [adversarial loss: 1.207294, acc: 0.031250]\n",
      "1270: [discriminator loss: 0.696779, acc: 0.554688] [adversarial loss: 0.597696, acc: 0.687500]\n",
      "1271: [discriminator loss: 0.707467, acc: 0.546875] [adversarial loss: 1.225623, acc: 0.000000]\n",
      "1272: [discriminator loss: 0.668189, acc: 0.632812] [adversarial loss: 0.623445, acc: 0.687500]\n",
      "1273: [discriminator loss: 0.651252, acc: 0.625000] [adversarial loss: 0.926614, acc: 0.140625]\n",
      "1274: [discriminator loss: 0.644180, acc: 0.593750] [adversarial loss: 0.651370, acc: 0.656250]\n",
      "1275: [discriminator loss: 0.664189, acc: 0.546875] [adversarial loss: 1.051486, acc: 0.140625]\n",
      "1276: [discriminator loss: 0.661105, acc: 0.617188] [adversarial loss: 0.646719, acc: 0.562500]\n",
      "1277: [discriminator loss: 0.628993, acc: 0.617188] [adversarial loss: 1.013352, acc: 0.109375]\n",
      "1278: [discriminator loss: 0.696626, acc: 0.554688] [adversarial loss: 0.642819, acc: 0.609375]\n",
      "1279: [discriminator loss: 0.675792, acc: 0.546875] [adversarial loss: 1.142823, acc: 0.031250]\n",
      "1280: [discriminator loss: 0.716968, acc: 0.531250] [adversarial loss: 0.562584, acc: 0.718750]\n",
      "1281: [discriminator loss: 0.640373, acc: 0.601562] [adversarial loss: 1.116385, acc: 0.093750]\n",
      "1282: [discriminator loss: 0.694258, acc: 0.585938] [adversarial loss: 0.673568, acc: 0.515625]\n",
      "1283: [discriminator loss: 0.670815, acc: 0.546875] [adversarial loss: 1.105932, acc: 0.078125]\n",
      "1284: [discriminator loss: 0.635580, acc: 0.640625] [adversarial loss: 0.677065, acc: 0.593750]\n",
      "1285: [discriminator loss: 0.651583, acc: 0.585938] [adversarial loss: 0.860427, acc: 0.296875]\n",
      "1286: [discriminator loss: 0.633209, acc: 0.687500] [adversarial loss: 0.855469, acc: 0.187500]\n",
      "1287: [discriminator loss: 0.658654, acc: 0.617188] [adversarial loss: 0.667784, acc: 0.609375]\n",
      "1288: [discriminator loss: 0.642998, acc: 0.625000] [adversarial loss: 0.840858, acc: 0.296875]\n",
      "1289: [discriminator loss: 0.672472, acc: 0.562500] [adversarial loss: 0.612239, acc: 0.718750]\n",
      "1290: [discriminator loss: 0.615719, acc: 0.703125] [adversarial loss: 0.931562, acc: 0.281250]\n",
      "1291: [discriminator loss: 0.658563, acc: 0.617188] [adversarial loss: 0.719676, acc: 0.531250]\n",
      "1292: [discriminator loss: 0.639228, acc: 0.617188] [adversarial loss: 0.947946, acc: 0.218750]\n",
      "1293: [discriminator loss: 0.662168, acc: 0.570312] [adversarial loss: 0.675757, acc: 0.593750]\n",
      "1294: [discriminator loss: 0.646835, acc: 0.601562] [adversarial loss: 0.974410, acc: 0.093750]\n",
      "1295: [discriminator loss: 0.643663, acc: 0.593750] [adversarial loss: 0.610517, acc: 0.718750]\n",
      "1296: [discriminator loss: 0.700387, acc: 0.570312] [adversarial loss: 1.225761, acc: 0.125000]\n",
      "1297: [discriminator loss: 0.717721, acc: 0.570312] [adversarial loss: 0.555965, acc: 0.750000]\n",
      "1298: [discriminator loss: 0.788858, acc: 0.515625] [adversarial loss: 1.231054, acc: 0.046875]\n",
      "1299: [discriminator loss: 0.722674, acc: 0.546875] [adversarial loss: 0.601103, acc: 0.703125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1300: [discriminator loss: 0.694289, acc: 0.570312] [adversarial loss: 0.998175, acc: 0.140625]\n",
      "1301: [discriminator loss: 0.637122, acc: 0.632812] [adversarial loss: 0.764809, acc: 0.406250]\n",
      "1302: [discriminator loss: 0.611867, acc: 0.679688] [adversarial loss: 0.774871, acc: 0.406250]\n",
      "1303: [discriminator loss: 0.672051, acc: 0.531250] [adversarial loss: 0.866085, acc: 0.218750]\n",
      "1304: [discriminator loss: 0.617048, acc: 0.687500] [adversarial loss: 0.727536, acc: 0.500000]\n",
      "1305: [discriminator loss: 0.642165, acc: 0.625000] [adversarial loss: 0.955830, acc: 0.234375]\n",
      "1306: [discriminator loss: 0.677664, acc: 0.578125] [adversarial loss: 0.705239, acc: 0.531250]\n",
      "1307: [discriminator loss: 0.721419, acc: 0.507812] [adversarial loss: 1.024941, acc: 0.125000]\n",
      "1308: [discriminator loss: 0.672469, acc: 0.593750] [adversarial loss: 0.708434, acc: 0.468750]\n",
      "1309: [discriminator loss: 0.624307, acc: 0.679688] [adversarial loss: 0.955417, acc: 0.296875]\n",
      "1310: [discriminator loss: 0.668657, acc: 0.585938] [adversarial loss: 0.696674, acc: 0.531250]\n",
      "1311: [discriminator loss: 0.625305, acc: 0.617188] [adversarial loss: 1.039822, acc: 0.203125]\n",
      "1312: [discriminator loss: 0.653976, acc: 0.617188] [adversarial loss: 0.731243, acc: 0.546875]\n",
      "1313: [discriminator loss: 0.652612, acc: 0.601562] [adversarial loss: 0.930502, acc: 0.171875]\n",
      "1314: [discriminator loss: 0.635841, acc: 0.632812] [adversarial loss: 0.748205, acc: 0.421875]\n",
      "1315: [discriminator loss: 0.657538, acc: 0.664062] [adversarial loss: 1.030008, acc: 0.062500]\n",
      "1316: [discriminator loss: 0.648974, acc: 0.593750] [adversarial loss: 0.573726, acc: 0.671875]\n",
      "1317: [discriminator loss: 0.699832, acc: 0.515625] [adversarial loss: 1.276402, acc: 0.109375]\n",
      "1318: [discriminator loss: 0.705173, acc: 0.585938] [adversarial loss: 0.617917, acc: 0.656250]\n",
      "1319: [discriminator loss: 0.692338, acc: 0.539062] [adversarial loss: 1.043974, acc: 0.171875]\n",
      "1320: [discriminator loss: 0.669466, acc: 0.609375] [adversarial loss: 0.551758, acc: 0.828125]\n",
      "1321: [discriminator loss: 0.686795, acc: 0.554688] [adversarial loss: 1.002210, acc: 0.156250]\n",
      "1322: [discriminator loss: 0.674618, acc: 0.554688] [adversarial loss: 0.677604, acc: 0.515625]\n",
      "1323: [discriminator loss: 0.679035, acc: 0.570312] [adversarial loss: 0.984175, acc: 0.093750]\n",
      "1324: [discriminator loss: 0.655047, acc: 0.578125] [adversarial loss: 0.668521, acc: 0.593750]\n",
      "1325: [discriminator loss: 0.691700, acc: 0.593750] [adversarial loss: 1.048266, acc: 0.046875]\n",
      "1326: [discriminator loss: 0.640091, acc: 0.585938] [adversarial loss: 0.641859, acc: 0.656250]\n",
      "1327: [discriminator loss: 0.668288, acc: 0.601562] [adversarial loss: 1.147235, acc: 0.046875]\n",
      "1328: [discriminator loss: 0.683738, acc: 0.585938] [adversarial loss: 0.586683, acc: 0.750000]\n",
      "1329: [discriminator loss: 0.664825, acc: 0.562500] [adversarial loss: 0.985556, acc: 0.078125]\n",
      "1330: [discriminator loss: 0.643866, acc: 0.578125] [adversarial loss: 0.676061, acc: 0.546875]\n",
      "1331: [discriminator loss: 0.602916, acc: 0.671875] [adversarial loss: 0.921174, acc: 0.203125]\n",
      "1332: [discriminator loss: 0.654740, acc: 0.617188] [adversarial loss: 0.750864, acc: 0.421875]\n",
      "1333: [discriminator loss: 0.675319, acc: 0.593750] [adversarial loss: 0.934860, acc: 0.140625]\n",
      "1334: [discriminator loss: 0.615203, acc: 0.679688] [adversarial loss: 0.785661, acc: 0.437500]\n",
      "1335: [discriminator loss: 0.640647, acc: 0.617188] [adversarial loss: 0.775828, acc: 0.390625]\n",
      "1336: [discriminator loss: 0.567383, acc: 0.781250] [adversarial loss: 0.838808, acc: 0.359375]\n",
      "1337: [discriminator loss: 0.709964, acc: 0.507812] [adversarial loss: 0.774947, acc: 0.390625]\n",
      "1338: [discriminator loss: 0.641784, acc: 0.617188] [adversarial loss: 0.923531, acc: 0.140625]\n",
      "1339: [discriminator loss: 0.690923, acc: 0.531250] [adversarial loss: 0.827773, acc: 0.328125]\n",
      "1340: [discriminator loss: 0.649449, acc: 0.632812] [adversarial loss: 0.877655, acc: 0.328125]\n",
      "1341: [discriminator loss: 0.703409, acc: 0.492188] [adversarial loss: 0.770374, acc: 0.359375]\n",
      "1342: [discriminator loss: 0.652092, acc: 0.578125] [adversarial loss: 1.035629, acc: 0.046875]\n",
      "1343: [discriminator loss: 0.611879, acc: 0.664062] [adversarial loss: 0.743879, acc: 0.406250]\n",
      "1344: [discriminator loss: 0.656248, acc: 0.609375] [adversarial loss: 0.874912, acc: 0.265625]\n",
      "1345: [discriminator loss: 0.662563, acc: 0.593750] [adversarial loss: 0.569704, acc: 0.734375]\n",
      "1346: [discriminator loss: 0.638107, acc: 0.601562] [adversarial loss: 1.341286, acc: 0.015625]\n",
      "1347: [discriminator loss: 0.742908, acc: 0.539062] [adversarial loss: 0.519903, acc: 0.796875]\n",
      "1348: [discriminator loss: 0.724666, acc: 0.578125] [adversarial loss: 1.209940, acc: 0.031250]\n",
      "1349: [discriminator loss: 0.706890, acc: 0.539062] [adversarial loss: 0.611108, acc: 0.640625]\n",
      "1350: [discriminator loss: 0.676283, acc: 0.554688] [adversarial loss: 0.935470, acc: 0.171875]\n",
      "1351: [discriminator loss: 0.659336, acc: 0.656250] [adversarial loss: 0.808127, acc: 0.343750]\n",
      "1352: [discriminator loss: 0.603149, acc: 0.656250] [adversarial loss: 0.811088, acc: 0.421875]\n",
      "1353: [discriminator loss: 0.595104, acc: 0.734375] [adversarial loss: 0.812109, acc: 0.328125]\n",
      "1354: [discriminator loss: 0.652924, acc: 0.554688] [adversarial loss: 0.788791, acc: 0.328125]\n",
      "1355: [discriminator loss: 0.676814, acc: 0.656250] [adversarial loss: 0.734912, acc: 0.453125]\n",
      "1356: [discriminator loss: 0.619982, acc: 0.687500] [adversarial loss: 0.909452, acc: 0.234375]\n",
      "1357: [discriminator loss: 0.639018, acc: 0.617188] [adversarial loss: 0.845787, acc: 0.328125]\n",
      "1358: [discriminator loss: 0.617142, acc: 0.679688] [adversarial loss: 0.688234, acc: 0.578125]\n",
      "1359: [discriminator loss: 0.637143, acc: 0.632812] [adversarial loss: 0.918742, acc: 0.250000]\n",
      "1360: [discriminator loss: 0.627750, acc: 0.601562] [adversarial loss: 0.527580, acc: 0.859375]\n",
      "1361: [discriminator loss: 0.666466, acc: 0.593750] [adversarial loss: 1.177973, acc: 0.062500]\n",
      "1362: [discriminator loss: 0.726561, acc: 0.539062] [adversarial loss: 0.568604, acc: 0.656250]\n",
      "1363: [discriminator loss: 0.696851, acc: 0.609375] [adversarial loss: 1.142144, acc: 0.062500]\n",
      "1364: [discriminator loss: 0.669892, acc: 0.585938] [adversarial loss: 0.582410, acc: 0.671875]\n",
      "1365: [discriminator loss: 0.670048, acc: 0.617188] [adversarial loss: 1.202928, acc: 0.078125]\n",
      "1366: [discriminator loss: 0.663245, acc: 0.609375] [adversarial loss: 0.693642, acc: 0.484375]\n",
      "1367: [discriminator loss: 0.629001, acc: 0.656250] [adversarial loss: 0.937648, acc: 0.234375]\n",
      "1368: [discriminator loss: 0.683759, acc: 0.578125] [adversarial loss: 0.764285, acc: 0.453125]\n",
      "1369: [discriminator loss: 0.625257, acc: 0.648438] [adversarial loss: 0.856458, acc: 0.375000]\n",
      "1370: [discriminator loss: 0.605174, acc: 0.679688] [adversarial loss: 0.878605, acc: 0.343750]\n",
      "1371: [discriminator loss: 0.668899, acc: 0.570312] [adversarial loss: 0.839081, acc: 0.296875]\n",
      "1372: [discriminator loss: 0.625724, acc: 0.625000] [adversarial loss: 0.743506, acc: 0.453125]\n",
      "1373: [discriminator loss: 0.611590, acc: 0.679688] [adversarial loss: 0.843184, acc: 0.406250]\n",
      "1374: [discriminator loss: 0.661501, acc: 0.593750] [adversarial loss: 0.775667, acc: 0.375000]\n",
      "1375: [discriminator loss: 0.673362, acc: 0.570312] [adversarial loss: 0.862108, acc: 0.281250]\n",
      "1376: [discriminator loss: 0.630847, acc: 0.687500] [adversarial loss: 0.829767, acc: 0.328125]\n",
      "1377: [discriminator loss: 0.637362, acc: 0.632812] [adversarial loss: 0.647114, acc: 0.656250]\n",
      "1378: [discriminator loss: 0.692610, acc: 0.609375] [adversarial loss: 1.161216, acc: 0.046875]\n",
      "1379: [discriminator loss: 0.713967, acc: 0.515625] [adversarial loss: 0.572354, acc: 0.750000]\n",
      "1380: [discriminator loss: 0.678982, acc: 0.539062] [adversarial loss: 1.034667, acc: 0.062500]\n",
      "1381: [discriminator loss: 0.657717, acc: 0.617188] [adversarial loss: 0.626472, acc: 0.671875]\n",
      "1382: [discriminator loss: 0.688704, acc: 0.562500] [adversarial loss: 1.103943, acc: 0.093750]\n",
      "1383: [discriminator loss: 0.681363, acc: 0.578125] [adversarial loss: 0.545123, acc: 0.781250]\n",
      "1384: [discriminator loss: 0.680179, acc: 0.593750] [adversarial loss: 1.226682, acc: 0.203125]\n",
      "1385: [discriminator loss: 0.737558, acc: 0.562500] [adversarial loss: 0.645992, acc: 0.578125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1386: [discriminator loss: 0.680056, acc: 0.570312] [adversarial loss: 1.054488, acc: 0.109375]\n",
      "1387: [discriminator loss: 0.659286, acc: 0.593750] [adversarial loss: 0.584841, acc: 0.718750]\n",
      "1388: [discriminator loss: 0.732680, acc: 0.507812] [adversarial loss: 1.061336, acc: 0.093750]\n",
      "1389: [discriminator loss: 0.664469, acc: 0.585938] [adversarial loss: 0.608781, acc: 0.734375]\n",
      "1390: [discriminator loss: 0.682454, acc: 0.554688] [adversarial loss: 1.088570, acc: 0.078125]\n",
      "1391: [discriminator loss: 0.628896, acc: 0.648438] [adversarial loss: 0.727709, acc: 0.421875]\n",
      "1392: [discriminator loss: 0.679556, acc: 0.531250] [adversarial loss: 0.882005, acc: 0.281250]\n",
      "1393: [discriminator loss: 0.620758, acc: 0.656250] [adversarial loss: 0.965620, acc: 0.250000]\n",
      "1394: [discriminator loss: 0.669787, acc: 0.625000] [adversarial loss: 0.807048, acc: 0.343750]\n",
      "1395: [discriminator loss: 0.652739, acc: 0.664062] [adversarial loss: 1.043461, acc: 0.093750]\n",
      "1396: [discriminator loss: 0.629616, acc: 0.687500] [adversarial loss: 0.691685, acc: 0.515625]\n",
      "1397: [discriminator loss: 0.689099, acc: 0.562500] [adversarial loss: 1.102144, acc: 0.140625]\n",
      "1398: [discriminator loss: 0.696867, acc: 0.570312] [adversarial loss: 0.633702, acc: 0.640625]\n",
      "1399: [discriminator loss: 0.700758, acc: 0.507812] [adversarial loss: 0.950804, acc: 0.187500]\n",
      "1400: [discriminator loss: 0.657694, acc: 0.562500] [adversarial loss: 0.674155, acc: 0.625000]\n",
      "1401: [discriminator loss: 0.655496, acc: 0.585938] [adversarial loss: 1.099497, acc: 0.093750]\n",
      "1402: [discriminator loss: 0.687740, acc: 0.570312] [adversarial loss: 0.598978, acc: 0.687500]\n",
      "1403: [discriminator loss: 0.693302, acc: 0.554688] [adversarial loss: 1.025445, acc: 0.171875]\n",
      "1404: [discriminator loss: 0.721136, acc: 0.546875] [adversarial loss: 0.678938, acc: 0.640625]\n",
      "1405: [discriminator loss: 0.674380, acc: 0.554688] [adversarial loss: 1.105645, acc: 0.109375]\n",
      "1406: [discriminator loss: 0.649741, acc: 0.585938] [adversarial loss: 0.606031, acc: 0.750000]\n",
      "1407: [discriminator loss: 0.684854, acc: 0.585938] [adversarial loss: 0.982223, acc: 0.125000]\n",
      "1408: [discriminator loss: 0.666612, acc: 0.625000] [adversarial loss: 0.685514, acc: 0.531250]\n",
      "1409: [discriminator loss: 0.697913, acc: 0.531250] [adversarial loss: 0.932253, acc: 0.250000]\n",
      "1410: [discriminator loss: 0.616313, acc: 0.648438] [adversarial loss: 0.687014, acc: 0.578125]\n",
      "1411: [discriminator loss: 0.637068, acc: 0.625000] [adversarial loss: 0.967569, acc: 0.187500]\n",
      "1412: [discriminator loss: 0.646804, acc: 0.617188] [adversarial loss: 0.729082, acc: 0.484375]\n",
      "1413: [discriminator loss: 0.694147, acc: 0.554688] [adversarial loss: 1.091421, acc: 0.125000]\n",
      "1414: [discriminator loss: 0.676847, acc: 0.593750] [adversarial loss: 0.568144, acc: 0.734375]\n",
      "1415: [discriminator loss: 0.680359, acc: 0.554688] [adversarial loss: 1.188944, acc: 0.015625]\n",
      "1416: [discriminator loss: 0.679371, acc: 0.562500] [adversarial loss: 0.655394, acc: 0.625000]\n",
      "1417: [discriminator loss: 0.672544, acc: 0.570312] [adversarial loss: 0.859594, acc: 0.203125]\n",
      "1418: [discriminator loss: 0.625942, acc: 0.656250] [adversarial loss: 0.697199, acc: 0.546875]\n",
      "1419: [discriminator loss: 0.687553, acc: 0.601562] [adversarial loss: 0.912536, acc: 0.203125]\n",
      "1420: [discriminator loss: 0.635419, acc: 0.671875] [adversarial loss: 0.730866, acc: 0.437500]\n",
      "1421: [discriminator loss: 0.641772, acc: 0.687500] [adversarial loss: 0.832080, acc: 0.250000]\n",
      "1422: [discriminator loss: 0.636436, acc: 0.632812] [adversarial loss: 0.762572, acc: 0.468750]\n",
      "1423: [discriminator loss: 0.629486, acc: 0.656250] [adversarial loss: 0.915960, acc: 0.281250]\n",
      "1424: [discriminator loss: 0.653290, acc: 0.593750] [adversarial loss: 0.611655, acc: 0.703125]\n",
      "1425: [discriminator loss: 0.652901, acc: 0.593750] [adversarial loss: 1.017690, acc: 0.078125]\n",
      "1426: [discriminator loss: 0.655106, acc: 0.570312] [adversarial loss: 0.625759, acc: 0.671875]\n",
      "1427: [discriminator loss: 0.699050, acc: 0.562500] [adversarial loss: 1.018026, acc: 0.171875]\n",
      "1428: [discriminator loss: 0.653779, acc: 0.640625] [adversarial loss: 0.599090, acc: 0.703125]\n",
      "1429: [discriminator loss: 0.676269, acc: 0.585938] [adversarial loss: 1.125086, acc: 0.140625]\n",
      "1430: [discriminator loss: 0.736825, acc: 0.515625] [adversarial loss: 0.565372, acc: 0.750000]\n",
      "1431: [discriminator loss: 0.718011, acc: 0.562500] [adversarial loss: 1.057422, acc: 0.031250]\n",
      "1432: [discriminator loss: 0.627805, acc: 0.625000] [adversarial loss: 0.718882, acc: 0.531250]\n",
      "1433: [discriminator loss: 0.647795, acc: 0.609375] [adversarial loss: 0.856418, acc: 0.281250]\n",
      "1434: [discriminator loss: 0.653581, acc: 0.617188] [adversarial loss: 0.763311, acc: 0.359375]\n",
      "1435: [discriminator loss: 0.615879, acc: 0.664062] [adversarial loss: 1.006935, acc: 0.203125]\n",
      "1436: [discriminator loss: 0.637448, acc: 0.648438] [adversarial loss: 0.753338, acc: 0.375000]\n",
      "1437: [discriminator loss: 0.681742, acc: 0.593750] [adversarial loss: 0.933963, acc: 0.140625]\n",
      "1438: [discriminator loss: 0.663743, acc: 0.570312] [adversarial loss: 0.598171, acc: 0.718750]\n",
      "1439: [discriminator loss: 0.638699, acc: 0.578125] [adversarial loss: 1.168463, acc: 0.015625]\n",
      "1440: [discriminator loss: 0.684487, acc: 0.593750] [adversarial loss: 0.586481, acc: 0.750000]\n",
      "1441: [discriminator loss: 0.715545, acc: 0.570312] [adversarial loss: 0.968958, acc: 0.093750]\n",
      "1442: [discriminator loss: 0.641888, acc: 0.601562] [adversarial loss: 0.675608, acc: 0.484375]\n",
      "1443: [discriminator loss: 0.662379, acc: 0.593750] [adversarial loss: 1.140041, acc: 0.031250]\n",
      "1444: [discriminator loss: 0.648165, acc: 0.640625] [adversarial loss: 0.711546, acc: 0.484375]\n",
      "1445: [discriminator loss: 0.666534, acc: 0.578125] [adversarial loss: 0.925967, acc: 0.234375]\n",
      "1446: [discriminator loss: 0.679636, acc: 0.578125] [adversarial loss: 0.595435, acc: 0.734375]\n",
      "1447: [discriminator loss: 0.649622, acc: 0.632812] [adversarial loss: 0.972402, acc: 0.171875]\n",
      "1448: [discriminator loss: 0.634614, acc: 0.648438] [adversarial loss: 0.726570, acc: 0.500000]\n",
      "1449: [discriminator loss: 0.698525, acc: 0.554688] [adversarial loss: 1.133456, acc: 0.031250]\n",
      "1450: [discriminator loss: 0.665823, acc: 0.609375] [adversarial loss: 0.551768, acc: 0.750000]\n",
      "1451: [discriminator loss: 0.698617, acc: 0.554688] [adversarial loss: 1.177991, acc: 0.078125]\n",
      "1452: [discriminator loss: 0.661797, acc: 0.570312] [adversarial loss: 0.580483, acc: 0.750000]\n",
      "1453: [discriminator loss: 0.703540, acc: 0.507812] [adversarial loss: 1.078490, acc: 0.125000]\n",
      "1454: [discriminator loss: 0.651012, acc: 0.601562] [adversarial loss: 0.667143, acc: 0.593750]\n",
      "1455: [discriminator loss: 0.670518, acc: 0.539062] [adversarial loss: 0.955394, acc: 0.156250]\n",
      "1456: [discriminator loss: 0.657092, acc: 0.640625] [adversarial loss: 0.687232, acc: 0.562500]\n",
      "1457: [discriminator loss: 0.694013, acc: 0.515625] [adversarial loss: 0.975667, acc: 0.125000]\n",
      "1458: [discriminator loss: 0.644664, acc: 0.617188] [adversarial loss: 0.737730, acc: 0.453125]\n",
      "1459: [discriminator loss: 0.655359, acc: 0.593750] [adversarial loss: 0.793786, acc: 0.328125]\n",
      "1460: [discriminator loss: 0.634199, acc: 0.609375] [adversarial loss: 0.707195, acc: 0.531250]\n",
      "1461: [discriminator loss: 0.656071, acc: 0.632812] [adversarial loss: 0.940230, acc: 0.187500]\n",
      "1462: [discriminator loss: 0.648611, acc: 0.554688] [adversarial loss: 0.728713, acc: 0.500000]\n",
      "1463: [discriminator loss: 0.653411, acc: 0.625000] [adversarial loss: 1.015106, acc: 0.187500]\n",
      "1464: [discriminator loss: 0.635658, acc: 0.617188] [adversarial loss: 0.730159, acc: 0.453125]\n",
      "1465: [discriminator loss: 0.684534, acc: 0.562500] [adversarial loss: 0.901492, acc: 0.218750]\n",
      "1466: [discriminator loss: 0.671369, acc: 0.578125] [adversarial loss: 0.729748, acc: 0.421875]\n",
      "1467: [discriminator loss: 0.662743, acc: 0.578125] [adversarial loss: 0.947681, acc: 0.109375]\n",
      "1468: [discriminator loss: 0.643216, acc: 0.609375] [adversarial loss: 0.593569, acc: 0.781250]\n",
      "1469: [discriminator loss: 0.676889, acc: 0.562500] [adversarial loss: 1.264720, acc: 0.046875]\n",
      "1470: [discriminator loss: 0.722207, acc: 0.539062] [adversarial loss: 0.501112, acc: 0.859375]\n",
      "1471: [discriminator loss: 0.711287, acc: 0.539062] [adversarial loss: 1.237334, acc: 0.062500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1472: [discriminator loss: 0.688351, acc: 0.570312] [adversarial loss: 0.698704, acc: 0.531250]\n",
      "1473: [discriminator loss: 0.650824, acc: 0.640625] [adversarial loss: 0.787297, acc: 0.375000]\n",
      "1474: [discriminator loss: 0.633560, acc: 0.664062] [adversarial loss: 0.799472, acc: 0.312500]\n",
      "1475: [discriminator loss: 0.643490, acc: 0.625000] [adversarial loss: 0.793503, acc: 0.296875]\n",
      "1476: [discriminator loss: 0.617014, acc: 0.695312] [adversarial loss: 0.916577, acc: 0.250000]\n",
      "1477: [discriminator loss: 0.603983, acc: 0.726562] [adversarial loss: 0.867170, acc: 0.250000]\n",
      "1478: [discriminator loss: 0.666257, acc: 0.601562] [adversarial loss: 0.814261, acc: 0.265625]\n",
      "1479: [discriminator loss: 0.649764, acc: 0.601562] [adversarial loss: 0.993720, acc: 0.140625]\n",
      "1480: [discriminator loss: 0.613764, acc: 0.617188] [adversarial loss: 0.713585, acc: 0.468750]\n",
      "1481: [discriminator loss: 0.682210, acc: 0.617188] [adversarial loss: 1.043467, acc: 0.093750]\n",
      "1482: [discriminator loss: 0.673095, acc: 0.570312] [adversarial loss: 0.598321, acc: 0.796875]\n",
      "1483: [discriminator loss: 0.671478, acc: 0.609375] [adversarial loss: 1.203340, acc: 0.031250]\n",
      "1484: [discriminator loss: 0.725676, acc: 0.515625] [adversarial loss: 0.686084, acc: 0.546875]\n",
      "1485: [discriminator loss: 0.635482, acc: 0.632812] [adversarial loss: 1.126532, acc: 0.062500]\n",
      "1486: [discriminator loss: 0.640632, acc: 0.625000] [adversarial loss: 0.630147, acc: 0.671875]\n",
      "1487: [discriminator loss: 0.683858, acc: 0.585938] [adversarial loss: 1.012725, acc: 0.156250]\n",
      "1488: [discriminator loss: 0.738079, acc: 0.554688] [adversarial loss: 0.601077, acc: 0.781250]\n",
      "1489: [discriminator loss: 0.675612, acc: 0.554688] [adversarial loss: 0.912197, acc: 0.203125]\n",
      "1490: [discriminator loss: 0.653157, acc: 0.632812] [adversarial loss: 0.628357, acc: 0.687500]\n",
      "1491: [discriminator loss: 0.679854, acc: 0.593750] [adversarial loss: 0.922264, acc: 0.140625]\n",
      "1492: [discriminator loss: 0.651710, acc: 0.625000] [adversarial loss: 0.676405, acc: 0.515625]\n",
      "1493: [discriminator loss: 0.660528, acc: 0.562500] [adversarial loss: 1.067832, acc: 0.125000]\n",
      "1494: [discriminator loss: 0.637486, acc: 0.640625] [adversarial loss: 0.801054, acc: 0.359375]\n",
      "1495: [discriminator loss: 0.663233, acc: 0.539062] [adversarial loss: 0.805855, acc: 0.359375]\n",
      "1496: [discriminator loss: 0.664900, acc: 0.656250] [adversarial loss: 0.752828, acc: 0.453125]\n",
      "1497: [discriminator loss: 0.636333, acc: 0.632812] [adversarial loss: 1.034680, acc: 0.171875]\n",
      "1498: [discriminator loss: 0.721915, acc: 0.523438] [adversarial loss: 0.546104, acc: 0.828125]\n",
      "1499: [discriminator loss: 0.661305, acc: 0.562500] [adversarial loss: 1.034326, acc: 0.062500]\n",
      "cgan_mnist  labels for generated images:  [0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5]\n",
      "1500: [discriminator loss: 0.642772, acc: 0.593750] [adversarial loss: 0.630684, acc: 0.671875]\n",
      "1501: [discriminator loss: 0.691387, acc: 0.539062] [adversarial loss: 1.030395, acc: 0.093750]\n",
      "1502: [discriminator loss: 0.678755, acc: 0.578125] [adversarial loss: 0.621081, acc: 0.734375]\n",
      "1503: [discriminator loss: 0.667812, acc: 0.593750] [adversarial loss: 0.944395, acc: 0.203125]\n",
      "1504: [discriminator loss: 0.668248, acc: 0.609375] [adversarial loss: 0.602176, acc: 0.718750]\n",
      "1505: [discriminator loss: 0.687190, acc: 0.601562] [adversarial loss: 0.978798, acc: 0.187500]\n",
      "1506: [discriminator loss: 0.689716, acc: 0.546875] [adversarial loss: 0.710076, acc: 0.437500]\n",
      "1507: [discriminator loss: 0.662958, acc: 0.617188] [adversarial loss: 0.916259, acc: 0.218750]\n",
      "1508: [discriminator loss: 0.658271, acc: 0.617188] [adversarial loss: 0.625115, acc: 0.671875]\n",
      "1509: [discriminator loss: 0.690503, acc: 0.585938] [adversarial loss: 0.937577, acc: 0.093750]\n",
      "1510: [discriminator loss: 0.669567, acc: 0.554688] [adversarial loss: 0.608167, acc: 0.656250]\n",
      "1511: [discriminator loss: 0.644442, acc: 0.601562] [adversarial loss: 1.146394, acc: 0.062500]\n",
      "1512: [discriminator loss: 0.741806, acc: 0.476562] [adversarial loss: 0.623976, acc: 0.703125]\n",
      "1513: [discriminator loss: 0.708221, acc: 0.515625] [adversarial loss: 1.003884, acc: 0.171875]\n",
      "1514: [discriminator loss: 0.662318, acc: 0.648438] [adversarial loss: 0.870450, acc: 0.359375]\n",
      "1515: [discriminator loss: 0.649330, acc: 0.640625] [adversarial loss: 0.762839, acc: 0.375000]\n",
      "1516: [discriminator loss: 0.684623, acc: 0.593750] [adversarial loss: 0.818245, acc: 0.343750]\n",
      "1517: [discriminator loss: 0.623861, acc: 0.671875] [adversarial loss: 0.808200, acc: 0.328125]\n",
      "1518: [discriminator loss: 0.669469, acc: 0.531250] [adversarial loss: 0.895213, acc: 0.156250]\n",
      "1519: [discriminator loss: 0.673370, acc: 0.593750] [adversarial loss: 0.789098, acc: 0.359375]\n",
      "1520: [discriminator loss: 0.645816, acc: 0.562500] [adversarial loss: 0.929276, acc: 0.140625]\n",
      "1521: [discriminator loss: 0.676205, acc: 0.601562] [adversarial loss: 0.730403, acc: 0.468750]\n",
      "1522: [discriminator loss: 0.633595, acc: 0.710938] [adversarial loss: 1.025539, acc: 0.218750]\n",
      "1523: [discriminator loss: 0.632230, acc: 0.710938] [adversarial loss: 0.591318, acc: 0.781250]\n",
      "1524: [discriminator loss: 0.675453, acc: 0.531250] [adversarial loss: 1.283504, acc: 0.062500]\n",
      "1525: [discriminator loss: 0.714467, acc: 0.531250] [adversarial loss: 0.539207, acc: 0.796875]\n",
      "1526: [discriminator loss: 0.715297, acc: 0.523438] [adversarial loss: 0.950952, acc: 0.203125]\n",
      "1527: [discriminator loss: 0.683691, acc: 0.531250] [adversarial loss: 0.692381, acc: 0.609375]\n",
      "1528: [discriminator loss: 0.618790, acc: 0.687500] [adversarial loss: 0.900386, acc: 0.312500]\n",
      "1529: [discriminator loss: 0.641073, acc: 0.625000] [adversarial loss: 0.795313, acc: 0.359375]\n",
      "1530: [discriminator loss: 0.672787, acc: 0.500000] [adversarial loss: 0.860015, acc: 0.250000]\n",
      "1531: [discriminator loss: 0.613265, acc: 0.671875] [adversarial loss: 0.775882, acc: 0.406250]\n",
      "1532: [discriminator loss: 0.656174, acc: 0.671875] [adversarial loss: 0.944772, acc: 0.250000]\n",
      "1533: [discriminator loss: 0.631880, acc: 0.648438] [adversarial loss: 0.644936, acc: 0.671875]\n",
      "1534: [discriminator loss: 0.642752, acc: 0.585938] [adversarial loss: 1.055038, acc: 0.078125]\n",
      "1535: [discriminator loss: 0.693621, acc: 0.523438] [adversarial loss: 0.543580, acc: 0.828125]\n",
      "1536: [discriminator loss: 0.681668, acc: 0.585938] [adversarial loss: 1.155041, acc: 0.125000]\n",
      "1537: [discriminator loss: 0.728757, acc: 0.562500] [adversarial loss: 0.636754, acc: 0.562500]\n",
      "1538: [discriminator loss: 0.674190, acc: 0.585938] [adversarial loss: 1.052219, acc: 0.156250]\n",
      "1539: [discriminator loss: 0.646657, acc: 0.617188] [adversarial loss: 0.660056, acc: 0.609375]\n",
      "1540: [discriminator loss: 0.695732, acc: 0.570312] [adversarial loss: 0.965028, acc: 0.125000]\n",
      "1541: [discriminator loss: 0.649504, acc: 0.578125] [adversarial loss: 0.707293, acc: 0.609375]\n",
      "1542: [discriminator loss: 0.645870, acc: 0.656250] [adversarial loss: 1.084770, acc: 0.046875]\n",
      "1543: [discriminator loss: 0.667516, acc: 0.609375] [adversarial loss: 0.605856, acc: 0.718750]\n",
      "1544: [discriminator loss: 0.692124, acc: 0.562500] [adversarial loss: 1.148559, acc: 0.062500]\n",
      "1545: [discriminator loss: 0.655046, acc: 0.625000] [adversarial loss: 0.609613, acc: 0.703125]\n",
      "1546: [discriminator loss: 0.682671, acc: 0.601562] [adversarial loss: 0.986812, acc: 0.093750]\n",
      "1547: [discriminator loss: 0.660677, acc: 0.585938] [adversarial loss: 0.607351, acc: 0.718750]\n",
      "1548: [discriminator loss: 0.637985, acc: 0.640625] [adversarial loss: 1.096117, acc: 0.109375]\n",
      "1549: [discriminator loss: 0.661997, acc: 0.593750] [adversarial loss: 0.648956, acc: 0.718750]\n",
      "1550: [discriminator loss: 0.688608, acc: 0.570312] [adversarial loss: 1.102887, acc: 0.140625]\n",
      "1551: [discriminator loss: 0.681033, acc: 0.625000] [adversarial loss: 0.666963, acc: 0.562500]\n",
      "1552: [discriminator loss: 0.702727, acc: 0.562500] [adversarial loss: 0.921580, acc: 0.250000]\n",
      "1553: [discriminator loss: 0.652109, acc: 0.601562] [adversarial loss: 0.810247, acc: 0.359375]\n",
      "1554: [discriminator loss: 0.655919, acc: 0.593750] [adversarial loss: 0.776876, acc: 0.343750]\n",
      "1555: [discriminator loss: 0.630455, acc: 0.609375] [adversarial loss: 0.715826, acc: 0.609375]\n",
      "1556: [discriminator loss: 0.675901, acc: 0.585938] [adversarial loss: 0.838580, acc: 0.250000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1557: [discriminator loss: 0.631879, acc: 0.664062] [adversarial loss: 0.793622, acc: 0.437500]\n",
      "1558: [discriminator loss: 0.677844, acc: 0.640625] [adversarial loss: 1.021479, acc: 0.187500]\n",
      "1559: [discriminator loss: 0.670176, acc: 0.562500] [adversarial loss: 0.615968, acc: 0.609375]\n",
      "1560: [discriminator loss: 0.681361, acc: 0.578125] [adversarial loss: 1.085216, acc: 0.125000]\n",
      "1561: [discriminator loss: 0.716212, acc: 0.500000] [adversarial loss: 0.534402, acc: 0.859375]\n",
      "1562: [discriminator loss: 0.684158, acc: 0.531250] [adversarial loss: 1.021414, acc: 0.156250]\n",
      "1563: [discriminator loss: 0.636015, acc: 0.625000] [adversarial loss: 0.662741, acc: 0.625000]\n",
      "1564: [discriminator loss: 0.673583, acc: 0.593750] [adversarial loss: 0.997565, acc: 0.140625]\n",
      "1565: [discriminator loss: 0.691823, acc: 0.523438] [adversarial loss: 0.661702, acc: 0.625000]\n",
      "1566: [discriminator loss: 0.706958, acc: 0.554688] [adversarial loss: 1.072605, acc: 0.125000]\n",
      "1567: [discriminator loss: 0.658172, acc: 0.578125] [adversarial loss: 0.715633, acc: 0.468750]\n",
      "1568: [discriminator loss: 0.656260, acc: 0.601562] [adversarial loss: 1.076168, acc: 0.078125]\n",
      "1569: [discriminator loss: 0.673789, acc: 0.554688] [adversarial loss: 0.652791, acc: 0.671875]\n",
      "1570: [discriminator loss: 0.693267, acc: 0.617188] [adversarial loss: 1.042196, acc: 0.125000]\n",
      "1571: [discriminator loss: 0.681303, acc: 0.601562] [adversarial loss: 0.568557, acc: 0.765625]\n",
      "1572: [discriminator loss: 0.630068, acc: 0.632812] [adversarial loss: 1.048328, acc: 0.125000]\n",
      "1573: [discriminator loss: 0.623828, acc: 0.671875] [adversarial loss: 0.757746, acc: 0.500000]\n",
      "1574: [discriminator loss: 0.662264, acc: 0.601562] [adversarial loss: 1.124013, acc: 0.093750]\n",
      "1575: [discriminator loss: 0.708649, acc: 0.546875] [adversarial loss: 0.673044, acc: 0.546875]\n",
      "1576: [discriminator loss: 0.664874, acc: 0.609375] [adversarial loss: 0.936101, acc: 0.140625]\n",
      "1577: [discriminator loss: 0.667313, acc: 0.609375] [adversarial loss: 0.640942, acc: 0.578125]\n",
      "1578: [discriminator loss: 0.652232, acc: 0.632812] [adversarial loss: 0.945496, acc: 0.156250]\n",
      "1579: [discriminator loss: 0.656536, acc: 0.632812] [adversarial loss: 0.720400, acc: 0.515625]\n",
      "1580: [discriminator loss: 0.667041, acc: 0.617188] [adversarial loss: 0.884265, acc: 0.296875]\n",
      "1581: [discriminator loss: 0.603765, acc: 0.710938] [adversarial loss: 0.728267, acc: 0.531250]\n",
      "1582: [discriminator loss: 0.671625, acc: 0.546875] [adversarial loss: 0.858869, acc: 0.218750]\n",
      "1583: [discriminator loss: 0.621079, acc: 0.710938] [adversarial loss: 0.819654, acc: 0.359375]\n",
      "1584: [discriminator loss: 0.668709, acc: 0.562500] [adversarial loss: 0.885155, acc: 0.234375]\n",
      "1585: [discriminator loss: 0.650953, acc: 0.617188] [adversarial loss: 0.707007, acc: 0.500000]\n",
      "1586: [discriminator loss: 0.677832, acc: 0.507812] [adversarial loss: 0.880184, acc: 0.218750]\n",
      "1587: [discriminator loss: 0.664757, acc: 0.593750] [adversarial loss: 0.662596, acc: 0.609375]\n",
      "1588: [discriminator loss: 0.606691, acc: 0.679688] [adversarial loss: 1.021534, acc: 0.234375]\n",
      "1589: [discriminator loss: 0.747307, acc: 0.531250] [adversarial loss: 0.731181, acc: 0.609375]\n",
      "1590: [discriminator loss: 0.696813, acc: 0.546875] [adversarial loss: 1.065454, acc: 0.031250]\n",
      "1591: [discriminator loss: 0.702788, acc: 0.554688] [adversarial loss: 0.514101, acc: 0.890625]\n",
      "1592: [discriminator loss: 0.723993, acc: 0.554688] [adversarial loss: 1.145635, acc: 0.046875]\n",
      "1593: [discriminator loss: 0.696923, acc: 0.578125] [adversarial loss: 0.642792, acc: 0.625000]\n",
      "1594: [discriminator loss: 0.650436, acc: 0.609375] [adversarial loss: 1.029736, acc: 0.109375]\n",
      "1595: [discriminator loss: 0.672195, acc: 0.585938] [adversarial loss: 0.680770, acc: 0.625000]\n",
      "1596: [discriminator loss: 0.651189, acc: 0.601562] [adversarial loss: 0.954884, acc: 0.250000]\n",
      "1597: [discriminator loss: 0.669192, acc: 0.578125] [adversarial loss: 0.764233, acc: 0.406250]\n",
      "1598: [discriminator loss: 0.649459, acc: 0.617188] [adversarial loss: 0.910873, acc: 0.171875]\n",
      "1599: [discriminator loss: 0.658268, acc: 0.593750] [adversarial loss: 0.766550, acc: 0.421875]\n",
      "1600: [discriminator loss: 0.641861, acc: 0.617188] [adversarial loss: 0.822255, acc: 0.390625]\n",
      "1601: [discriminator loss: 0.660329, acc: 0.601562] [adversarial loss: 0.808176, acc: 0.296875]\n",
      "1602: [discriminator loss: 0.659332, acc: 0.601562] [adversarial loss: 0.861915, acc: 0.250000]\n",
      "1603: [discriminator loss: 0.642559, acc: 0.640625] [adversarial loss: 0.877644, acc: 0.250000]\n",
      "1604: [discriminator loss: 0.677542, acc: 0.570312] [adversarial loss: 0.693294, acc: 0.500000]\n",
      "1605: [discriminator loss: 0.664627, acc: 0.617188] [adversarial loss: 1.120076, acc: 0.109375]\n",
      "1606: [discriminator loss: 0.666120, acc: 0.562500] [adversarial loss: 0.514323, acc: 0.750000]\n",
      "1607: [discriminator loss: 0.684614, acc: 0.546875] [adversarial loss: 1.168622, acc: 0.078125]\n",
      "1608: [discriminator loss: 0.632345, acc: 0.609375] [adversarial loss: 0.722444, acc: 0.578125]\n",
      "1609: [discriminator loss: 0.693925, acc: 0.578125] [adversarial loss: 1.134077, acc: 0.125000]\n",
      "1610: [discriminator loss: 0.726442, acc: 0.562500] [adversarial loss: 0.647575, acc: 0.546875]\n",
      "1611: [discriminator loss: 0.675457, acc: 0.562500] [adversarial loss: 1.064484, acc: 0.015625]\n",
      "1612: [discriminator loss: 0.661406, acc: 0.578125] [adversarial loss: 0.656992, acc: 0.578125]\n",
      "1613: [discriminator loss: 0.656686, acc: 0.570312] [adversarial loss: 0.951530, acc: 0.156250]\n",
      "1614: [discriminator loss: 0.649106, acc: 0.640625] [adversarial loss: 0.599981, acc: 0.656250]\n",
      "1615: [discriminator loss: 0.676505, acc: 0.632812] [adversarial loss: 0.875630, acc: 0.125000]\n",
      "1616: [discriminator loss: 0.635177, acc: 0.664062] [adversarial loss: 0.682870, acc: 0.562500]\n",
      "1617: [discriminator loss: 0.675677, acc: 0.578125] [adversarial loss: 0.975072, acc: 0.140625]\n",
      "1618: [discriminator loss: 0.640770, acc: 0.601562] [adversarial loss: 0.800916, acc: 0.343750]\n",
      "1619: [discriminator loss: 0.690138, acc: 0.539062] [adversarial loss: 1.008028, acc: 0.125000]\n",
      "1620: [discriminator loss: 0.672158, acc: 0.617188] [adversarial loss: 0.659105, acc: 0.515625]\n",
      "1621: [discriminator loss: 0.669196, acc: 0.585938] [adversarial loss: 1.052014, acc: 0.093750]\n",
      "1622: [discriminator loss: 0.661540, acc: 0.601562] [adversarial loss: 0.666235, acc: 0.593750]\n",
      "1623: [discriminator loss: 0.690194, acc: 0.578125] [adversarial loss: 1.035244, acc: 0.156250]\n",
      "1624: [discriminator loss: 0.715186, acc: 0.570312] [adversarial loss: 0.607342, acc: 0.703125]\n",
      "1625: [discriminator loss: 0.691024, acc: 0.515625] [adversarial loss: 0.995864, acc: 0.109375]\n",
      "1626: [discriminator loss: 0.641470, acc: 0.656250] [adversarial loss: 0.631423, acc: 0.593750]\n",
      "1627: [discriminator loss: 0.648084, acc: 0.593750] [adversarial loss: 0.976702, acc: 0.125000]\n",
      "1628: [discriminator loss: 0.661814, acc: 0.562500] [adversarial loss: 0.690866, acc: 0.578125]\n",
      "1629: [discriminator loss: 0.660233, acc: 0.609375] [adversarial loss: 1.019989, acc: 0.218750]\n",
      "1630: [discriminator loss: 0.704795, acc: 0.554688] [adversarial loss: 0.688391, acc: 0.562500]\n",
      "1631: [discriminator loss: 0.661081, acc: 0.585938] [adversarial loss: 0.924919, acc: 0.250000]\n",
      "1632: [discriminator loss: 0.654950, acc: 0.601562] [adversarial loss: 0.834556, acc: 0.359375]\n",
      "1633: [discriminator loss: 0.679533, acc: 0.578125] [adversarial loss: 0.774520, acc: 0.359375]\n",
      "1634: [discriminator loss: 0.622534, acc: 0.695312] [adversarial loss: 0.822346, acc: 0.250000]\n",
      "1635: [discriminator loss: 0.634585, acc: 0.570312] [adversarial loss: 0.641724, acc: 0.625000]\n",
      "1636: [discriminator loss: 0.638152, acc: 0.632812] [adversarial loss: 0.851525, acc: 0.390625]\n",
      "1637: [discriminator loss: 0.673515, acc: 0.585938] [adversarial loss: 0.678413, acc: 0.578125]\n",
      "1638: [discriminator loss: 0.628411, acc: 0.640625] [adversarial loss: 0.959290, acc: 0.171875]\n",
      "1639: [discriminator loss: 0.711734, acc: 0.523438] [adversarial loss: 0.600289, acc: 0.671875]\n",
      "1640: [discriminator loss: 0.678073, acc: 0.578125] [adversarial loss: 1.172563, acc: 0.109375]\n",
      "1641: [discriminator loss: 0.679792, acc: 0.578125] [adversarial loss: 0.544176, acc: 0.734375]\n",
      "1642: [discriminator loss: 0.696138, acc: 0.570312] [adversarial loss: 1.052732, acc: 0.218750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1643: [discriminator loss: 0.705777, acc: 0.554688] [adversarial loss: 0.725502, acc: 0.421875]\n",
      "1644: [discriminator loss: 0.674680, acc: 0.562500] [adversarial loss: 0.861152, acc: 0.265625]\n",
      "1645: [discriminator loss: 0.656913, acc: 0.625000] [adversarial loss: 0.711979, acc: 0.515625]\n",
      "1646: [discriminator loss: 0.665087, acc: 0.601562] [adversarial loss: 0.855890, acc: 0.265625]\n",
      "1647: [discriminator loss: 0.667084, acc: 0.601562] [adversarial loss: 0.840741, acc: 0.296875]\n",
      "1648: [discriminator loss: 0.652854, acc: 0.570312] [adversarial loss: 0.852554, acc: 0.312500]\n",
      "1649: [discriminator loss: 0.679904, acc: 0.570312] [adversarial loss: 0.761473, acc: 0.421875]\n",
      "1650: [discriminator loss: 0.644680, acc: 0.609375] [adversarial loss: 0.984607, acc: 0.109375]\n",
      "1651: [discriminator loss: 0.655131, acc: 0.617188] [adversarial loss: 0.582461, acc: 0.718750]\n",
      "1652: [discriminator loss: 0.669963, acc: 0.554688] [adversarial loss: 0.891975, acc: 0.203125]\n",
      "1653: [discriminator loss: 0.663168, acc: 0.570312] [adversarial loss: 0.693696, acc: 0.578125]\n",
      "1654: [discriminator loss: 0.681402, acc: 0.570312] [adversarial loss: 1.079424, acc: 0.046875]\n",
      "1655: [discriminator loss: 0.674525, acc: 0.593750] [adversarial loss: 0.551264, acc: 0.875000]\n",
      "1656: [discriminator loss: 0.677709, acc: 0.585938] [adversarial loss: 0.968294, acc: 0.140625]\n",
      "1657: [discriminator loss: 0.691172, acc: 0.531250] [adversarial loss: 0.534849, acc: 0.812500]\n",
      "1658: [discriminator loss: 0.675170, acc: 0.507812] [adversarial loss: 1.027844, acc: 0.187500]\n",
      "1659: [discriminator loss: 0.678558, acc: 0.601562] [adversarial loss: 0.646593, acc: 0.656250]\n",
      "1660: [discriminator loss: 0.746128, acc: 0.523438] [adversarial loss: 1.051962, acc: 0.109375]\n",
      "1661: [discriminator loss: 0.669434, acc: 0.593750] [adversarial loss: 0.677131, acc: 0.546875]\n",
      "1662: [discriminator loss: 0.651649, acc: 0.585938] [adversarial loss: 0.906741, acc: 0.234375]\n",
      "1663: [discriminator loss: 0.664395, acc: 0.546875] [adversarial loss: 0.637946, acc: 0.656250]\n",
      "1664: [discriminator loss: 0.646546, acc: 0.617188] [adversarial loss: 1.109575, acc: 0.140625]\n",
      "1665: [discriminator loss: 0.697800, acc: 0.562500] [adversarial loss: 0.669332, acc: 0.578125]\n",
      "1666: [discriminator loss: 0.650627, acc: 0.609375] [adversarial loss: 0.837862, acc: 0.406250]\n",
      "1667: [discriminator loss: 0.660149, acc: 0.578125] [adversarial loss: 0.756685, acc: 0.468750]\n",
      "1668: [discriminator loss: 0.629825, acc: 0.664062] [adversarial loss: 0.863169, acc: 0.234375]\n",
      "1669: [discriminator loss: 0.636371, acc: 0.640625] [adversarial loss: 0.677829, acc: 0.578125]\n",
      "1670: [discriminator loss: 0.652582, acc: 0.593750] [adversarial loss: 0.872738, acc: 0.296875]\n",
      "1671: [discriminator loss: 0.641365, acc: 0.601562] [adversarial loss: 0.720173, acc: 0.531250]\n",
      "1672: [discriminator loss: 0.681510, acc: 0.539062] [adversarial loss: 0.925228, acc: 0.250000]\n",
      "1673: [discriminator loss: 0.705645, acc: 0.546875] [adversarial loss: 0.759124, acc: 0.421875]\n",
      "1674: [discriminator loss: 0.658677, acc: 0.617188] [adversarial loss: 0.836033, acc: 0.250000]\n",
      "1675: [discriminator loss: 0.679182, acc: 0.609375] [adversarial loss: 0.789199, acc: 0.343750]\n",
      "1676: [discriminator loss: 0.692235, acc: 0.554688] [adversarial loss: 0.710198, acc: 0.453125]\n",
      "1677: [discriminator loss: 0.630192, acc: 0.656250] [adversarial loss: 0.954854, acc: 0.203125]\n",
      "1678: [discriminator loss: 0.669482, acc: 0.578125] [adversarial loss: 0.516072, acc: 0.781250]\n",
      "1679: [discriminator loss: 0.711352, acc: 0.523438] [adversarial loss: 1.102860, acc: 0.093750]\n",
      "1680: [discriminator loss: 0.702275, acc: 0.601562] [adversarial loss: 0.633309, acc: 0.640625]\n",
      "1681: [discriminator loss: 0.674687, acc: 0.585938] [adversarial loss: 0.807804, acc: 0.375000]\n",
      "1682: [discriminator loss: 0.635572, acc: 0.679688] [adversarial loss: 0.822428, acc: 0.312500]\n",
      "1683: [discriminator loss: 0.650245, acc: 0.640625] [adversarial loss: 0.704325, acc: 0.500000]\n",
      "1684: [discriminator loss: 0.642314, acc: 0.585938] [adversarial loss: 0.973464, acc: 0.187500]\n",
      "1685: [discriminator loss: 0.629734, acc: 0.664062] [adversarial loss: 0.809310, acc: 0.390625]\n",
      "1686: [discriminator loss: 0.643522, acc: 0.601562] [adversarial loss: 0.915831, acc: 0.250000]\n",
      "1687: [discriminator loss: 0.692027, acc: 0.554688] [adversarial loss: 0.663117, acc: 0.656250]\n",
      "1688: [discriminator loss: 0.665198, acc: 0.601562] [adversarial loss: 1.093303, acc: 0.109375]\n",
      "1689: [discriminator loss: 0.705426, acc: 0.593750] [adversarial loss: 0.681725, acc: 0.546875]\n",
      "1690: [discriminator loss: 0.673246, acc: 0.578125] [adversarial loss: 1.036714, acc: 0.093750]\n",
      "1691: [discriminator loss: 0.661873, acc: 0.601562] [adversarial loss: 0.621359, acc: 0.703125]\n",
      "1692: [discriminator loss: 0.678930, acc: 0.554688] [adversarial loss: 1.085701, acc: 0.109375]\n",
      "1693: [discriminator loss: 0.682316, acc: 0.585938] [adversarial loss: 0.690784, acc: 0.562500]\n",
      "1694: [discriminator loss: 0.675931, acc: 0.554688] [adversarial loss: 1.059836, acc: 0.078125]\n",
      "1695: [discriminator loss: 0.659296, acc: 0.570312] [adversarial loss: 0.727386, acc: 0.484375]\n",
      "1696: [discriminator loss: 0.677007, acc: 0.570312] [adversarial loss: 0.844506, acc: 0.296875]\n",
      "1697: [discriminator loss: 0.716974, acc: 0.476562] [adversarial loss: 0.906037, acc: 0.187500]\n",
      "1698: [discriminator loss: 0.652791, acc: 0.640625] [adversarial loss: 0.839073, acc: 0.421875]\n",
      "1699: [discriminator loss: 0.660338, acc: 0.578125] [adversarial loss: 0.835759, acc: 0.203125]\n",
      "1700: [discriminator loss: 0.625701, acc: 0.679688] [adversarial loss: 0.804083, acc: 0.343750]\n",
      "1701: [discriminator loss: 0.650760, acc: 0.625000] [adversarial loss: 1.035466, acc: 0.093750]\n",
      "1702: [discriminator loss: 0.664921, acc: 0.617188] [adversarial loss: 0.562274, acc: 0.750000]\n",
      "1703: [discriminator loss: 0.699415, acc: 0.546875] [adversarial loss: 1.149258, acc: 0.046875]\n",
      "1704: [discriminator loss: 0.653592, acc: 0.601562] [adversarial loss: 0.628391, acc: 0.687500]\n",
      "1705: [discriminator loss: 0.664836, acc: 0.609375] [adversarial loss: 0.992811, acc: 0.218750]\n",
      "1706: [discriminator loss: 0.664079, acc: 0.570312] [adversarial loss: 0.754074, acc: 0.453125]\n",
      "1707: [discriminator loss: 0.634487, acc: 0.656250] [adversarial loss: 0.920712, acc: 0.187500]\n",
      "1708: [discriminator loss: 0.693972, acc: 0.539062] [adversarial loss: 0.615935, acc: 0.671875]\n",
      "1709: [discriminator loss: 0.666569, acc: 0.578125] [adversarial loss: 1.041486, acc: 0.171875]\n",
      "1710: [discriminator loss: 0.695324, acc: 0.570312] [adversarial loss: 0.645478, acc: 0.640625]\n",
      "1711: [discriminator loss: 0.625190, acc: 0.632812] [adversarial loss: 1.077355, acc: 0.171875]\n",
      "1712: [discriminator loss: 0.709871, acc: 0.593750] [adversarial loss: 0.727829, acc: 0.500000]\n",
      "1713: [discriminator loss: 0.658146, acc: 0.578125] [adversarial loss: 0.903196, acc: 0.218750]\n",
      "1714: [discriminator loss: 0.691065, acc: 0.609375] [adversarial loss: 0.822530, acc: 0.406250]\n",
      "1715: [discriminator loss: 0.645616, acc: 0.648438] [adversarial loss: 0.681359, acc: 0.515625]\n",
      "1716: [discriminator loss: 0.634731, acc: 0.625000] [adversarial loss: 0.944895, acc: 0.234375]\n",
      "1717: [discriminator loss: 0.693798, acc: 0.562500] [adversarial loss: 0.754634, acc: 0.390625]\n",
      "1718: [discriminator loss: 0.696363, acc: 0.578125] [adversarial loss: 0.758373, acc: 0.375000]\n",
      "1719: [discriminator loss: 0.701846, acc: 0.531250] [adversarial loss: 0.798912, acc: 0.328125]\n",
      "1720: [discriminator loss: 0.623082, acc: 0.679688] [adversarial loss: 0.809347, acc: 0.390625]\n",
      "1721: [discriminator loss: 0.644711, acc: 0.656250] [adversarial loss: 0.907276, acc: 0.234375]\n",
      "1722: [discriminator loss: 0.622226, acc: 0.664062] [adversarial loss: 0.682908, acc: 0.531250]\n",
      "1723: [discriminator loss: 0.671645, acc: 0.625000] [adversarial loss: 0.833718, acc: 0.296875]\n",
      "1724: [discriminator loss: 0.635842, acc: 0.664062] [adversarial loss: 0.751859, acc: 0.453125]\n",
      "1725: [discriminator loss: 0.681284, acc: 0.546875] [adversarial loss: 0.843400, acc: 0.281250]\n",
      "1726: [discriminator loss: 0.633190, acc: 0.617188] [adversarial loss: 0.832361, acc: 0.343750]\n",
      "1727: [discriminator loss: 0.676533, acc: 0.570312] [adversarial loss: 0.803761, acc: 0.312500]\n",
      "1728: [discriminator loss: 0.648262, acc: 0.640625] [adversarial loss: 0.836931, acc: 0.343750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1729: [discriminator loss: 0.665697, acc: 0.601562] [adversarial loss: 0.688892, acc: 0.578125]\n",
      "1730: [discriminator loss: 0.674412, acc: 0.578125] [adversarial loss: 0.799441, acc: 0.312500]\n",
      "1731: [discriminator loss: 0.609131, acc: 0.703125] [adversarial loss: 0.916733, acc: 0.281250]\n",
      "1732: [discriminator loss: 0.728030, acc: 0.523438] [adversarial loss: 0.591640, acc: 0.687500]\n",
      "1733: [discriminator loss: 0.693140, acc: 0.562500] [adversarial loss: 1.412010, acc: 0.000000]\n",
      "1734: [discriminator loss: 0.781396, acc: 0.507812] [adversarial loss: 0.433885, acc: 0.906250]\n",
      "1735: [discriminator loss: 0.753631, acc: 0.515625] [adversarial loss: 1.205364, acc: 0.015625]\n",
      "1736: [discriminator loss: 0.674915, acc: 0.609375] [adversarial loss: 0.554229, acc: 0.765625]\n",
      "1737: [discriminator loss: 0.649873, acc: 0.585938] [adversarial loss: 0.923343, acc: 0.187500]\n",
      "1738: [discriminator loss: 0.673216, acc: 0.593750] [adversarial loss: 0.699296, acc: 0.546875]\n",
      "1739: [discriminator loss: 0.684833, acc: 0.523438] [adversarial loss: 0.796055, acc: 0.359375]\n",
      "1740: [discriminator loss: 0.636085, acc: 0.640625] [adversarial loss: 0.889766, acc: 0.265625]\n",
      "1741: [discriminator loss: 0.679459, acc: 0.570312] [adversarial loss: 0.735253, acc: 0.453125]\n",
      "1742: [discriminator loss: 0.663973, acc: 0.562500] [adversarial loss: 0.849933, acc: 0.265625]\n",
      "1743: [discriminator loss: 0.650157, acc: 0.632812] [adversarial loss: 0.901007, acc: 0.218750]\n",
      "1744: [discriminator loss: 0.687777, acc: 0.562500] [adversarial loss: 0.762145, acc: 0.453125]\n",
      "1745: [discriminator loss: 0.677379, acc: 0.617188] [adversarial loss: 0.966982, acc: 0.125000]\n",
      "1746: [discriminator loss: 0.683019, acc: 0.601562] [adversarial loss: 0.578958, acc: 0.812500]\n",
      "1747: [discriminator loss: 0.712223, acc: 0.523438] [adversarial loss: 1.136767, acc: 0.078125]\n",
      "1748: [discriminator loss: 0.717476, acc: 0.531250] [adversarial loss: 0.612333, acc: 0.734375]\n",
      "1749: [discriminator loss: 0.660578, acc: 0.562500] [adversarial loss: 0.960167, acc: 0.156250]\n",
      "1750: [discriminator loss: 0.661500, acc: 0.640625] [adversarial loss: 0.699927, acc: 0.562500]\n",
      "1751: [discriminator loss: 0.700083, acc: 0.609375] [adversarial loss: 0.904030, acc: 0.140625]\n",
      "1752: [discriminator loss: 0.678705, acc: 0.570312] [adversarial loss: 0.757001, acc: 0.390625]\n",
      "1753: [discriminator loss: 0.674981, acc: 0.554688] [adversarial loss: 0.875203, acc: 0.218750]\n",
      "1754: [discriminator loss: 0.642220, acc: 0.640625] [adversarial loss: 0.669348, acc: 0.546875]\n",
      "1755: [discriminator loss: 0.687405, acc: 0.578125] [adversarial loss: 0.742340, acc: 0.421875]\n",
      "1756: [discriminator loss: 0.634211, acc: 0.617188] [adversarial loss: 1.023444, acc: 0.156250]\n",
      "1757: [discriminator loss: 0.643584, acc: 0.632812] [adversarial loss: 0.797393, acc: 0.421875]\n",
      "1758: [discriminator loss: 0.664559, acc: 0.601562] [adversarial loss: 0.762823, acc: 0.453125]\n",
      "1759: [discriminator loss: 0.705013, acc: 0.539062] [adversarial loss: 0.749784, acc: 0.375000]\n",
      "1760: [discriminator loss: 0.659154, acc: 0.617188] [adversarial loss: 0.914196, acc: 0.093750]\n",
      "1761: [discriminator loss: 0.688048, acc: 0.523438] [adversarial loss: 0.921147, acc: 0.156250]\n",
      "1762: [discriminator loss: 0.633438, acc: 0.664062] [adversarial loss: 0.630087, acc: 0.656250]\n",
      "1763: [discriminator loss: 0.687041, acc: 0.546875] [adversarial loss: 1.014894, acc: 0.125000]\n",
      "1764: [discriminator loss: 0.677830, acc: 0.601562] [adversarial loss: 0.628313, acc: 0.687500]\n",
      "1765: [discriminator loss: 0.666603, acc: 0.570312] [adversarial loss: 1.001079, acc: 0.156250]\n",
      "1766: [discriminator loss: 0.666999, acc: 0.593750] [adversarial loss: 0.694157, acc: 0.546875]\n",
      "1767: [discriminator loss: 0.668356, acc: 0.593750] [adversarial loss: 0.976710, acc: 0.140625]\n",
      "1768: [discriminator loss: 0.660695, acc: 0.601562] [adversarial loss: 0.557755, acc: 0.796875]\n",
      "1769: [discriminator loss: 0.714614, acc: 0.515625] [adversarial loss: 1.135528, acc: 0.031250]\n",
      "1770: [discriminator loss: 0.688778, acc: 0.554688] [adversarial loss: 0.638302, acc: 0.625000]\n",
      "1771: [discriminator loss: 0.728476, acc: 0.523438] [adversarial loss: 1.051273, acc: 0.078125]\n",
      "1772: [discriminator loss: 0.712863, acc: 0.554688] [adversarial loss: 0.564882, acc: 0.812500]\n",
      "1773: [discriminator loss: 0.669370, acc: 0.539062] [adversarial loss: 1.053672, acc: 0.109375]\n",
      "1774: [discriminator loss: 0.692458, acc: 0.562500] [adversarial loss: 0.709870, acc: 0.515625]\n",
      "1775: [discriminator loss: 0.661403, acc: 0.593750] [adversarial loss: 0.856294, acc: 0.250000]\n",
      "1776: [discriminator loss: 0.630551, acc: 0.671875] [adversarial loss: 0.733896, acc: 0.546875]\n",
      "1777: [discriminator loss: 0.643074, acc: 0.656250] [adversarial loss: 0.849770, acc: 0.265625]\n",
      "1778: [discriminator loss: 0.647577, acc: 0.664062] [adversarial loss: 0.698444, acc: 0.531250]\n",
      "1779: [discriminator loss: 0.649630, acc: 0.593750] [adversarial loss: 0.938359, acc: 0.140625]\n",
      "1780: [discriminator loss: 0.647456, acc: 0.585938] [adversarial loss: 0.700035, acc: 0.578125]\n",
      "1781: [discriminator loss: 0.674850, acc: 0.617188] [adversarial loss: 0.966518, acc: 0.140625]\n",
      "1782: [discriminator loss: 0.717593, acc: 0.523438] [adversarial loss: 0.581336, acc: 0.796875]\n",
      "1783: [discriminator loss: 0.672886, acc: 0.578125] [adversarial loss: 1.203606, acc: 0.031250]\n",
      "1784: [discriminator loss: 0.730221, acc: 0.523438] [adversarial loss: 0.632115, acc: 0.671875]\n",
      "1785: [discriminator loss: 0.637004, acc: 0.609375] [adversarial loss: 0.978365, acc: 0.078125]\n",
      "1786: [discriminator loss: 0.651672, acc: 0.609375] [adversarial loss: 0.711848, acc: 0.468750]\n",
      "1787: [discriminator loss: 0.688698, acc: 0.554688] [adversarial loss: 0.838185, acc: 0.312500]\n",
      "1788: [discriminator loss: 0.677992, acc: 0.515625] [adversarial loss: 0.684493, acc: 0.562500]\n",
      "1789: [discriminator loss: 0.690764, acc: 0.570312] [adversarial loss: 0.855818, acc: 0.250000]\n",
      "1790: [discriminator loss: 0.629705, acc: 0.632812] [adversarial loss: 0.763758, acc: 0.437500]\n",
      "1791: [discriminator loss: 0.648745, acc: 0.578125] [adversarial loss: 0.955073, acc: 0.296875]\n",
      "1792: [discriminator loss: 0.706164, acc: 0.492188] [adversarial loss: 0.697907, acc: 0.484375]\n",
      "1793: [discriminator loss: 0.663564, acc: 0.585938] [adversarial loss: 0.860471, acc: 0.312500]\n",
      "1794: [discriminator loss: 0.691515, acc: 0.578125] [adversarial loss: 0.710847, acc: 0.500000]\n",
      "1795: [discriminator loss: 0.666392, acc: 0.570312] [adversarial loss: 0.906228, acc: 0.296875]\n",
      "1796: [discriminator loss: 0.679770, acc: 0.609375] [adversarial loss: 0.640818, acc: 0.640625]\n",
      "1797: [discriminator loss: 0.678007, acc: 0.578125] [adversarial loss: 0.898004, acc: 0.171875]\n",
      "1798: [discriminator loss: 0.678012, acc: 0.570312] [adversarial loss: 0.691816, acc: 0.515625]\n",
      "1799: [discriminator loss: 0.702937, acc: 0.523438] [adversarial loss: 0.950246, acc: 0.156250]\n",
      "1800: [discriminator loss: 0.677723, acc: 0.523438] [adversarial loss: 0.676936, acc: 0.593750]\n",
      "1801: [discriminator loss: 0.703830, acc: 0.562500] [adversarial loss: 1.045995, acc: 0.109375]\n",
      "1802: [discriminator loss: 0.666398, acc: 0.578125] [adversarial loss: 0.601410, acc: 0.718750]\n",
      "1803: [discriminator loss: 0.704813, acc: 0.507812] [adversarial loss: 0.952299, acc: 0.171875]\n",
      "1804: [discriminator loss: 0.711974, acc: 0.554688] [adversarial loss: 0.608291, acc: 0.703125]\n",
      "1805: [discriminator loss: 0.706098, acc: 0.515625] [adversarial loss: 1.007728, acc: 0.109375]\n",
      "1806: [discriminator loss: 0.714087, acc: 0.523438] [adversarial loss: 0.638859, acc: 0.625000]\n",
      "1807: [discriminator loss: 0.693917, acc: 0.531250] [adversarial loss: 0.754925, acc: 0.437500]\n",
      "1808: [discriminator loss: 0.653944, acc: 0.632812] [adversarial loss: 0.795128, acc: 0.406250]\n",
      "1809: [discriminator loss: 0.660897, acc: 0.585938] [adversarial loss: 0.779292, acc: 0.343750]\n",
      "1810: [discriminator loss: 0.650254, acc: 0.625000] [adversarial loss: 0.706361, acc: 0.578125]\n",
      "1811: [discriminator loss: 0.690398, acc: 0.562500] [adversarial loss: 0.879712, acc: 0.250000]\n",
      "1812: [discriminator loss: 0.676490, acc: 0.632812] [adversarial loss: 0.747963, acc: 0.390625]\n",
      "1813: [discriminator loss: 0.629937, acc: 0.671875] [adversarial loss: 0.920650, acc: 0.187500]\n",
      "1814: [discriminator loss: 0.693716, acc: 0.554688] [adversarial loss: 0.621718, acc: 0.703125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1815: [discriminator loss: 0.708801, acc: 0.539062] [adversarial loss: 0.990648, acc: 0.140625]\n",
      "1816: [discriminator loss: 0.721744, acc: 0.539062] [adversarial loss: 0.603022, acc: 0.609375]\n",
      "1817: [discriminator loss: 0.679260, acc: 0.570312] [adversarial loss: 0.945580, acc: 0.046875]\n",
      "1818: [discriminator loss: 0.680838, acc: 0.570312] [adversarial loss: 0.541297, acc: 0.765625]\n",
      "1819: [discriminator loss: 0.676973, acc: 0.562500] [adversarial loss: 1.034972, acc: 0.140625]\n",
      "1820: [discriminator loss: 0.706337, acc: 0.593750] [adversarial loss: 0.684196, acc: 0.578125]\n",
      "1821: [discriminator loss: 0.677890, acc: 0.546875] [adversarial loss: 0.954214, acc: 0.140625]\n",
      "1822: [discriminator loss: 0.656978, acc: 0.617188] [adversarial loss: 0.691381, acc: 0.468750]\n",
      "1823: [discriminator loss: 0.712939, acc: 0.492188] [adversarial loss: 0.747074, acc: 0.468750]\n",
      "1824: [discriminator loss: 0.673049, acc: 0.578125] [adversarial loss: 0.879090, acc: 0.093750]\n",
      "1825: [discriminator loss: 0.659567, acc: 0.632812] [adversarial loss: 0.659033, acc: 0.656250]\n",
      "1826: [discriminator loss: 0.685491, acc: 0.578125] [adversarial loss: 0.810120, acc: 0.234375]\n",
      "1827: [discriminator loss: 0.622630, acc: 0.671875] [adversarial loss: 0.766655, acc: 0.343750]\n",
      "1828: [discriminator loss: 0.643138, acc: 0.593750] [adversarial loss: 0.734018, acc: 0.421875]\n",
      "1829: [discriminator loss: 0.659941, acc: 0.656250] [adversarial loss: 0.886402, acc: 0.265625]\n",
      "1830: [discriminator loss: 0.667911, acc: 0.593750] [adversarial loss: 0.830564, acc: 0.312500]\n",
      "1831: [discriminator loss: 0.652780, acc: 0.609375] [adversarial loss: 0.828178, acc: 0.281250]\n",
      "1832: [discriminator loss: 0.681112, acc: 0.570312] [adversarial loss: 0.652595, acc: 0.625000]\n",
      "1833: [discriminator loss: 0.677316, acc: 0.546875] [adversarial loss: 0.990866, acc: 0.156250]\n",
      "1834: [discriminator loss: 0.664216, acc: 0.601562] [adversarial loss: 0.641769, acc: 0.609375]\n",
      "1835: [discriminator loss: 0.673422, acc: 0.578125] [adversarial loss: 0.989113, acc: 0.203125]\n",
      "1836: [discriminator loss: 0.672747, acc: 0.593750] [adversarial loss: 0.595015, acc: 0.703125]\n",
      "1837: [discriminator loss: 0.704774, acc: 0.562500] [adversarial loss: 1.257898, acc: 0.031250]\n",
      "1838: [discriminator loss: 0.727825, acc: 0.492188] [adversarial loss: 0.623189, acc: 0.593750]\n",
      "1839: [discriminator loss: 0.694122, acc: 0.554688] [adversarial loss: 0.967274, acc: 0.140625]\n",
      "1840: [discriminator loss: 0.675608, acc: 0.601562] [adversarial loss: 0.649804, acc: 0.640625]\n",
      "1841: [discriminator loss: 0.656732, acc: 0.632812] [adversarial loss: 1.006454, acc: 0.187500]\n",
      "1842: [discriminator loss: 0.718740, acc: 0.523438] [adversarial loss: 0.669785, acc: 0.578125]\n",
      "1843: [discriminator loss: 0.672460, acc: 0.593750] [adversarial loss: 0.857252, acc: 0.265625]\n",
      "1844: [discriminator loss: 0.700203, acc: 0.578125] [adversarial loss: 0.763474, acc: 0.281250]\n",
      "1845: [discriminator loss: 0.660231, acc: 0.632812] [adversarial loss: 0.931234, acc: 0.203125]\n",
      "1846: [discriminator loss: 0.690564, acc: 0.531250] [adversarial loss: 0.527705, acc: 0.781250]\n",
      "1847: [discriminator loss: 0.705401, acc: 0.507812] [adversarial loss: 0.989070, acc: 0.156250]\n",
      "1848: [discriminator loss: 0.650147, acc: 0.593750] [adversarial loss: 0.652197, acc: 0.546875]\n",
      "1849: [discriminator loss: 0.701094, acc: 0.554688] [adversarial loss: 1.012762, acc: 0.171875]\n",
      "1850: [discriminator loss: 0.694816, acc: 0.570312] [adversarial loss: 0.685598, acc: 0.546875]\n",
      "1851: [discriminator loss: 0.670310, acc: 0.570312] [adversarial loss: 0.808736, acc: 0.312500]\n",
      "1852: [discriminator loss: 0.664676, acc: 0.570312] [adversarial loss: 0.714228, acc: 0.531250]\n",
      "1853: [discriminator loss: 0.650635, acc: 0.601562] [adversarial loss: 0.932624, acc: 0.140625]\n",
      "1854: [discriminator loss: 0.658889, acc: 0.578125] [adversarial loss: 0.727994, acc: 0.484375]\n",
      "1855: [discriminator loss: 0.727837, acc: 0.507812] [adversarial loss: 0.894165, acc: 0.281250]\n",
      "1856: [discriminator loss: 0.650729, acc: 0.625000] [adversarial loss: 0.798367, acc: 0.421875]\n",
      "1857: [discriminator loss: 0.689995, acc: 0.585938] [adversarial loss: 0.942807, acc: 0.156250]\n",
      "1858: [discriminator loss: 0.686107, acc: 0.570312] [adversarial loss: 0.578355, acc: 0.765625]\n",
      "1859: [discriminator loss: 0.699956, acc: 0.523438] [adversarial loss: 1.065279, acc: 0.062500]\n",
      "1860: [discriminator loss: 0.683391, acc: 0.554688] [adversarial loss: 0.638021, acc: 0.656250]\n",
      "1861: [discriminator loss: 0.676853, acc: 0.585938] [adversarial loss: 0.935162, acc: 0.218750]\n",
      "1862: [discriminator loss: 0.686316, acc: 0.593750] [adversarial loss: 0.698262, acc: 0.546875]\n",
      "1863: [discriminator loss: 0.663644, acc: 0.609375] [adversarial loss: 0.782005, acc: 0.328125]\n",
      "1864: [discriminator loss: 0.684510, acc: 0.593750] [adversarial loss: 0.846572, acc: 0.281250]\n",
      "1865: [discriminator loss: 0.676627, acc: 0.609375] [adversarial loss: 0.805613, acc: 0.312500]\n",
      "1866: [discriminator loss: 0.666021, acc: 0.593750] [adversarial loss: 0.784421, acc: 0.375000]\n",
      "1867: [discriminator loss: 0.702046, acc: 0.531250] [adversarial loss: 0.682940, acc: 0.562500]\n",
      "1868: [discriminator loss: 0.680025, acc: 0.578125] [adversarial loss: 0.777157, acc: 0.234375]\n",
      "1869: [discriminator loss: 0.671812, acc: 0.570312] [adversarial loss: 0.846474, acc: 0.234375]\n",
      "1870: [discriminator loss: 0.648850, acc: 0.609375] [adversarial loss: 0.955997, acc: 0.203125]\n",
      "1871: [discriminator loss: 0.655959, acc: 0.570312] [adversarial loss: 0.623711, acc: 0.671875]\n",
      "1872: [discriminator loss: 0.672314, acc: 0.585938] [adversarial loss: 1.062889, acc: 0.125000]\n",
      "1873: [discriminator loss: 0.718733, acc: 0.515625] [adversarial loss: 0.574091, acc: 0.796875]\n",
      "1874: [discriminator loss: 0.658137, acc: 0.570312] [adversarial loss: 1.045377, acc: 0.109375]\n",
      "1875: [discriminator loss: 0.741647, acc: 0.500000] [adversarial loss: 0.583406, acc: 0.765625]\n",
      "1876: [discriminator loss: 0.676586, acc: 0.546875] [adversarial loss: 0.964487, acc: 0.062500]\n",
      "1877: [discriminator loss: 0.672263, acc: 0.609375] [adversarial loss: 0.639796, acc: 0.656250]\n",
      "1878: [discriminator loss: 0.697785, acc: 0.531250] [adversarial loss: 0.922053, acc: 0.171875]\n",
      "1879: [discriminator loss: 0.655693, acc: 0.617188] [adversarial loss: 0.644603, acc: 0.687500]\n",
      "1880: [discriminator loss: 0.675830, acc: 0.609375] [adversarial loss: 0.911397, acc: 0.125000]\n",
      "1881: [discriminator loss: 0.713444, acc: 0.531250] [adversarial loss: 0.729500, acc: 0.437500]\n",
      "1882: [discriminator loss: 0.693767, acc: 0.578125] [adversarial loss: 0.845556, acc: 0.265625]\n",
      "1883: [discriminator loss: 0.654908, acc: 0.593750] [adversarial loss: 0.841954, acc: 0.281250]\n",
      "1884: [discriminator loss: 0.641931, acc: 0.609375] [adversarial loss: 0.758333, acc: 0.437500]\n",
      "1885: [discriminator loss: 0.614344, acc: 0.718750] [adversarial loss: 0.894457, acc: 0.296875]\n",
      "1886: [discriminator loss: 0.661174, acc: 0.593750] [adversarial loss: 0.740806, acc: 0.500000]\n",
      "1887: [discriminator loss: 0.702195, acc: 0.546875] [adversarial loss: 0.711593, acc: 0.500000]\n",
      "1888: [discriminator loss: 0.686135, acc: 0.593750] [adversarial loss: 0.826364, acc: 0.281250]\n",
      "1889: [discriminator loss: 0.670808, acc: 0.554688] [adversarial loss: 0.866973, acc: 0.265625]\n",
      "1890: [discriminator loss: 0.683315, acc: 0.585938] [adversarial loss: 0.701545, acc: 0.578125]\n",
      "1891: [discriminator loss: 0.686676, acc: 0.562500] [adversarial loss: 0.931940, acc: 0.171875]\n",
      "1892: [discriminator loss: 0.693426, acc: 0.539062] [adversarial loss: 0.771295, acc: 0.437500]\n",
      "1893: [discriminator loss: 0.668748, acc: 0.617188] [adversarial loss: 0.795850, acc: 0.406250]\n",
      "1894: [discriminator loss: 0.674289, acc: 0.593750] [adversarial loss: 0.692494, acc: 0.562500]\n",
      "1895: [discriminator loss: 0.654325, acc: 0.625000] [adversarial loss: 0.816778, acc: 0.296875]\n",
      "1896: [discriminator loss: 0.651491, acc: 0.585938] [adversarial loss: 0.743633, acc: 0.390625]\n",
      "1897: [discriminator loss: 0.663158, acc: 0.539062] [adversarial loss: 0.886700, acc: 0.234375]\n",
      "1898: [discriminator loss: 0.702354, acc: 0.531250] [adversarial loss: 0.618114, acc: 0.765625]\n",
      "1899: [discriminator loss: 0.685320, acc: 0.546875] [adversarial loss: 1.031263, acc: 0.109375]\n",
      "1900: [discriminator loss: 0.718688, acc: 0.570312] [adversarial loss: 0.584739, acc: 0.703125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1901: [discriminator loss: 0.748034, acc: 0.484375] [adversarial loss: 1.008532, acc: 0.046875]\n",
      "1902: [discriminator loss: 0.709112, acc: 0.523438] [adversarial loss: 0.648538, acc: 0.625000]\n",
      "1903: [discriminator loss: 0.690274, acc: 0.578125] [adversarial loss: 1.060805, acc: 0.046875]\n",
      "1904: [discriminator loss: 0.667561, acc: 0.625000] [adversarial loss: 0.669222, acc: 0.515625]\n",
      "1905: [discriminator loss: 0.657132, acc: 0.625000] [adversarial loss: 0.887759, acc: 0.140625]\n",
      "1906: [discriminator loss: 0.655376, acc: 0.585938] [adversarial loss: 0.717515, acc: 0.515625]\n",
      "1907: [discriminator loss: 0.640413, acc: 0.609375] [adversarial loss: 0.777698, acc: 0.359375]\n",
      "1908: [discriminator loss: 0.692942, acc: 0.523438] [adversarial loss: 0.773281, acc: 0.421875]\n",
      "1909: [discriminator loss: 0.689224, acc: 0.593750] [adversarial loss: 0.717690, acc: 0.468750]\n",
      "1910: [discriminator loss: 0.641455, acc: 0.625000] [adversarial loss: 0.935030, acc: 0.171875]\n",
      "1911: [discriminator loss: 0.672978, acc: 0.578125] [adversarial loss: 0.583028, acc: 0.718750]\n",
      "1912: [discriminator loss: 0.660551, acc: 0.617188] [adversarial loss: 1.001136, acc: 0.125000]\n",
      "1913: [discriminator loss: 0.701861, acc: 0.523438] [adversarial loss: 0.589384, acc: 0.734375]\n",
      "1914: [discriminator loss: 0.727964, acc: 0.539062] [adversarial loss: 1.186274, acc: 0.015625]\n",
      "1915: [discriminator loss: 0.733157, acc: 0.531250] [adversarial loss: 0.594121, acc: 0.781250]\n",
      "1916: [discriminator loss: 0.675761, acc: 0.562500] [adversarial loss: 0.839403, acc: 0.281250]\n",
      "1917: [discriminator loss: 0.625984, acc: 0.687500] [adversarial loss: 0.780475, acc: 0.359375]\n",
      "1918: [discriminator loss: 0.690276, acc: 0.539062] [adversarial loss: 0.826159, acc: 0.296875]\n",
      "1919: [discriminator loss: 0.660751, acc: 0.625000] [adversarial loss: 0.810594, acc: 0.312500]\n",
      "1920: [discriminator loss: 0.686231, acc: 0.484375] [adversarial loss: 0.905967, acc: 0.234375]\n",
      "1921: [discriminator loss: 0.639620, acc: 0.648438] [adversarial loss: 0.851450, acc: 0.250000]\n",
      "1922: [discriminator loss: 0.631693, acc: 0.632812] [adversarial loss: 0.752949, acc: 0.453125]\n",
      "1923: [discriminator loss: 0.673329, acc: 0.578125] [adversarial loss: 0.948366, acc: 0.125000]\n",
      "1924: [discriminator loss: 0.697270, acc: 0.562500] [adversarial loss: 0.555682, acc: 0.828125]\n",
      "1925: [discriminator loss: 0.655269, acc: 0.601562] [adversarial loss: 0.834922, acc: 0.296875]\n",
      "1926: [discriminator loss: 0.662713, acc: 0.609375] [adversarial loss: 0.698309, acc: 0.515625]\n",
      "1927: [discriminator loss: 0.680605, acc: 0.570312] [adversarial loss: 0.767424, acc: 0.437500]\n",
      "1928: [discriminator loss: 0.689936, acc: 0.546875] [adversarial loss: 0.824886, acc: 0.328125]\n",
      "1929: [discriminator loss: 0.676554, acc: 0.601562] [adversarial loss: 0.866149, acc: 0.265625]\n",
      "1930: [discriminator loss: 0.719858, acc: 0.500000] [adversarial loss: 0.877285, acc: 0.359375]\n",
      "1931: [discriminator loss: 0.694642, acc: 0.585938] [adversarial loss: 0.750453, acc: 0.453125]\n",
      "1932: [discriminator loss: 0.621845, acc: 0.710938] [adversarial loss: 0.718629, acc: 0.484375]\n",
      "1933: [discriminator loss: 0.647119, acc: 0.617188] [adversarial loss: 0.807222, acc: 0.281250]\n",
      "1934: [discriminator loss: 0.650241, acc: 0.585938] [adversarial loss: 0.674761, acc: 0.609375]\n",
      "1935: [discriminator loss: 0.696465, acc: 0.531250] [adversarial loss: 0.956540, acc: 0.062500]\n",
      "1936: [discriminator loss: 0.653242, acc: 0.656250] [adversarial loss: 0.656702, acc: 0.671875]\n",
      "1937: [discriminator loss: 0.692709, acc: 0.570312] [adversarial loss: 1.056058, acc: 0.046875]\n",
      "1938: [discriminator loss: 0.699603, acc: 0.539062] [adversarial loss: 0.573040, acc: 0.796875]\n",
      "1939: [discriminator loss: 0.688359, acc: 0.523438] [adversarial loss: 1.272390, acc: 0.046875]\n",
      "1940: [discriminator loss: 0.782772, acc: 0.476562] [adversarial loss: 0.539587, acc: 0.875000]\n",
      "1941: [discriminator loss: 0.682360, acc: 0.539062] [adversarial loss: 0.955874, acc: 0.140625]\n",
      "1942: [discriminator loss: 0.659309, acc: 0.585938] [adversarial loss: 0.676062, acc: 0.562500]\n",
      "1943: [discriminator loss: 0.696391, acc: 0.523438] [adversarial loss: 0.908226, acc: 0.078125]\n",
      "1944: [discriminator loss: 0.704574, acc: 0.515625] [adversarial loss: 0.688283, acc: 0.562500]\n",
      "1945: [discriminator loss: 0.651357, acc: 0.585938] [adversarial loss: 0.924440, acc: 0.171875]\n",
      "1946: [discriminator loss: 0.683925, acc: 0.562500] [adversarial loss: 0.662391, acc: 0.656250]\n",
      "1947: [discriminator loss: 0.673785, acc: 0.562500] [adversarial loss: 0.939905, acc: 0.187500]\n",
      "1948: [discriminator loss: 0.677444, acc: 0.593750] [adversarial loss: 0.594449, acc: 0.781250]\n",
      "1949: [discriminator loss: 0.663894, acc: 0.546875] [adversarial loss: 0.939605, acc: 0.140625]\n",
      "1950: [discriminator loss: 0.675208, acc: 0.546875] [adversarial loss: 0.659558, acc: 0.625000]\n",
      "1951: [discriminator loss: 0.685933, acc: 0.515625] [adversarial loss: 0.846406, acc: 0.234375]\n",
      "1952: [discriminator loss: 0.673216, acc: 0.609375] [adversarial loss: 0.774553, acc: 0.453125]\n",
      "1953: [discriminator loss: 0.671914, acc: 0.593750] [adversarial loss: 0.933592, acc: 0.156250]\n",
      "1954: [discriminator loss: 0.699109, acc: 0.570312] [adversarial loss: 0.713315, acc: 0.562500]\n",
      "1955: [discriminator loss: 0.685558, acc: 0.554688] [adversarial loss: 0.967711, acc: 0.203125]\n",
      "1956: [discriminator loss: 0.668500, acc: 0.578125] [adversarial loss: 0.588740, acc: 0.734375]\n",
      "1957: [discriminator loss: 0.694158, acc: 0.539062] [adversarial loss: 0.936005, acc: 0.140625]\n",
      "1958: [discriminator loss: 0.673353, acc: 0.554688] [adversarial loss: 0.672802, acc: 0.640625]\n",
      "1959: [discriminator loss: 0.659736, acc: 0.656250] [adversarial loss: 0.862104, acc: 0.203125]\n",
      "1960: [discriminator loss: 0.659574, acc: 0.625000] [adversarial loss: 0.787969, acc: 0.328125]\n",
      "1961: [discriminator loss: 0.660451, acc: 0.609375] [adversarial loss: 0.943533, acc: 0.156250]\n",
      "1962: [discriminator loss: 0.647197, acc: 0.632812] [adversarial loss: 0.706628, acc: 0.515625]\n",
      "1963: [discriminator loss: 0.677903, acc: 0.601562] [adversarial loss: 0.927879, acc: 0.093750]\n",
      "1964: [discriminator loss: 0.675790, acc: 0.570312] [adversarial loss: 0.639558, acc: 0.640625]\n",
      "1965: [discriminator loss: 0.635643, acc: 0.640625] [adversarial loss: 0.922928, acc: 0.265625]\n",
      "1966: [discriminator loss: 0.716315, acc: 0.515625] [adversarial loss: 0.571986, acc: 0.750000]\n",
      "1967: [discriminator loss: 0.688992, acc: 0.523438] [adversarial loss: 0.911590, acc: 0.156250]\n",
      "1968: [discriminator loss: 0.664520, acc: 0.578125] [adversarial loss: 0.704492, acc: 0.546875]\n",
      "1969: [discriminator loss: 0.684414, acc: 0.554688] [adversarial loss: 0.953522, acc: 0.171875]\n",
      "1970: [discriminator loss: 0.700159, acc: 0.539062] [adversarial loss: 0.663129, acc: 0.625000]\n",
      "1971: [discriminator loss: 0.701897, acc: 0.515625] [adversarial loss: 1.078652, acc: 0.046875]\n",
      "1972: [discriminator loss: 0.673731, acc: 0.578125] [adversarial loss: 0.574868, acc: 0.718750]\n",
      "1973: [discriminator loss: 0.735944, acc: 0.500000] [adversarial loss: 0.965515, acc: 0.156250]\n",
      "1974: [discriminator loss: 0.660154, acc: 0.671875] [adversarial loss: 0.736963, acc: 0.453125]\n",
      "1975: [discriminator loss: 0.700932, acc: 0.554688] [adversarial loss: 0.732287, acc: 0.359375]\n",
      "1976: [discriminator loss: 0.668793, acc: 0.585938] [adversarial loss: 0.720550, acc: 0.437500]\n",
      "1977: [discriminator loss: 0.691937, acc: 0.546875] [adversarial loss: 0.805256, acc: 0.312500]\n",
      "1978: [discriminator loss: 0.658006, acc: 0.601562] [adversarial loss: 0.746572, acc: 0.406250]\n",
      "1979: [discriminator loss: 0.652654, acc: 0.648438] [adversarial loss: 0.904630, acc: 0.296875]\n",
      "1980: [discriminator loss: 0.680698, acc: 0.617188] [adversarial loss: 0.837792, acc: 0.265625]\n",
      "1981: [discriminator loss: 0.690345, acc: 0.601562] [adversarial loss: 0.836403, acc: 0.218750]\n",
      "1982: [discriminator loss: 0.632929, acc: 0.609375] [adversarial loss: 0.817856, acc: 0.281250]\n",
      "1983: [discriminator loss: 0.663203, acc: 0.593750] [adversarial loss: 0.749777, acc: 0.406250]\n",
      "1984: [discriminator loss: 0.661909, acc: 0.601562] [adversarial loss: 0.701794, acc: 0.593750]\n",
      "1985: [discriminator loss: 0.668808, acc: 0.601562] [adversarial loss: 0.894552, acc: 0.234375]\n",
      "1986: [discriminator loss: 0.638066, acc: 0.640625] [adversarial loss: 0.664938, acc: 0.593750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1987: [discriminator loss: 0.698297, acc: 0.554688] [adversarial loss: 0.726647, acc: 0.531250]\n",
      "1988: [discriminator loss: 0.638782, acc: 0.671875] [adversarial loss: 0.955113, acc: 0.125000]\n",
      "1989: [discriminator loss: 0.679701, acc: 0.585938] [adversarial loss: 0.661578, acc: 0.609375]\n",
      "1990: [discriminator loss: 0.673836, acc: 0.570312] [adversarial loss: 1.042827, acc: 0.109375]\n",
      "1991: [discriminator loss: 0.702070, acc: 0.562500] [adversarial loss: 0.569482, acc: 0.765625]\n",
      "1992: [discriminator loss: 0.719223, acc: 0.531250] [adversarial loss: 0.971984, acc: 0.203125]\n",
      "1993: [discriminator loss: 0.690884, acc: 0.585938] [adversarial loss: 0.573017, acc: 0.781250]\n",
      "1994: [discriminator loss: 0.706778, acc: 0.500000] [adversarial loss: 0.908082, acc: 0.171875]\n",
      "1995: [discriminator loss: 0.643283, acc: 0.609375] [adversarial loss: 0.689873, acc: 0.562500]\n",
      "1996: [discriminator loss: 0.670311, acc: 0.562500] [adversarial loss: 0.867745, acc: 0.296875]\n",
      "1997: [discriminator loss: 0.717947, acc: 0.539062] [adversarial loss: 0.744037, acc: 0.421875]\n",
      "1998: [discriminator loss: 0.710170, acc: 0.539062] [adversarial loss: 1.052773, acc: 0.093750]\n",
      "1999: [discriminator loss: 0.688173, acc: 0.609375] [adversarial loss: 0.612442, acc: 0.687500]\n",
      "cgan_mnist  labels for generated images:  [0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5]\n",
      "2000: [discriminator loss: 0.660877, acc: 0.601562] [adversarial loss: 1.021970, acc: 0.125000]\n",
      "2001: [discriminator loss: 0.722915, acc: 0.562500] [adversarial loss: 0.626647, acc: 0.703125]\n",
      "2002: [discriminator loss: 0.671913, acc: 0.562500] [adversarial loss: 0.810769, acc: 0.234375]\n",
      "2003: [discriminator loss: 0.673656, acc: 0.539062] [adversarial loss: 0.737733, acc: 0.437500]\n",
      "2004: [discriminator loss: 0.690946, acc: 0.570312] [adversarial loss: 0.874562, acc: 0.328125]\n",
      "2005: [discriminator loss: 0.683545, acc: 0.554688] [adversarial loss: 0.663347, acc: 0.609375]\n",
      "2006: [discriminator loss: 0.669754, acc: 0.531250] [adversarial loss: 0.920348, acc: 0.093750]\n",
      "2007: [discriminator loss: 0.660253, acc: 0.609375] [adversarial loss: 0.696466, acc: 0.531250]\n",
      "2008: [discriminator loss: 0.687303, acc: 0.578125] [adversarial loss: 1.092092, acc: 0.046875]\n",
      "2009: [discriminator loss: 0.748970, acc: 0.515625] [adversarial loss: 0.613419, acc: 0.734375]\n",
      "2010: [discriminator loss: 0.669589, acc: 0.578125] [adversarial loss: 0.936264, acc: 0.171875]\n",
      "2011: [discriminator loss: 0.677242, acc: 0.546875] [adversarial loss: 0.725310, acc: 0.437500]\n",
      "2012: [discriminator loss: 0.695031, acc: 0.515625] [adversarial loss: 0.806841, acc: 0.281250]\n",
      "2013: [discriminator loss: 0.655541, acc: 0.601562] [adversarial loss: 0.721539, acc: 0.406250]\n",
      "2014: [discriminator loss: 0.648144, acc: 0.625000] [adversarial loss: 0.882677, acc: 0.156250]\n",
      "2015: [discriminator loss: 0.664113, acc: 0.585938] [adversarial loss: 0.685485, acc: 0.546875]\n",
      "2016: [discriminator loss: 0.700046, acc: 0.507812] [adversarial loss: 1.003835, acc: 0.015625]\n",
      "2017: [discriminator loss: 0.669849, acc: 0.546875] [adversarial loss: 0.634668, acc: 0.671875]\n",
      "2018: [discriminator loss: 0.659423, acc: 0.640625] [adversarial loss: 1.008961, acc: 0.296875]\n",
      "2019: [discriminator loss: 0.686271, acc: 0.570312] [adversarial loss: 0.631470, acc: 0.718750]\n",
      "2020: [discriminator loss: 0.720961, acc: 0.531250] [adversarial loss: 0.954799, acc: 0.125000]\n",
      "2021: [discriminator loss: 0.698722, acc: 0.562500] [adversarial loss: 0.617985, acc: 0.734375]\n",
      "2022: [discriminator loss: 0.675563, acc: 0.562500] [adversarial loss: 0.895846, acc: 0.156250]\n",
      "2023: [discriminator loss: 0.673599, acc: 0.562500] [adversarial loss: 0.681313, acc: 0.546875]\n",
      "2024: [discriminator loss: 0.678434, acc: 0.554688] [adversarial loss: 0.842022, acc: 0.203125]\n",
      "2025: [discriminator loss: 0.698976, acc: 0.515625] [adversarial loss: 0.719589, acc: 0.468750]\n",
      "2026: [discriminator loss: 0.641087, acc: 0.671875] [adversarial loss: 0.845607, acc: 0.281250]\n",
      "2027: [discriminator loss: 0.683411, acc: 0.625000] [adversarial loss: 0.633722, acc: 0.671875]\n",
      "2028: [discriminator loss: 0.698982, acc: 0.539062] [adversarial loss: 0.886504, acc: 0.203125]\n",
      "2029: [discriminator loss: 0.693454, acc: 0.539062] [adversarial loss: 0.678610, acc: 0.656250]\n",
      "2030: [discriminator loss: 0.679608, acc: 0.546875] [adversarial loss: 0.902376, acc: 0.156250]\n",
      "2031: [discriminator loss: 0.647524, acc: 0.617188] [adversarial loss: 0.700972, acc: 0.500000]\n",
      "2032: [discriminator loss: 0.690255, acc: 0.515625] [adversarial loss: 1.030993, acc: 0.062500]\n",
      "2033: [discriminator loss: 0.682362, acc: 0.585938] [adversarial loss: 0.642440, acc: 0.703125]\n",
      "2034: [discriminator loss: 0.709786, acc: 0.539062] [adversarial loss: 0.958390, acc: 0.203125]\n",
      "2035: [discriminator loss: 0.655369, acc: 0.640625] [adversarial loss: 0.651210, acc: 0.609375]\n",
      "2036: [discriminator loss: 0.734073, acc: 0.445312] [adversarial loss: 0.740598, acc: 0.421875]\n",
      "2037: [discriminator loss: 0.700743, acc: 0.570312] [adversarial loss: 0.829870, acc: 0.328125]\n",
      "2038: [discriminator loss: 0.650947, acc: 0.632812] [adversarial loss: 0.797503, acc: 0.421875]\n",
      "2039: [discriminator loss: 0.696488, acc: 0.562500] [adversarial loss: 0.753381, acc: 0.390625]\n",
      "2040: [discriminator loss: 0.696038, acc: 0.539062] [adversarial loss: 0.822650, acc: 0.234375]\n",
      "2041: [discriminator loss: 0.674481, acc: 0.570312] [adversarial loss: 0.653835, acc: 0.562500]\n",
      "2042: [discriminator loss: 0.697855, acc: 0.515625] [adversarial loss: 0.946624, acc: 0.156250]\n",
      "2043: [discriminator loss: 0.665339, acc: 0.617188] [adversarial loss: 0.638131, acc: 0.671875]\n",
      "2044: [discriminator loss: 0.684845, acc: 0.554688] [adversarial loss: 0.911869, acc: 0.218750]\n",
      "2045: [discriminator loss: 0.698798, acc: 0.531250] [adversarial loss: 0.599459, acc: 0.703125]\n",
      "2046: [discriminator loss: 0.674994, acc: 0.539062] [adversarial loss: 1.001829, acc: 0.093750]\n",
      "2047: [discriminator loss: 0.687985, acc: 0.531250] [adversarial loss: 0.589897, acc: 0.750000]\n",
      "2048: [discriminator loss: 0.704432, acc: 0.546875] [adversarial loss: 0.850708, acc: 0.265625]\n",
      "2049: [discriminator loss: 0.673289, acc: 0.554688] [adversarial loss: 0.719973, acc: 0.468750]\n",
      "2050: [discriminator loss: 0.696447, acc: 0.578125] [adversarial loss: 0.891000, acc: 0.171875]\n",
      "2051: [discriminator loss: 0.670615, acc: 0.593750] [adversarial loss: 0.759303, acc: 0.468750]\n",
      "2052: [discriminator loss: 0.660674, acc: 0.570312] [adversarial loss: 0.712115, acc: 0.484375]\n",
      "2053: [discriminator loss: 0.668926, acc: 0.585938] [adversarial loss: 0.944923, acc: 0.125000]\n",
      "2054: [discriminator loss: 0.668760, acc: 0.585938] [adversarial loss: 0.630591, acc: 0.687500]\n",
      "2055: [discriminator loss: 0.742442, acc: 0.453125] [adversarial loss: 0.902104, acc: 0.203125]\n",
      "2056: [discriminator loss: 0.670447, acc: 0.562500] [adversarial loss: 0.719743, acc: 0.500000]\n",
      "2057: [discriminator loss: 0.679767, acc: 0.546875] [adversarial loss: 0.877562, acc: 0.203125]\n",
      "2058: [discriminator loss: 0.694561, acc: 0.562500] [adversarial loss: 0.619834, acc: 0.671875]\n",
      "2059: [discriminator loss: 0.683532, acc: 0.531250] [adversarial loss: 0.964192, acc: 0.156250]\n",
      "2060: [discriminator loss: 0.709400, acc: 0.570312] [adversarial loss: 0.608858, acc: 0.718750]\n",
      "2061: [discriminator loss: 0.704393, acc: 0.507812] [adversarial loss: 0.837181, acc: 0.328125]\n",
      "2062: [discriminator loss: 0.680350, acc: 0.601562] [adversarial loss: 0.720683, acc: 0.515625]\n",
      "2063: [discriminator loss: 0.671844, acc: 0.578125] [adversarial loss: 0.851192, acc: 0.234375]\n",
      "2064: [discriminator loss: 0.670957, acc: 0.648438] [adversarial loss: 0.716878, acc: 0.500000]\n",
      "2065: [discriminator loss: 0.690869, acc: 0.500000] [adversarial loss: 0.829245, acc: 0.296875]\n",
      "2066: [discriminator loss: 0.667116, acc: 0.617188] [adversarial loss: 0.713832, acc: 0.484375]\n",
      "2067: [discriminator loss: 0.657360, acc: 0.601562] [adversarial loss: 0.931680, acc: 0.203125]\n",
      "2068: [discriminator loss: 0.675392, acc: 0.554688] [adversarial loss: 0.673858, acc: 0.578125]\n",
      "2069: [discriminator loss: 0.696141, acc: 0.531250] [adversarial loss: 0.884258, acc: 0.234375]\n",
      "2070: [discriminator loss: 0.676782, acc: 0.562500] [adversarial loss: 0.719894, acc: 0.484375]\n",
      "2071: [discriminator loss: 0.673671, acc: 0.601562] [adversarial loss: 0.769250, acc: 0.390625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2072: [discriminator loss: 0.646380, acc: 0.640625] [adversarial loss: 0.721969, acc: 0.453125]\n",
      "2073: [discriminator loss: 0.692254, acc: 0.531250] [adversarial loss: 0.851380, acc: 0.265625]\n",
      "2074: [discriminator loss: 0.688861, acc: 0.554688] [adversarial loss: 0.651884, acc: 0.625000]\n",
      "2075: [discriminator loss: 0.728657, acc: 0.523438] [adversarial loss: 1.020799, acc: 0.078125]\n",
      "2076: [discriminator loss: 0.667507, acc: 0.578125] [adversarial loss: 0.595962, acc: 0.765625]\n",
      "2077: [discriminator loss: 0.687793, acc: 0.562500] [adversarial loss: 0.922086, acc: 0.156250]\n",
      "2078: [discriminator loss: 0.656851, acc: 0.585938] [adversarial loss: 0.687944, acc: 0.500000]\n",
      "2079: [discriminator loss: 0.683539, acc: 0.539062] [adversarial loss: 0.751912, acc: 0.468750]\n",
      "2080: [discriminator loss: 0.680447, acc: 0.531250] [adversarial loss: 0.658221, acc: 0.640625]\n",
      "2081: [discriminator loss: 0.665351, acc: 0.546875] [adversarial loss: 0.862157, acc: 0.125000]\n",
      "2082: [discriminator loss: 0.653606, acc: 0.656250] [adversarial loss: 0.726704, acc: 0.468750]\n",
      "2083: [discriminator loss: 0.694716, acc: 0.578125] [adversarial loss: 0.811056, acc: 0.218750]\n",
      "2084: [discriminator loss: 0.640699, acc: 0.679688] [adversarial loss: 0.626318, acc: 0.671875]\n",
      "2085: [discriminator loss: 0.710094, acc: 0.453125] [adversarial loss: 0.879042, acc: 0.203125]\n",
      "2086: [discriminator loss: 0.667255, acc: 0.632812] [adversarial loss: 0.694809, acc: 0.546875]\n",
      "2087: [discriminator loss: 0.671070, acc: 0.593750] [adversarial loss: 0.938668, acc: 0.203125]\n",
      "2088: [discriminator loss: 0.649800, acc: 0.648438] [adversarial loss: 0.706251, acc: 0.515625]\n",
      "2089: [discriminator loss: 0.699278, acc: 0.500000] [adversarial loss: 0.962305, acc: 0.125000]\n",
      "2090: [discriminator loss: 0.651917, acc: 0.601562] [adversarial loss: 0.645461, acc: 0.656250]\n",
      "2091: [discriminator loss: 0.694402, acc: 0.578125] [adversarial loss: 0.983490, acc: 0.234375]\n",
      "2092: [discriminator loss: 0.741726, acc: 0.539062] [adversarial loss: 0.686194, acc: 0.640625]\n",
      "2093: [discriminator loss: 0.690104, acc: 0.585938] [adversarial loss: 0.769792, acc: 0.312500]\n",
      "2094: [discriminator loss: 0.683013, acc: 0.562500] [adversarial loss: 0.719112, acc: 0.453125]\n",
      "2095: [discriminator loss: 0.671840, acc: 0.593750] [adversarial loss: 0.842617, acc: 0.203125]\n",
      "2096: [discriminator loss: 0.674472, acc: 0.531250] [adversarial loss: 0.653564, acc: 0.609375]\n",
      "2097: [discriminator loss: 0.676209, acc: 0.539062] [adversarial loss: 0.899194, acc: 0.156250]\n",
      "2098: [discriminator loss: 0.653175, acc: 0.609375] [adversarial loss: 0.765641, acc: 0.343750]\n",
      "2099: [discriminator loss: 0.664831, acc: 0.578125] [adversarial loss: 0.826483, acc: 0.312500]\n",
      "2100: [discriminator loss: 0.699544, acc: 0.523438] [adversarial loss: 0.708409, acc: 0.468750]\n",
      "2101: [discriminator loss: 0.664312, acc: 0.578125] [adversarial loss: 0.848311, acc: 0.187500]\n",
      "2102: [discriminator loss: 0.665772, acc: 0.593750] [adversarial loss: 0.750843, acc: 0.390625]\n",
      "2103: [discriminator loss: 0.665301, acc: 0.578125] [adversarial loss: 1.014033, acc: 0.078125]\n",
      "2104: [discriminator loss: 0.706994, acc: 0.554688] [adversarial loss: 0.548918, acc: 0.843750]\n",
      "2105: [discriminator loss: 0.684214, acc: 0.523438] [adversarial loss: 1.008447, acc: 0.140625]\n",
      "2106: [discriminator loss: 0.704477, acc: 0.562500] [adversarial loss: 0.615441, acc: 0.671875]\n",
      "2107: [discriminator loss: 0.676542, acc: 0.609375] [adversarial loss: 0.858996, acc: 0.265625]\n",
      "2108: [discriminator loss: 0.686474, acc: 0.484375] [adversarial loss: 0.655566, acc: 0.656250]\n",
      "2109: [discriminator loss: 0.689562, acc: 0.539062] [adversarial loss: 0.856786, acc: 0.203125]\n",
      "2110: [discriminator loss: 0.687858, acc: 0.523438] [adversarial loss: 0.706565, acc: 0.484375]\n",
      "2111: [discriminator loss: 0.665315, acc: 0.609375] [adversarial loss: 0.822127, acc: 0.406250]\n",
      "2112: [discriminator loss: 0.681115, acc: 0.562500] [adversarial loss: 0.703915, acc: 0.562500]\n",
      "2113: [discriminator loss: 0.698283, acc: 0.539062] [adversarial loss: 0.872240, acc: 0.234375]\n",
      "2114: [discriminator loss: 0.650772, acc: 0.625000] [adversarial loss: 0.705270, acc: 0.515625]\n",
      "2115: [discriminator loss: 0.706354, acc: 0.531250] [adversarial loss: 0.845736, acc: 0.312500]\n",
      "2116: [discriminator loss: 0.657746, acc: 0.562500] [adversarial loss: 0.653493, acc: 0.625000]\n",
      "2117: [discriminator loss: 0.684162, acc: 0.593750] [adversarial loss: 0.877969, acc: 0.265625]\n",
      "2118: [discriminator loss: 0.676343, acc: 0.546875] [adversarial loss: 0.756584, acc: 0.500000]\n",
      "2119: [discriminator loss: 0.663433, acc: 0.554688] [adversarial loss: 0.812806, acc: 0.296875]\n",
      "2120: [discriminator loss: 0.642707, acc: 0.625000] [adversarial loss: 0.846837, acc: 0.250000]\n",
      "2121: [discriminator loss: 0.691962, acc: 0.523438] [adversarial loss: 0.716972, acc: 0.546875]\n",
      "2122: [discriminator loss: 0.672457, acc: 0.585938] [adversarial loss: 0.873722, acc: 0.265625]\n",
      "2123: [discriminator loss: 0.663137, acc: 0.554688] [adversarial loss: 0.627281, acc: 0.656250]\n",
      "2124: [discriminator loss: 0.700500, acc: 0.531250] [adversarial loss: 0.941039, acc: 0.140625]\n",
      "2125: [discriminator loss: 0.711632, acc: 0.554688] [adversarial loss: 0.603880, acc: 0.718750]\n",
      "2126: [discriminator loss: 0.663116, acc: 0.570312] [adversarial loss: 1.035857, acc: 0.093750]\n",
      "2127: [discriminator loss: 0.713989, acc: 0.523438] [adversarial loss: 0.652704, acc: 0.625000]\n",
      "2128: [discriminator loss: 0.715716, acc: 0.492188] [adversarial loss: 0.932027, acc: 0.062500]\n",
      "2129: [discriminator loss: 0.675901, acc: 0.593750] [adversarial loss: 0.705368, acc: 0.468750]\n",
      "2130: [discriminator loss: 0.690464, acc: 0.601562] [adversarial loss: 0.874969, acc: 0.250000]\n",
      "2131: [discriminator loss: 0.670930, acc: 0.578125] [adversarial loss: 0.753771, acc: 0.390625]\n",
      "2132: [discriminator loss: 0.691138, acc: 0.554688] [adversarial loss: 0.913280, acc: 0.250000]\n",
      "2133: [discriminator loss: 0.663562, acc: 0.593750] [adversarial loss: 0.702928, acc: 0.484375]\n",
      "2134: [discriminator loss: 0.692026, acc: 0.515625] [adversarial loss: 0.771870, acc: 0.359375]\n",
      "2135: [discriminator loss: 0.715006, acc: 0.515625] [adversarial loss: 0.755506, acc: 0.359375]\n",
      "2136: [discriminator loss: 0.688096, acc: 0.578125] [adversarial loss: 0.808235, acc: 0.312500]\n",
      "2137: [discriminator loss: 0.691750, acc: 0.585938] [adversarial loss: 0.923179, acc: 0.187500]\n",
      "2138: [discriminator loss: 0.671912, acc: 0.601562] [adversarial loss: 0.661333, acc: 0.546875]\n",
      "2139: [discriminator loss: 0.657865, acc: 0.578125] [adversarial loss: 0.938239, acc: 0.171875]\n",
      "2140: [discriminator loss: 0.656077, acc: 0.609375] [adversarial loss: 0.692235, acc: 0.562500]\n",
      "2141: [discriminator loss: 0.684836, acc: 0.554688] [adversarial loss: 0.735540, acc: 0.421875]\n",
      "2142: [discriminator loss: 0.656882, acc: 0.640625] [adversarial loss: 0.838281, acc: 0.281250]\n",
      "2143: [discriminator loss: 0.711257, acc: 0.515625] [adversarial loss: 0.635682, acc: 0.625000]\n",
      "2144: [discriminator loss: 0.709931, acc: 0.500000] [adversarial loss: 0.811894, acc: 0.281250]\n",
      "2145: [discriminator loss: 0.691770, acc: 0.546875] [adversarial loss: 0.743319, acc: 0.343750]\n",
      "2146: [discriminator loss: 0.665127, acc: 0.632812] [adversarial loss: 0.706126, acc: 0.453125]\n",
      "2147: [discriminator loss: 0.705430, acc: 0.539062] [adversarial loss: 0.878343, acc: 0.171875]\n",
      "2148: [discriminator loss: 0.679083, acc: 0.523438] [adversarial loss: 0.690432, acc: 0.500000]\n",
      "2149: [discriminator loss: 0.707945, acc: 0.507812] [adversarial loss: 0.766341, acc: 0.375000]\n",
      "2150: [discriminator loss: 0.686624, acc: 0.578125] [adversarial loss: 0.704260, acc: 0.500000]\n",
      "2151: [discriminator loss: 0.664084, acc: 0.625000] [adversarial loss: 0.800532, acc: 0.296875]\n",
      "2152: [discriminator loss: 0.707710, acc: 0.500000] [adversarial loss: 0.852169, acc: 0.250000]\n",
      "2153: [discriminator loss: 0.673331, acc: 0.601562] [adversarial loss: 0.605169, acc: 0.671875]\n",
      "2154: [discriminator loss: 0.685790, acc: 0.546875] [adversarial loss: 0.865206, acc: 0.281250]\n",
      "2155: [discriminator loss: 0.682358, acc: 0.531250] [adversarial loss: 0.725973, acc: 0.421875]\n",
      "2156: [discriminator loss: 0.665643, acc: 0.609375] [adversarial loss: 0.869532, acc: 0.156250]\n",
      "2157: [discriminator loss: 0.655899, acc: 0.601562] [adversarial loss: 0.738643, acc: 0.437500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2158: [discriminator loss: 0.678438, acc: 0.554688] [adversarial loss: 0.670116, acc: 0.531250]\n",
      "2159: [discriminator loss: 0.712866, acc: 0.554688] [adversarial loss: 0.828979, acc: 0.234375]\n",
      "2160: [discriminator loss: 0.698387, acc: 0.562500] [adversarial loss: 0.710547, acc: 0.468750]\n",
      "2161: [discriminator loss: 0.666099, acc: 0.562500] [adversarial loss: 0.859651, acc: 0.218750]\n",
      "2162: [discriminator loss: 0.661512, acc: 0.570312] [adversarial loss: 0.699350, acc: 0.500000]\n",
      "2163: [discriminator loss: 0.662654, acc: 0.601562] [adversarial loss: 0.948698, acc: 0.187500]\n",
      "2164: [discriminator loss: 0.666039, acc: 0.601562] [adversarial loss: 0.625341, acc: 0.656250]\n",
      "2165: [discriminator loss: 0.679200, acc: 0.546875] [adversarial loss: 0.804764, acc: 0.406250]\n",
      "2166: [discriminator loss: 0.670291, acc: 0.578125] [adversarial loss: 0.737717, acc: 0.421875]\n",
      "2167: [discriminator loss: 0.664540, acc: 0.601562] [adversarial loss: 0.805937, acc: 0.343750]\n",
      "2168: [discriminator loss: 0.707458, acc: 0.585938] [adversarial loss: 0.850002, acc: 0.250000]\n",
      "2169: [discriminator loss: 0.652545, acc: 0.609375] [adversarial loss: 0.751192, acc: 0.437500]\n",
      "2170: [discriminator loss: 0.658211, acc: 0.585938] [adversarial loss: 0.821314, acc: 0.328125]\n",
      "2171: [discriminator loss: 0.682595, acc: 0.515625] [adversarial loss: 0.747520, acc: 0.343750]\n",
      "2172: [discriminator loss: 0.687914, acc: 0.593750] [adversarial loss: 0.756893, acc: 0.359375]\n",
      "2173: [discriminator loss: 0.665189, acc: 0.601562] [adversarial loss: 0.814173, acc: 0.312500]\n",
      "2174: [discriminator loss: 0.678363, acc: 0.578125] [adversarial loss: 0.814045, acc: 0.234375]\n",
      "2175: [discriminator loss: 0.630731, acc: 0.695312] [adversarial loss: 0.616901, acc: 0.671875]\n",
      "2176: [discriminator loss: 0.718680, acc: 0.515625] [adversarial loss: 1.010942, acc: 0.062500]\n",
      "2177: [discriminator loss: 0.703473, acc: 0.578125] [adversarial loss: 0.635364, acc: 0.687500]\n",
      "2178: [discriminator loss: 0.708441, acc: 0.515625] [adversarial loss: 1.048679, acc: 0.062500]\n",
      "2179: [discriminator loss: 0.742496, acc: 0.500000] [adversarial loss: 0.592510, acc: 0.781250]\n",
      "2180: [discriminator loss: 0.685795, acc: 0.562500] [adversarial loss: 0.992344, acc: 0.125000]\n",
      "2181: [discriminator loss: 0.718574, acc: 0.546875] [adversarial loss: 0.675584, acc: 0.593750]\n",
      "2182: [discriminator loss: 0.681827, acc: 0.578125] [adversarial loss: 0.752969, acc: 0.531250]\n",
      "2183: [discriminator loss: 0.650307, acc: 0.640625] [adversarial loss: 0.837220, acc: 0.281250]\n",
      "2184: [discriminator loss: 0.735482, acc: 0.476562] [adversarial loss: 0.656983, acc: 0.593750]\n",
      "2185: [discriminator loss: 0.689511, acc: 0.585938] [adversarial loss: 0.860496, acc: 0.218750]\n",
      "2186: [discriminator loss: 0.688879, acc: 0.484375] [adversarial loss: 0.772274, acc: 0.375000]\n",
      "2187: [discriminator loss: 0.687366, acc: 0.546875] [adversarial loss: 0.809755, acc: 0.203125]\n",
      "2188: [discriminator loss: 0.669971, acc: 0.640625] [adversarial loss: 0.701922, acc: 0.468750]\n",
      "2189: [discriminator loss: 0.702439, acc: 0.484375] [adversarial loss: 0.834571, acc: 0.250000]\n",
      "2190: [discriminator loss: 0.690417, acc: 0.539062] [adversarial loss: 0.696642, acc: 0.515625]\n",
      "2191: [discriminator loss: 0.673268, acc: 0.554688] [adversarial loss: 0.808226, acc: 0.296875]\n",
      "2192: [discriminator loss: 0.675156, acc: 0.585938] [adversarial loss: 0.724421, acc: 0.453125]\n",
      "2193: [discriminator loss: 0.675648, acc: 0.585938] [adversarial loss: 0.856956, acc: 0.250000]\n",
      "2194: [discriminator loss: 0.717305, acc: 0.492188] [adversarial loss: 0.665218, acc: 0.609375]\n",
      "2195: [discriminator loss: 0.680995, acc: 0.531250] [adversarial loss: 0.911966, acc: 0.140625]\n",
      "2196: [discriminator loss: 0.686928, acc: 0.546875] [adversarial loss: 0.595164, acc: 0.765625]\n",
      "2197: [discriminator loss: 0.655417, acc: 0.640625] [adversarial loss: 0.967718, acc: 0.125000]\n",
      "2198: [discriminator loss: 0.718805, acc: 0.507812] [adversarial loss: 0.593244, acc: 0.750000]\n",
      "2199: [discriminator loss: 0.700980, acc: 0.484375] [adversarial loss: 0.860269, acc: 0.250000]\n",
      "2200: [discriminator loss: 0.702150, acc: 0.546875] [adversarial loss: 0.624888, acc: 0.703125]\n",
      "2201: [discriminator loss: 0.673217, acc: 0.625000] [adversarial loss: 0.799352, acc: 0.437500]\n",
      "2202: [discriminator loss: 0.655925, acc: 0.609375] [adversarial loss: 0.890032, acc: 0.312500]\n",
      "2203: [discriminator loss: 0.710066, acc: 0.507812] [adversarial loss: 0.757591, acc: 0.375000]\n",
      "2204: [discriminator loss: 0.725899, acc: 0.507812] [adversarial loss: 1.025415, acc: 0.062500]\n",
      "2205: [discriminator loss: 0.704957, acc: 0.500000] [adversarial loss: 0.638189, acc: 0.671875]\n",
      "2206: [discriminator loss: 0.703174, acc: 0.492188] [adversarial loss: 1.000780, acc: 0.203125]\n",
      "2207: [discriminator loss: 0.697077, acc: 0.554688] [adversarial loss: 0.640419, acc: 0.750000]\n",
      "2208: [discriminator loss: 0.670578, acc: 0.593750] [adversarial loss: 0.912488, acc: 0.265625]\n",
      "2209: [discriminator loss: 0.668861, acc: 0.601562] [adversarial loss: 0.643480, acc: 0.671875]\n",
      "2210: [discriminator loss: 0.639356, acc: 0.632812] [adversarial loss: 0.892241, acc: 0.218750]\n",
      "2211: [discriminator loss: 0.723498, acc: 0.507812] [adversarial loss: 0.595532, acc: 0.765625]\n",
      "2212: [discriminator loss: 0.693419, acc: 0.546875] [adversarial loss: 0.854047, acc: 0.234375]\n",
      "2213: [discriminator loss: 0.698482, acc: 0.515625] [adversarial loss: 0.670185, acc: 0.609375]\n",
      "2214: [discriminator loss: 0.692082, acc: 0.562500] [adversarial loss: 0.772741, acc: 0.328125]\n",
      "2215: [discriminator loss: 0.683645, acc: 0.546875] [adversarial loss: 0.780147, acc: 0.328125]\n",
      "2216: [discriminator loss: 0.662813, acc: 0.609375] [adversarial loss: 0.760991, acc: 0.406250]\n",
      "2217: [discriminator loss: 0.686446, acc: 0.570312] [adversarial loss: 0.837730, acc: 0.187500]\n",
      "2218: [discriminator loss: 0.702106, acc: 0.593750] [adversarial loss: 0.713110, acc: 0.500000]\n",
      "2219: [discriminator loss: 0.670664, acc: 0.578125] [adversarial loss: 0.828923, acc: 0.265625]\n",
      "2220: [discriminator loss: 0.671551, acc: 0.554688] [adversarial loss: 0.668957, acc: 0.593750]\n",
      "2221: [discriminator loss: 0.697377, acc: 0.507812] [adversarial loss: 0.956409, acc: 0.062500]\n",
      "2222: [discriminator loss: 0.687426, acc: 0.570312] [adversarial loss: 0.698204, acc: 0.546875]\n",
      "2223: [discriminator loss: 0.661688, acc: 0.625000] [adversarial loss: 0.760468, acc: 0.343750]\n",
      "2224: [discriminator loss: 0.723167, acc: 0.445312] [adversarial loss: 0.843179, acc: 0.234375]\n",
      "2225: [discriminator loss: 0.675182, acc: 0.593750] [adversarial loss: 0.713440, acc: 0.484375]\n",
      "2226: [discriminator loss: 0.663348, acc: 0.593750] [adversarial loss: 0.719973, acc: 0.515625]\n",
      "2227: [discriminator loss: 0.669604, acc: 0.640625] [adversarial loss: 0.897378, acc: 0.187500]\n",
      "2228: [discriminator loss: 0.689576, acc: 0.570312] [adversarial loss: 0.688867, acc: 0.609375]\n",
      "2229: [discriminator loss: 0.683745, acc: 0.554688] [adversarial loss: 0.810641, acc: 0.265625]\n",
      "2230: [discriminator loss: 0.672990, acc: 0.562500] [adversarial loss: 0.727710, acc: 0.468750]\n",
      "2231: [discriminator loss: 0.696450, acc: 0.554688] [adversarial loss: 0.793296, acc: 0.343750]\n",
      "2232: [discriminator loss: 0.676925, acc: 0.570312] [adversarial loss: 0.614916, acc: 0.718750]\n",
      "2233: [discriminator loss: 0.724113, acc: 0.539062] [adversarial loss: 0.984393, acc: 0.109375]\n",
      "2234: [discriminator loss: 0.709184, acc: 0.523438] [adversarial loss: 0.536317, acc: 0.890625]\n",
      "2235: [discriminator loss: 0.686542, acc: 0.554688] [adversarial loss: 0.732426, acc: 0.390625]\n",
      "2236: [discriminator loss: 0.681887, acc: 0.554688] [adversarial loss: 0.766217, acc: 0.343750]\n",
      "2237: [discriminator loss: 0.696628, acc: 0.492188] [adversarial loss: 0.678354, acc: 0.546875]\n",
      "2238: [discriminator loss: 0.657444, acc: 0.648438] [adversarial loss: 0.753596, acc: 0.375000]\n",
      "2239: [discriminator loss: 0.670403, acc: 0.617188] [adversarial loss: 0.760507, acc: 0.359375]\n",
      "2240: [discriminator loss: 0.660268, acc: 0.625000] [adversarial loss: 0.876437, acc: 0.218750]\n",
      "2241: [discriminator loss: 0.715879, acc: 0.484375] [adversarial loss: 0.852866, acc: 0.250000]\n",
      "2242: [discriminator loss: 0.685558, acc: 0.570312] [adversarial loss: 0.754575, acc: 0.484375]\n",
      "2243: [discriminator loss: 0.679960, acc: 0.554688] [adversarial loss: 0.877788, acc: 0.171875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2244: [discriminator loss: 0.680680, acc: 0.562500] [adversarial loss: 0.563438, acc: 0.843750]\n",
      "2245: [discriminator loss: 0.684643, acc: 0.546875] [adversarial loss: 0.995327, acc: 0.093750]\n",
      "2246: [discriminator loss: 0.681102, acc: 0.585938] [adversarial loss: 0.628302, acc: 0.656250]\n",
      "2247: [discriminator loss: 0.681932, acc: 0.539062] [adversarial loss: 0.830434, acc: 0.296875]\n",
      "2248: [discriminator loss: 0.691642, acc: 0.546875] [adversarial loss: 0.694817, acc: 0.515625]\n",
      "2249: [discriminator loss: 0.664659, acc: 0.609375] [adversarial loss: 0.752491, acc: 0.406250]\n",
      "2250: [discriminator loss: 0.650216, acc: 0.656250] [adversarial loss: 0.830454, acc: 0.390625]\n",
      "2251: [discriminator loss: 0.672539, acc: 0.570312] [adversarial loss: 0.779269, acc: 0.453125]\n",
      "2252: [discriminator loss: 0.679788, acc: 0.539062] [adversarial loss: 0.668341, acc: 0.562500]\n",
      "2253: [discriminator loss: 0.682886, acc: 0.570312] [adversarial loss: 0.815188, acc: 0.281250]\n",
      "2254: [discriminator loss: 0.718306, acc: 0.492188] [adversarial loss: 0.570575, acc: 0.718750]\n",
      "2255: [discriminator loss: 0.685137, acc: 0.570312] [adversarial loss: 0.906137, acc: 0.062500]\n",
      "2256: [discriminator loss: 0.659318, acc: 0.617188] [adversarial loss: 0.661141, acc: 0.609375]\n",
      "2257: [discriminator loss: 0.669567, acc: 0.609375] [adversarial loss: 0.766910, acc: 0.375000]\n",
      "2258: [discriminator loss: 0.651948, acc: 0.648438] [adversarial loss: 0.738409, acc: 0.359375]\n",
      "2259: [discriminator loss: 0.678422, acc: 0.609375] [adversarial loss: 0.806913, acc: 0.296875]\n",
      "2260: [discriminator loss: 0.685196, acc: 0.593750] [adversarial loss: 0.684122, acc: 0.515625]\n",
      "2261: [discriminator loss: 0.679456, acc: 0.593750] [adversarial loss: 0.783926, acc: 0.328125]\n",
      "2262: [discriminator loss: 0.697112, acc: 0.539062] [adversarial loss: 0.795364, acc: 0.265625]\n",
      "2263: [discriminator loss: 0.655804, acc: 0.679688] [adversarial loss: 0.879325, acc: 0.281250]\n",
      "2264: [discriminator loss: 0.692710, acc: 0.562500] [adversarial loss: 0.721580, acc: 0.484375]\n",
      "2265: [discriminator loss: 0.674697, acc: 0.625000] [adversarial loss: 0.678130, acc: 0.546875]\n",
      "2266: [discriminator loss: 0.681358, acc: 0.570312] [adversarial loss: 0.946731, acc: 0.109375]\n",
      "2267: [discriminator loss: 0.695252, acc: 0.531250] [adversarial loss: 0.694377, acc: 0.546875]\n",
      "2268: [discriminator loss: 0.656841, acc: 0.601562] [adversarial loss: 0.964904, acc: 0.187500]\n",
      "2269: [discriminator loss: 0.679629, acc: 0.546875] [adversarial loss: 0.600342, acc: 0.703125]\n",
      "2270: [discriminator loss: 0.690784, acc: 0.570312] [adversarial loss: 0.705696, acc: 0.531250]\n",
      "2271: [discriminator loss: 0.672992, acc: 0.539062] [adversarial loss: 0.671068, acc: 0.609375]\n",
      "2272: [discriminator loss: 0.663027, acc: 0.632812] [adversarial loss: 0.810281, acc: 0.328125]\n",
      "2273: [discriminator loss: 0.681609, acc: 0.570312] [adversarial loss: 0.715140, acc: 0.500000]\n",
      "2274: [discriminator loss: 0.708665, acc: 0.468750] [adversarial loss: 0.828127, acc: 0.359375]\n",
      "2275: [discriminator loss: 0.689221, acc: 0.507812] [adversarial loss: 0.733814, acc: 0.531250]\n",
      "2276: [discriminator loss: 0.655460, acc: 0.625000] [adversarial loss: 0.866228, acc: 0.281250]\n",
      "2277: [discriminator loss: 0.695112, acc: 0.601562] [adversarial loss: 0.616901, acc: 0.687500]\n",
      "2278: [discriminator loss: 0.693006, acc: 0.554688] [adversarial loss: 1.117005, acc: 0.000000]\n",
      "2279: [discriminator loss: 0.730135, acc: 0.515625] [adversarial loss: 0.594916, acc: 0.703125]\n",
      "2280: [discriminator loss: 0.708386, acc: 0.570312] [adversarial loss: 0.741848, acc: 0.406250]\n",
      "2281: [discriminator loss: 0.665906, acc: 0.609375] [adversarial loss: 0.679304, acc: 0.500000]\n",
      "2282: [discriminator loss: 0.691300, acc: 0.546875] [adversarial loss: 0.763615, acc: 0.328125]\n",
      "2283: [discriminator loss: 0.660255, acc: 0.617188] [adversarial loss: 0.679360, acc: 0.500000]\n",
      "2284: [discriminator loss: 0.666438, acc: 0.617188] [adversarial loss: 0.731617, acc: 0.515625]\n",
      "2285: [discriminator loss: 0.686188, acc: 0.585938] [adversarial loss: 0.678358, acc: 0.546875]\n",
      "2286: [discriminator loss: 0.663708, acc: 0.617188] [adversarial loss: 0.780841, acc: 0.265625]\n",
      "2287: [discriminator loss: 0.643651, acc: 0.679688] [adversarial loss: 0.770596, acc: 0.390625]\n",
      "2288: [discriminator loss: 0.685673, acc: 0.539062] [adversarial loss: 0.742688, acc: 0.421875]\n",
      "2289: [discriminator loss: 0.676417, acc: 0.562500] [adversarial loss: 0.760975, acc: 0.500000]\n",
      "2290: [discriminator loss: 0.682721, acc: 0.523438] [adversarial loss: 0.828054, acc: 0.187500]\n",
      "2291: [discriminator loss: 0.677801, acc: 0.570312] [adversarial loss: 0.690822, acc: 0.500000]\n",
      "2292: [discriminator loss: 0.685288, acc: 0.554688] [adversarial loss: 0.890435, acc: 0.265625]\n",
      "2293: [discriminator loss: 0.697973, acc: 0.492188] [adversarial loss: 0.700540, acc: 0.484375]\n",
      "2294: [discriminator loss: 0.678281, acc: 0.617188] [adversarial loss: 0.708660, acc: 0.515625]\n",
      "2295: [discriminator loss: 0.686951, acc: 0.539062] [adversarial loss: 0.780075, acc: 0.375000]\n",
      "2296: [discriminator loss: 0.667744, acc: 0.570312] [adversarial loss: 0.749967, acc: 0.421875]\n",
      "2297: [discriminator loss: 0.684854, acc: 0.593750] [adversarial loss: 0.943678, acc: 0.078125]\n",
      "2298: [discriminator loss: 0.670108, acc: 0.578125] [adversarial loss: 0.616063, acc: 0.765625]\n",
      "2299: [discriminator loss: 0.680144, acc: 0.578125] [adversarial loss: 0.877909, acc: 0.171875]\n",
      "2300: [discriminator loss: 0.683791, acc: 0.609375] [adversarial loss: 0.621862, acc: 0.593750]\n",
      "2301: [discriminator loss: 0.675293, acc: 0.546875] [adversarial loss: 0.753665, acc: 0.343750]\n",
      "2302: [discriminator loss: 0.668245, acc: 0.570312] [adversarial loss: 0.722170, acc: 0.421875]\n",
      "2303: [discriminator loss: 0.715685, acc: 0.507812] [adversarial loss: 0.871297, acc: 0.171875]\n",
      "2304: [discriminator loss: 0.694059, acc: 0.554688] [adversarial loss: 0.811691, acc: 0.250000]\n",
      "2305: [discriminator loss: 0.646732, acc: 0.656250] [adversarial loss: 0.698829, acc: 0.515625]\n",
      "2306: [discriminator loss: 0.698675, acc: 0.523438] [adversarial loss: 0.898718, acc: 0.156250]\n",
      "2307: [discriminator loss: 0.657257, acc: 0.609375] [adversarial loss: 0.583629, acc: 0.765625]\n",
      "2308: [discriminator loss: 0.697201, acc: 0.554688] [adversarial loss: 0.956467, acc: 0.171875]\n",
      "2309: [discriminator loss: 0.679710, acc: 0.554688] [adversarial loss: 0.645588, acc: 0.640625]\n",
      "2310: [discriminator loss: 0.722791, acc: 0.531250] [adversarial loss: 0.949686, acc: 0.078125]\n",
      "2311: [discriminator loss: 0.662622, acc: 0.554688] [adversarial loss: 0.692064, acc: 0.515625]\n",
      "2312: [discriminator loss: 0.666790, acc: 0.601562] [adversarial loss: 0.890484, acc: 0.140625]\n",
      "2313: [discriminator loss: 0.698788, acc: 0.515625] [adversarial loss: 0.631791, acc: 0.718750]\n",
      "2314: [discriminator loss: 0.673236, acc: 0.562500] [adversarial loss: 0.792566, acc: 0.375000]\n",
      "2315: [discriminator loss: 0.704536, acc: 0.554688] [adversarial loss: 0.654460, acc: 0.609375]\n",
      "2316: [discriminator loss: 0.705525, acc: 0.492188] [adversarial loss: 0.771341, acc: 0.281250]\n",
      "2317: [discriminator loss: 0.695027, acc: 0.531250] [adversarial loss: 0.745024, acc: 0.406250]\n",
      "2318: [discriminator loss: 0.703927, acc: 0.492188] [adversarial loss: 0.735641, acc: 0.406250]\n",
      "2319: [discriminator loss: 0.705790, acc: 0.531250] [adversarial loss: 0.980665, acc: 0.062500]\n",
      "2320: [discriminator loss: 0.684414, acc: 0.531250] [adversarial loss: 0.580588, acc: 0.765625]\n",
      "2321: [discriminator loss: 0.696767, acc: 0.531250] [adversarial loss: 0.886888, acc: 0.109375]\n",
      "2322: [discriminator loss: 0.686106, acc: 0.601562] [adversarial loss: 0.664806, acc: 0.625000]\n",
      "2323: [discriminator loss: 0.718631, acc: 0.460938] [adversarial loss: 0.771432, acc: 0.312500]\n",
      "2324: [discriminator loss: 0.664950, acc: 0.609375] [adversarial loss: 0.664367, acc: 0.593750]\n",
      "2325: [discriminator loss: 0.651601, acc: 0.593750] [adversarial loss: 0.740127, acc: 0.390625]\n",
      "2326: [discriminator loss: 0.679213, acc: 0.570312] [adversarial loss: 0.717025, acc: 0.406250]\n",
      "2327: [discriminator loss: 0.683958, acc: 0.601562] [adversarial loss: 0.762101, acc: 0.343750]\n",
      "2328: [discriminator loss: 0.652559, acc: 0.687500] [adversarial loss: 0.689776, acc: 0.593750]\n",
      "2329: [discriminator loss: 0.693044, acc: 0.523438] [adversarial loss: 0.925557, acc: 0.140625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2330: [discriminator loss: 0.680122, acc: 0.593750] [adversarial loss: 0.681016, acc: 0.562500]\n",
      "2331: [discriminator loss: 0.678149, acc: 0.539062] [adversarial loss: 0.946827, acc: 0.062500]\n",
      "2332: [discriminator loss: 0.681408, acc: 0.570312] [adversarial loss: 0.591603, acc: 0.734375]\n",
      "2333: [discriminator loss: 0.655423, acc: 0.585938] [adversarial loss: 0.847342, acc: 0.281250]\n",
      "2334: [discriminator loss: 0.698567, acc: 0.507812] [adversarial loss: 0.796642, acc: 0.343750]\n",
      "2335: [discriminator loss: 0.681197, acc: 0.539062] [adversarial loss: 0.805160, acc: 0.328125]\n",
      "2336: [discriminator loss: 0.679857, acc: 0.554688] [adversarial loss: 0.647530, acc: 0.609375]\n",
      "2337: [discriminator loss: 0.675060, acc: 0.585938] [adversarial loss: 0.747918, acc: 0.328125]\n",
      "2338: [discriminator loss: 0.654209, acc: 0.609375] [adversarial loss: 0.677593, acc: 0.531250]\n",
      "2339: [discriminator loss: 0.708545, acc: 0.515625] [adversarial loss: 0.758661, acc: 0.375000]\n",
      "2340: [discriminator loss: 0.678724, acc: 0.546875] [adversarial loss: 0.794713, acc: 0.218750]\n",
      "2341: [discriminator loss: 0.674356, acc: 0.570312] [adversarial loss: 0.720818, acc: 0.500000]\n",
      "2342: [discriminator loss: 0.715928, acc: 0.476562] [adversarial loss: 0.887723, acc: 0.234375]\n",
      "2343: [discriminator loss: 0.701041, acc: 0.531250] [adversarial loss: 0.665730, acc: 0.625000]\n",
      "2344: [discriminator loss: 0.716017, acc: 0.484375] [adversarial loss: 0.868413, acc: 0.140625]\n",
      "2345: [discriminator loss: 0.664661, acc: 0.593750] [adversarial loss: 0.655954, acc: 0.578125]\n",
      "2346: [discriminator loss: 0.668862, acc: 0.664062] [adversarial loss: 0.814449, acc: 0.343750]\n",
      "2347: [discriminator loss: 0.705183, acc: 0.515625] [adversarial loss: 0.719179, acc: 0.453125]\n",
      "2348: [discriminator loss: 0.676471, acc: 0.578125] [adversarial loss: 0.748414, acc: 0.375000]\n",
      "2349: [discriminator loss: 0.703502, acc: 0.523438] [adversarial loss: 0.694227, acc: 0.500000]\n",
      "2350: [discriminator loss: 0.697359, acc: 0.515625] [adversarial loss: 0.818699, acc: 0.234375]\n",
      "2351: [discriminator loss: 0.677452, acc: 0.500000] [adversarial loss: 0.728557, acc: 0.500000]\n",
      "2352: [discriminator loss: 0.657516, acc: 0.656250] [adversarial loss: 0.884964, acc: 0.218750]\n",
      "2353: [discriminator loss: 0.678994, acc: 0.601562] [adversarial loss: 0.735337, acc: 0.453125]\n",
      "2354: [discriminator loss: 0.676636, acc: 0.578125] [adversarial loss: 0.762791, acc: 0.484375]\n",
      "2355: [discriminator loss: 0.702351, acc: 0.539062] [adversarial loss: 0.671013, acc: 0.578125]\n",
      "2356: [discriminator loss: 0.728645, acc: 0.507812] [adversarial loss: 0.994243, acc: 0.093750]\n",
      "2357: [discriminator loss: 0.692909, acc: 0.562500] [adversarial loss: 0.600310, acc: 0.765625]\n",
      "2358: [discriminator loss: 0.703743, acc: 0.492188] [adversarial loss: 0.863161, acc: 0.125000]\n",
      "2359: [discriminator loss: 0.661865, acc: 0.625000] [adversarial loss: 0.674576, acc: 0.562500]\n",
      "2360: [discriminator loss: 0.679314, acc: 0.554688] [adversarial loss: 0.810289, acc: 0.296875]\n",
      "2361: [discriminator loss: 0.653832, acc: 0.609375] [adversarial loss: 0.718017, acc: 0.546875]\n",
      "2362: [discriminator loss: 0.706309, acc: 0.492188] [adversarial loss: 0.646146, acc: 0.640625]\n",
      "2363: [discriminator loss: 0.681879, acc: 0.585938] [adversarial loss: 0.832073, acc: 0.312500]\n",
      "2364: [discriminator loss: 0.689201, acc: 0.601562] [adversarial loss: 0.747923, acc: 0.468750]\n",
      "2365: [discriminator loss: 0.653514, acc: 0.664062] [adversarial loss: 0.762898, acc: 0.359375]\n",
      "2366: [discriminator loss: 0.674139, acc: 0.585938] [adversarial loss: 0.650577, acc: 0.703125]\n",
      "2367: [discriminator loss: 0.655124, acc: 0.617188] [adversarial loss: 0.903420, acc: 0.171875]\n",
      "2368: [discriminator loss: 0.691130, acc: 0.570312] [adversarial loss: 0.671685, acc: 0.578125]\n",
      "2369: [discriminator loss: 0.704124, acc: 0.531250] [adversarial loss: 0.846167, acc: 0.218750]\n",
      "2370: [discriminator loss: 0.693599, acc: 0.507812] [adversarial loss: 0.656557, acc: 0.546875]\n",
      "2371: [discriminator loss: 0.668099, acc: 0.593750] [adversarial loss: 0.835714, acc: 0.234375]\n",
      "2372: [discriminator loss: 0.668240, acc: 0.617188] [adversarial loss: 0.640190, acc: 0.625000]\n",
      "2373: [discriminator loss: 0.675722, acc: 0.593750] [adversarial loss: 0.843245, acc: 0.218750]\n",
      "2374: [discriminator loss: 0.692286, acc: 0.500000] [adversarial loss: 0.692434, acc: 0.484375]\n",
      "2375: [discriminator loss: 0.664130, acc: 0.601562] [adversarial loss: 0.707062, acc: 0.484375]\n",
      "2376: [discriminator loss: 0.659309, acc: 0.640625] [adversarial loss: 0.871676, acc: 0.250000]\n",
      "2377: [discriminator loss: 0.700688, acc: 0.539062] [adversarial loss: 0.647536, acc: 0.640625]\n",
      "2378: [discriminator loss: 0.721405, acc: 0.484375] [adversarial loss: 0.944031, acc: 0.156250]\n",
      "2379: [discriminator loss: 0.686442, acc: 0.562500] [adversarial loss: 0.648681, acc: 0.609375]\n",
      "2380: [discriminator loss: 0.685794, acc: 0.593750] [adversarial loss: 0.921306, acc: 0.187500]\n",
      "2381: [discriminator loss: 0.664501, acc: 0.593750] [adversarial loss: 0.612000, acc: 0.671875]\n",
      "2382: [discriminator loss: 0.660457, acc: 0.601562] [adversarial loss: 0.899175, acc: 0.218750]\n",
      "2383: [discriminator loss: 0.701148, acc: 0.531250] [adversarial loss: 0.693144, acc: 0.609375]\n",
      "2384: [discriminator loss: 0.672906, acc: 0.585938] [adversarial loss: 0.784447, acc: 0.359375]\n",
      "2385: [discriminator loss: 0.673126, acc: 0.632812] [adversarial loss: 0.796936, acc: 0.406250]\n",
      "2386: [discriminator loss: 0.676437, acc: 0.570312] [adversarial loss: 0.874167, acc: 0.265625]\n",
      "2387: [discriminator loss: 0.690852, acc: 0.562500] [adversarial loss: 0.658575, acc: 0.671875]\n",
      "2388: [discriminator loss: 0.712235, acc: 0.531250] [adversarial loss: 1.028432, acc: 0.171875]\n",
      "2389: [discriminator loss: 0.684550, acc: 0.578125] [adversarial loss: 0.548424, acc: 0.859375]\n",
      "2390: [discriminator loss: 0.712477, acc: 0.492188] [adversarial loss: 0.870675, acc: 0.203125]\n",
      "2391: [discriminator loss: 0.695935, acc: 0.539062] [adversarial loss: 0.613031, acc: 0.656250]\n",
      "2392: [discriminator loss: 0.663281, acc: 0.585938] [adversarial loss: 0.731905, acc: 0.421875]\n",
      "2393: [discriminator loss: 0.668369, acc: 0.601562] [adversarial loss: 0.809569, acc: 0.343750]\n",
      "2394: [discriminator loss: 0.641886, acc: 0.656250] [adversarial loss: 0.687419, acc: 0.578125]\n",
      "2395: [discriminator loss: 0.675185, acc: 0.585938] [adversarial loss: 0.764496, acc: 0.437500]\n",
      "2396: [discriminator loss: 0.689354, acc: 0.539062] [adversarial loss: 0.837168, acc: 0.171875]\n",
      "2397: [discriminator loss: 0.656037, acc: 0.562500] [adversarial loss: 0.759417, acc: 0.468750]\n",
      "2398: [discriminator loss: 0.698271, acc: 0.515625] [adversarial loss: 0.704352, acc: 0.421875]\n",
      "2399: [discriminator loss: 0.676129, acc: 0.562500] [adversarial loss: 0.911048, acc: 0.218750]\n",
      "2400: [discriminator loss: 0.670602, acc: 0.609375] [adversarial loss: 0.571523, acc: 0.796875]\n",
      "2401: [discriminator loss: 0.693471, acc: 0.546875] [adversarial loss: 0.850901, acc: 0.250000]\n",
      "2402: [discriminator loss: 0.685525, acc: 0.546875] [adversarial loss: 0.656544, acc: 0.640625]\n",
      "2403: [discriminator loss: 0.692967, acc: 0.562500] [adversarial loss: 0.723711, acc: 0.437500]\n",
      "2404: [discriminator loss: 0.683831, acc: 0.554688] [adversarial loss: 0.839849, acc: 0.218750]\n",
      "2405: [discriminator loss: 0.669111, acc: 0.570312] [adversarial loss: 0.634845, acc: 0.671875]\n",
      "2406: [discriminator loss: 0.695345, acc: 0.515625] [adversarial loss: 0.916343, acc: 0.218750]\n",
      "2407: [discriminator loss: 0.700873, acc: 0.570312] [adversarial loss: 0.663389, acc: 0.640625]\n",
      "2408: [discriminator loss: 0.665661, acc: 0.570312] [adversarial loss: 0.822571, acc: 0.281250]\n",
      "2409: [discriminator loss: 0.668649, acc: 0.593750] [adversarial loss: 0.713850, acc: 0.421875]\n",
      "2410: [discriminator loss: 0.717817, acc: 0.531250] [adversarial loss: 0.804970, acc: 0.312500]\n",
      "2411: [discriminator loss: 0.674513, acc: 0.617188] [adversarial loss: 0.663621, acc: 0.546875]\n",
      "2412: [discriminator loss: 0.686421, acc: 0.585938] [adversarial loss: 0.909635, acc: 0.125000]\n",
      "2413: [discriminator loss: 0.637979, acc: 0.656250] [adversarial loss: 0.653742, acc: 0.515625]\n",
      "2414: [discriminator loss: 0.707115, acc: 0.507812] [adversarial loss: 0.866127, acc: 0.234375]\n",
      "2415: [discriminator loss: 0.697130, acc: 0.507812] [adversarial loss: 0.618822, acc: 0.734375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2416: [discriminator loss: 0.699800, acc: 0.531250] [adversarial loss: 0.883456, acc: 0.203125]\n",
      "2417: [discriminator loss: 0.716388, acc: 0.507812] [adversarial loss: 0.596517, acc: 0.781250]\n",
      "2418: [discriminator loss: 0.665390, acc: 0.640625] [adversarial loss: 0.876371, acc: 0.296875]\n",
      "2419: [discriminator loss: 0.699980, acc: 0.585938] [adversarial loss: 0.725204, acc: 0.484375]\n",
      "2420: [discriminator loss: 0.680447, acc: 0.546875] [adversarial loss: 0.845697, acc: 0.281250]\n",
      "2421: [discriminator loss: 0.702646, acc: 0.515625] [adversarial loss: 0.644876, acc: 0.625000]\n",
      "2422: [discriminator loss: 0.676583, acc: 0.554688] [adversarial loss: 0.937921, acc: 0.156250]\n",
      "2423: [discriminator loss: 0.699239, acc: 0.546875] [adversarial loss: 0.652928, acc: 0.562500]\n",
      "2424: [discriminator loss: 0.678804, acc: 0.546875] [adversarial loss: 0.867195, acc: 0.093750]\n",
      "2425: [discriminator loss: 0.686064, acc: 0.578125] [adversarial loss: 0.711923, acc: 0.500000]\n",
      "2426: [discriminator loss: 0.670079, acc: 0.562500] [adversarial loss: 0.895729, acc: 0.250000]\n",
      "2427: [discriminator loss: 0.690008, acc: 0.578125] [adversarial loss: 0.615421, acc: 0.578125]\n",
      "2428: [discriminator loss: 0.698782, acc: 0.562500] [adversarial loss: 1.000027, acc: 0.031250]\n",
      "2429: [discriminator loss: 0.679059, acc: 0.585938] [adversarial loss: 0.698230, acc: 0.531250]\n",
      "2430: [discriminator loss: 0.663100, acc: 0.554688] [adversarial loss: 0.697227, acc: 0.578125]\n",
      "2431: [discriminator loss: 0.669800, acc: 0.570312] [adversarial loss: 0.755393, acc: 0.328125]\n",
      "2432: [discriminator loss: 0.683577, acc: 0.617188] [adversarial loss: 0.879125, acc: 0.234375]\n",
      "2433: [discriminator loss: 0.658265, acc: 0.601562] [adversarial loss: 0.656451, acc: 0.578125]\n",
      "2434: [discriminator loss: 0.723325, acc: 0.562500] [adversarial loss: 0.922648, acc: 0.156250]\n",
      "2435: [discriminator loss: 0.700451, acc: 0.484375] [adversarial loss: 0.735928, acc: 0.375000]\n",
      "2436: [discriminator loss: 0.662807, acc: 0.632812] [adversarial loss: 0.882278, acc: 0.140625]\n",
      "2437: [discriminator loss: 0.687303, acc: 0.515625] [adversarial loss: 0.608291, acc: 0.718750]\n",
      "2438: [discriminator loss: 0.680038, acc: 0.578125] [adversarial loss: 0.894465, acc: 0.171875]\n",
      "2439: [discriminator loss: 0.684195, acc: 0.585938] [adversarial loss: 0.586088, acc: 0.734375]\n",
      "2440: [discriminator loss: 0.697987, acc: 0.507812] [adversarial loss: 0.918611, acc: 0.109375]\n",
      "2441: [discriminator loss: 0.659275, acc: 0.609375] [adversarial loss: 0.659447, acc: 0.578125]\n",
      "2442: [discriminator loss: 0.705055, acc: 0.554688] [adversarial loss: 0.881946, acc: 0.187500]\n",
      "2443: [discriminator loss: 0.672113, acc: 0.562500] [adversarial loss: 0.635539, acc: 0.671875]\n",
      "2444: [discriminator loss: 0.709673, acc: 0.539062] [adversarial loss: 0.714020, acc: 0.468750]\n",
      "2445: [discriminator loss: 0.679203, acc: 0.531250] [adversarial loss: 0.728986, acc: 0.421875]\n",
      "2446: [discriminator loss: 0.675701, acc: 0.578125] [adversarial loss: 0.766273, acc: 0.328125]\n",
      "2447: [discriminator loss: 0.653193, acc: 0.640625] [adversarial loss: 0.655932, acc: 0.703125]\n",
      "2448: [discriminator loss: 0.682567, acc: 0.507812] [adversarial loss: 0.972256, acc: 0.250000]\n",
      "2449: [discriminator loss: 0.701168, acc: 0.531250] [adversarial loss: 0.609186, acc: 0.718750]\n",
      "2450: [discriminator loss: 0.689234, acc: 0.546875] [adversarial loss: 0.895226, acc: 0.250000]\n",
      "2451: [discriminator loss: 0.694784, acc: 0.507812] [adversarial loss: 0.701713, acc: 0.531250]\n",
      "2452: [discriminator loss: 0.692950, acc: 0.507812] [adversarial loss: 0.784373, acc: 0.359375]\n",
      "2453: [discriminator loss: 0.703366, acc: 0.515625] [adversarial loss: 0.788362, acc: 0.359375]\n",
      "2454: [discriminator loss: 0.680978, acc: 0.578125] [adversarial loss: 0.711894, acc: 0.468750]\n",
      "2455: [discriminator loss: 0.660882, acc: 0.625000] [adversarial loss: 0.874627, acc: 0.234375]\n",
      "2456: [discriminator loss: 0.690296, acc: 0.585938] [adversarial loss: 0.616324, acc: 0.640625]\n",
      "2457: [discriminator loss: 0.720578, acc: 0.523438] [adversarial loss: 0.969885, acc: 0.125000]\n",
      "2458: [discriminator loss: 0.714516, acc: 0.570312] [adversarial loss: 0.648487, acc: 0.640625]\n",
      "2459: [discriminator loss: 0.690751, acc: 0.523438] [adversarial loss: 0.785705, acc: 0.328125]\n",
      "2460: [discriminator loss: 0.696509, acc: 0.515625] [adversarial loss: 0.708616, acc: 0.531250]\n",
      "2461: [discriminator loss: 0.678594, acc: 0.539062] [adversarial loss: 0.767401, acc: 0.406250]\n",
      "2462: [discriminator loss: 0.687777, acc: 0.523438] [adversarial loss: 0.679013, acc: 0.562500]\n",
      "2463: [discriminator loss: 0.715991, acc: 0.515625] [adversarial loss: 0.780103, acc: 0.296875]\n",
      "2464: [discriminator loss: 0.653913, acc: 0.609375] [adversarial loss: 0.831015, acc: 0.218750]\n",
      "2465: [discriminator loss: 0.654655, acc: 0.687500] [adversarial loss: 0.665245, acc: 0.687500]\n",
      "2466: [discriminator loss: 0.666465, acc: 0.601562] [adversarial loss: 0.898088, acc: 0.171875]\n",
      "2467: [discriminator loss: 0.677897, acc: 0.593750] [adversarial loss: 0.725051, acc: 0.531250]\n",
      "2468: [discriminator loss: 0.682613, acc: 0.625000] [adversarial loss: 0.899173, acc: 0.203125]\n",
      "2469: [discriminator loss: 0.686053, acc: 0.570312] [adversarial loss: 0.717941, acc: 0.468750]\n",
      "2470: [discriminator loss: 0.699679, acc: 0.531250] [adversarial loss: 0.937068, acc: 0.187500]\n",
      "2471: [discriminator loss: 0.677365, acc: 0.585938] [adversarial loss: 0.658402, acc: 0.578125]\n",
      "2472: [discriminator loss: 0.661531, acc: 0.601562] [adversarial loss: 0.783510, acc: 0.359375]\n",
      "2473: [discriminator loss: 0.709652, acc: 0.468750] [adversarial loss: 0.725996, acc: 0.500000]\n",
      "2474: [discriminator loss: 0.700819, acc: 0.570312] [adversarial loss: 0.840762, acc: 0.156250]\n",
      "2475: [discriminator loss: 0.695275, acc: 0.546875] [adversarial loss: 0.609929, acc: 0.718750]\n",
      "2476: [discriminator loss: 0.681361, acc: 0.546875] [adversarial loss: 0.831173, acc: 0.203125]\n",
      "2477: [discriminator loss: 0.672350, acc: 0.601562] [adversarial loss: 0.693812, acc: 0.515625]\n",
      "2478: [discriminator loss: 0.675673, acc: 0.578125] [adversarial loss: 0.783512, acc: 0.375000]\n",
      "2479: [discriminator loss: 0.713148, acc: 0.515625] [adversarial loss: 0.778282, acc: 0.312500]\n",
      "2480: [discriminator loss: 0.684513, acc: 0.585938] [adversarial loss: 0.759894, acc: 0.375000]\n",
      "2481: [discriminator loss: 0.683172, acc: 0.523438] [adversarial loss: 0.679262, acc: 0.562500]\n",
      "2482: [discriminator loss: 0.668252, acc: 0.578125] [adversarial loss: 0.871144, acc: 0.171875]\n",
      "2483: [discriminator loss: 0.702789, acc: 0.570312] [adversarial loss: 0.675613, acc: 0.531250]\n",
      "2484: [discriminator loss: 0.658263, acc: 0.609375] [adversarial loss: 0.876830, acc: 0.218750]\n",
      "2485: [discriminator loss: 0.672110, acc: 0.562500] [adversarial loss: 0.627831, acc: 0.687500]\n",
      "2486: [discriminator loss: 0.660477, acc: 0.570312] [adversarial loss: 0.877929, acc: 0.250000]\n",
      "2487: [discriminator loss: 0.672957, acc: 0.601562] [adversarial loss: 0.667466, acc: 0.578125]\n",
      "2488: [discriminator loss: 0.707060, acc: 0.554688] [adversarial loss: 0.720887, acc: 0.453125]\n",
      "2489: [discriminator loss: 0.682051, acc: 0.578125] [adversarial loss: 0.699884, acc: 0.468750]\n",
      "2490: [discriminator loss: 0.670627, acc: 0.593750] [adversarial loss: 0.751393, acc: 0.390625]\n",
      "2491: [discriminator loss: 0.674986, acc: 0.578125] [adversarial loss: 0.803237, acc: 0.265625]\n",
      "2492: [discriminator loss: 0.715478, acc: 0.476562] [adversarial loss: 0.834850, acc: 0.296875]\n",
      "2493: [discriminator loss: 0.676888, acc: 0.585938] [adversarial loss: 0.722706, acc: 0.468750]\n",
      "2494: [discriminator loss: 0.681148, acc: 0.546875] [adversarial loss: 0.900518, acc: 0.218750]\n",
      "2495: [discriminator loss: 0.697825, acc: 0.539062] [adversarial loss: 0.621005, acc: 0.656250]\n",
      "2496: [discriminator loss: 0.734721, acc: 0.500000] [adversarial loss: 0.949755, acc: 0.250000]\n",
      "2497: [discriminator loss: 0.671923, acc: 0.578125] [adversarial loss: 0.675419, acc: 0.578125]\n",
      "2498: [discriminator loss: 0.738939, acc: 0.476562] [adversarial loss: 0.969922, acc: 0.140625]\n",
      "2499: [discriminator loss: 0.719031, acc: 0.492188] [adversarial loss: 0.616990, acc: 0.734375]\n",
      "cgan_mnist  labels for generated images:  [0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5]\n",
      "2500: [discriminator loss: 0.696993, acc: 0.515625] [adversarial loss: 0.911871, acc: 0.093750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2501: [discriminator loss: 0.670888, acc: 0.562500] [adversarial loss: 0.767749, acc: 0.453125]\n",
      "2502: [discriminator loss: 0.732806, acc: 0.507812] [adversarial loss: 0.714775, acc: 0.468750]\n",
      "2503: [discriminator loss: 0.668242, acc: 0.609375] [adversarial loss: 0.657647, acc: 0.609375]\n",
      "2504: [discriminator loss: 0.698336, acc: 0.554688] [adversarial loss: 0.684898, acc: 0.593750]\n",
      "2505: [discriminator loss: 0.658040, acc: 0.570312] [adversarial loss: 0.710627, acc: 0.468750]\n",
      "2506: [discriminator loss: 0.702572, acc: 0.523438] [adversarial loss: 0.800332, acc: 0.265625]\n",
      "2507: [discriminator loss: 0.667620, acc: 0.570312] [adversarial loss: 0.842021, acc: 0.203125]\n",
      "2508: [discriminator loss: 0.687858, acc: 0.531250] [adversarial loss: 0.763558, acc: 0.421875]\n",
      "2509: [discriminator loss: 0.686576, acc: 0.562500] [adversarial loss: 0.840304, acc: 0.250000]\n",
      "2510: [discriminator loss: 0.668453, acc: 0.593750] [adversarial loss: 0.687667, acc: 0.515625]\n",
      "2511: [discriminator loss: 0.673317, acc: 0.570312] [adversarial loss: 0.791053, acc: 0.296875]\n",
      "2512: [discriminator loss: 0.654376, acc: 0.601562] [adversarial loss: 0.651438, acc: 0.656250]\n",
      "2513: [discriminator loss: 0.686128, acc: 0.507812] [adversarial loss: 0.812619, acc: 0.406250]\n",
      "2514: [discriminator loss: 0.690494, acc: 0.585938] [adversarial loss: 0.717385, acc: 0.531250]\n",
      "2515: [discriminator loss: 0.693371, acc: 0.523438] [adversarial loss: 0.676315, acc: 0.609375]\n",
      "2516: [discriminator loss: 0.656629, acc: 0.593750] [adversarial loss: 0.742663, acc: 0.375000]\n",
      "2517: [discriminator loss: 0.680435, acc: 0.562500] [adversarial loss: 0.673635, acc: 0.593750]\n",
      "2518: [discriminator loss: 0.703883, acc: 0.460938] [adversarial loss: 0.697664, acc: 0.578125]\n",
      "2519: [discriminator loss: 0.704367, acc: 0.484375] [adversarial loss: 0.850634, acc: 0.187500]\n",
      "2520: [discriminator loss: 0.694808, acc: 0.546875] [adversarial loss: 0.911719, acc: 0.234375]\n",
      "2521: [discriminator loss: 0.701227, acc: 0.585938] [adversarial loss: 0.715005, acc: 0.500000]\n",
      "2522: [discriminator loss: 0.681828, acc: 0.531250] [adversarial loss: 0.876003, acc: 0.140625]\n",
      "2523: [discriminator loss: 0.671859, acc: 0.593750] [adversarial loss: 0.668591, acc: 0.640625]\n",
      "2524: [discriminator loss: 0.710944, acc: 0.484375] [adversarial loss: 0.843549, acc: 0.187500]\n",
      "2525: [discriminator loss: 0.691456, acc: 0.500000] [adversarial loss: 0.650308, acc: 0.578125]\n",
      "2526: [discriminator loss: 0.690184, acc: 0.531250] [adversarial loss: 0.874339, acc: 0.156250]\n",
      "2527: [discriminator loss: 0.679368, acc: 0.523438] [adversarial loss: 0.641857, acc: 0.703125]\n",
      "2528: [discriminator loss: 0.662711, acc: 0.593750] [adversarial loss: 0.837994, acc: 0.281250]\n",
      "2529: [discriminator loss: 0.695425, acc: 0.500000] [adversarial loss: 0.624560, acc: 0.718750]\n",
      "2530: [discriminator loss: 0.677614, acc: 0.578125] [adversarial loss: 0.799847, acc: 0.203125]\n",
      "2531: [discriminator loss: 0.666283, acc: 0.617188] [adversarial loss: 0.809132, acc: 0.328125]\n",
      "2532: [discriminator loss: 0.689647, acc: 0.539062] [adversarial loss: 0.721325, acc: 0.453125]\n",
      "2533: [discriminator loss: 0.690893, acc: 0.523438] [adversarial loss: 0.773298, acc: 0.406250]\n",
      "2534: [discriminator loss: 0.686611, acc: 0.539062] [adversarial loss: 0.787985, acc: 0.312500]\n",
      "2535: [discriminator loss: 0.664698, acc: 0.617188] [adversarial loss: 0.674512, acc: 0.593750]\n",
      "2536: [discriminator loss: 0.651833, acc: 0.640625] [adversarial loss: 0.971683, acc: 0.125000]\n",
      "2537: [discriminator loss: 0.704915, acc: 0.554688] [adversarial loss: 0.532394, acc: 0.859375]\n",
      "2538: [discriminator loss: 0.676053, acc: 0.593750] [adversarial loss: 0.926350, acc: 0.234375]\n",
      "2539: [discriminator loss: 0.732301, acc: 0.500000] [adversarial loss: 0.639722, acc: 0.718750]\n",
      "2540: [discriminator loss: 0.677041, acc: 0.562500] [adversarial loss: 0.671776, acc: 0.578125]\n",
      "2541: [discriminator loss: 0.713050, acc: 0.484375] [adversarial loss: 0.798111, acc: 0.375000]\n",
      "2542: [discriminator loss: 0.671755, acc: 0.546875] [adversarial loss: 0.662502, acc: 0.562500]\n",
      "2543: [discriminator loss: 0.692661, acc: 0.531250] [adversarial loss: 0.946150, acc: 0.046875]\n",
      "2544: [discriminator loss: 0.690868, acc: 0.546875] [adversarial loss: 0.644418, acc: 0.671875]\n",
      "2545: [discriminator loss: 0.707635, acc: 0.492188] [adversarial loss: 0.924389, acc: 0.109375]\n",
      "2546: [discriminator loss: 0.689998, acc: 0.554688] [adversarial loss: 0.595538, acc: 0.750000]\n",
      "2547: [discriminator loss: 0.681225, acc: 0.617188] [adversarial loss: 0.826085, acc: 0.296875]\n",
      "2548: [discriminator loss: 0.723776, acc: 0.507812] [adversarial loss: 0.676765, acc: 0.562500]\n",
      "2549: [discriminator loss: 0.671609, acc: 0.625000] [adversarial loss: 0.882391, acc: 0.203125]\n",
      "2550: [discriminator loss: 0.698125, acc: 0.546875] [adversarial loss: 0.690231, acc: 0.515625]\n",
      "2551: [discriminator loss: 0.677047, acc: 0.546875] [adversarial loss: 0.789719, acc: 0.312500]\n",
      "2552: [discriminator loss: 0.689346, acc: 0.531250] [adversarial loss: 0.707602, acc: 0.515625]\n",
      "2553: [discriminator loss: 0.692948, acc: 0.531250] [adversarial loss: 0.765676, acc: 0.375000]\n",
      "2554: [discriminator loss: 0.680153, acc: 0.554688] [adversarial loss: 0.788235, acc: 0.265625]\n",
      "2555: [discriminator loss: 0.655783, acc: 0.656250] [adversarial loss: 0.769156, acc: 0.500000]\n",
      "2556: [discriminator loss: 0.742875, acc: 0.484375] [adversarial loss: 0.734392, acc: 0.359375]\n",
      "2557: [discriminator loss: 0.664110, acc: 0.601562] [adversarial loss: 0.819894, acc: 0.312500]\n",
      "2558: [discriminator loss: 0.680293, acc: 0.562500] [adversarial loss: 0.638482, acc: 0.625000]\n",
      "2559: [discriminator loss: 0.653234, acc: 0.625000] [adversarial loss: 0.683033, acc: 0.578125]\n",
      "2560: [discriminator loss: 0.661651, acc: 0.632812] [adversarial loss: 0.636102, acc: 0.718750]\n",
      "2561: [discriminator loss: 0.693798, acc: 0.500000] [adversarial loss: 0.852380, acc: 0.203125]\n",
      "2562: [discriminator loss: 0.688054, acc: 0.554688] [adversarial loss: 0.644036, acc: 0.578125]\n",
      "2563: [discriminator loss: 0.679841, acc: 0.539062] [adversarial loss: 0.757556, acc: 0.296875]\n",
      "2564: [discriminator loss: 0.679604, acc: 0.593750] [adversarial loss: 0.700197, acc: 0.531250]\n",
      "2565: [discriminator loss: 0.657777, acc: 0.625000] [adversarial loss: 0.894968, acc: 0.140625]\n",
      "2566: [discriminator loss: 0.686647, acc: 0.585938] [adversarial loss: 0.657646, acc: 0.656250]\n",
      "2567: [discriminator loss: 0.689668, acc: 0.601562] [adversarial loss: 0.814326, acc: 0.218750]\n",
      "2568: [discriminator loss: 0.691085, acc: 0.539062] [adversarial loss: 0.669171, acc: 0.593750]\n",
      "2569: [discriminator loss: 0.695720, acc: 0.570312] [adversarial loss: 0.823930, acc: 0.296875]\n",
      "2570: [discriminator loss: 0.649944, acc: 0.679688] [adversarial loss: 0.841828, acc: 0.296875]\n",
      "2571: [discriminator loss: 0.671402, acc: 0.617188] [adversarial loss: 0.600664, acc: 0.765625]\n",
      "2572: [discriminator loss: 0.725083, acc: 0.523438] [adversarial loss: 0.947877, acc: 0.078125]\n",
      "2573: [discriminator loss: 0.681107, acc: 0.539062] [adversarial loss: 0.673036, acc: 0.578125]\n",
      "2574: [discriminator loss: 0.682292, acc: 0.570312] [adversarial loss: 0.822907, acc: 0.312500]\n",
      "2575: [discriminator loss: 0.671755, acc: 0.593750] [adversarial loss: 0.712477, acc: 0.437500]\n",
      "2576: [discriminator loss: 0.681683, acc: 0.500000] [adversarial loss: 0.869455, acc: 0.140625]\n",
      "2577: [discriminator loss: 0.677928, acc: 0.609375] [adversarial loss: 0.708628, acc: 0.500000]\n",
      "2578: [discriminator loss: 0.684357, acc: 0.617188] [adversarial loss: 0.867178, acc: 0.234375]\n",
      "2579: [discriminator loss: 0.697753, acc: 0.515625] [adversarial loss: 0.637651, acc: 0.640625]\n",
      "2580: [discriminator loss: 0.729492, acc: 0.437500] [adversarial loss: 0.861622, acc: 0.218750]\n",
      "2581: [discriminator loss: 0.703704, acc: 0.507812] [adversarial loss: 0.731338, acc: 0.328125]\n",
      "2582: [discriminator loss: 0.704861, acc: 0.476562] [adversarial loss: 0.755568, acc: 0.437500]\n",
      "2583: [discriminator loss: 0.685665, acc: 0.562500] [adversarial loss: 0.670588, acc: 0.593750]\n",
      "2584: [discriminator loss: 0.666118, acc: 0.593750] [adversarial loss: 0.773036, acc: 0.437500]\n",
      "2585: [discriminator loss: 0.756136, acc: 0.453125] [adversarial loss: 0.777772, acc: 0.296875]\n",
      "2586: [discriminator loss: 0.690054, acc: 0.539062] [adversarial loss: 0.740736, acc: 0.406250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2587: [discriminator loss: 0.671323, acc: 0.570312] [adversarial loss: 0.810227, acc: 0.265625]\n",
      "2588: [discriminator loss: 0.675971, acc: 0.539062] [adversarial loss: 0.731012, acc: 0.421875]\n",
      "2589: [discriminator loss: 0.683565, acc: 0.539062] [adversarial loss: 0.813752, acc: 0.234375]\n",
      "2590: [discriminator loss: 0.656493, acc: 0.664062] [adversarial loss: 0.831442, acc: 0.265625]\n",
      "2591: [discriminator loss: 0.666886, acc: 0.632812] [adversarial loss: 0.666237, acc: 0.625000]\n",
      "2592: [discriminator loss: 0.671682, acc: 0.609375] [adversarial loss: 0.796122, acc: 0.250000]\n",
      "2593: [discriminator loss: 0.670125, acc: 0.601562] [adversarial loss: 0.636109, acc: 0.640625]\n",
      "2594: [discriminator loss: 0.669003, acc: 0.585938] [adversarial loss: 0.887600, acc: 0.171875]\n",
      "2595: [discriminator loss: 0.706217, acc: 0.492188] [adversarial loss: 0.713986, acc: 0.500000]\n",
      "2596: [discriminator loss: 0.658834, acc: 0.609375] [adversarial loss: 0.778156, acc: 0.390625]\n",
      "2597: [discriminator loss: 0.700518, acc: 0.562500] [adversarial loss: 0.781230, acc: 0.375000]\n",
      "2598: [discriminator loss: 0.697713, acc: 0.546875] [adversarial loss: 0.860151, acc: 0.203125]\n",
      "2599: [discriminator loss: 0.669644, acc: 0.539062] [adversarial loss: 0.662771, acc: 0.609375]\n",
      "2600: [discriminator loss: 0.676039, acc: 0.539062] [adversarial loss: 1.024984, acc: 0.093750]\n",
      "2601: [discriminator loss: 0.734442, acc: 0.554688] [adversarial loss: 0.536327, acc: 0.875000]\n",
      "2602: [discriminator loss: 0.695445, acc: 0.554688] [adversarial loss: 0.851403, acc: 0.265625]\n",
      "2603: [discriminator loss: 0.691028, acc: 0.562500] [adversarial loss: 0.641152, acc: 0.687500]\n",
      "2604: [discriminator loss: 0.685456, acc: 0.539062] [adversarial loss: 0.813976, acc: 0.312500]\n",
      "2605: [discriminator loss: 0.691311, acc: 0.546875] [adversarial loss: 0.704714, acc: 0.453125]\n",
      "2606: [discriminator loss: 0.705508, acc: 0.492188] [adversarial loss: 0.883108, acc: 0.109375]\n",
      "2607: [discriminator loss: 0.676825, acc: 0.562500] [adversarial loss: 0.714326, acc: 0.468750]\n",
      "2608: [discriminator loss: 0.665261, acc: 0.593750] [adversarial loss: 0.824179, acc: 0.234375]\n",
      "2609: [discriminator loss: 0.677359, acc: 0.609375] [adversarial loss: 0.727371, acc: 0.484375]\n",
      "2610: [discriminator loss: 0.708782, acc: 0.539062] [adversarial loss: 0.679563, acc: 0.562500]\n",
      "2611: [discriminator loss: 0.673605, acc: 0.531250] [adversarial loss: 0.807701, acc: 0.312500]\n",
      "2612: [discriminator loss: 0.664910, acc: 0.585938] [adversarial loss: 0.755320, acc: 0.406250]\n",
      "2613: [discriminator loss: 0.687315, acc: 0.468750] [adversarial loss: 0.667687, acc: 0.562500]\n",
      "2614: [discriminator loss: 0.695260, acc: 0.523438] [adversarial loss: 0.868828, acc: 0.156250]\n",
      "2615: [discriminator loss: 0.663634, acc: 0.601562] [adversarial loss: 0.572969, acc: 0.781250]\n",
      "2616: [discriminator loss: 0.711400, acc: 0.546875] [adversarial loss: 0.825845, acc: 0.390625]\n",
      "2617: [discriminator loss: 0.662024, acc: 0.617188] [adversarial loss: 0.718055, acc: 0.437500]\n",
      "2618: [discriminator loss: 0.673934, acc: 0.609375] [adversarial loss: 0.753639, acc: 0.406250]\n",
      "2619: [discriminator loss: 0.682755, acc: 0.578125] [adversarial loss: 0.716186, acc: 0.468750]\n",
      "2620: [discriminator loss: 0.680674, acc: 0.562500] [adversarial loss: 0.722310, acc: 0.437500]\n",
      "2621: [discriminator loss: 0.666368, acc: 0.609375] [adversarial loss: 0.782074, acc: 0.375000]\n",
      "2622: [discriminator loss: 0.679033, acc: 0.585938] [adversarial loss: 0.767842, acc: 0.406250]\n",
      "2623: [discriminator loss: 0.684135, acc: 0.585938] [adversarial loss: 0.826122, acc: 0.281250]\n",
      "2624: [discriminator loss: 0.688600, acc: 0.507812] [adversarial loss: 0.605957, acc: 0.796875]\n",
      "2625: [discriminator loss: 0.675708, acc: 0.562500] [adversarial loss: 0.852022, acc: 0.171875]\n",
      "2626: [discriminator loss: 0.670499, acc: 0.570312] [adversarial loss: 0.688031, acc: 0.515625]\n",
      "2627: [discriminator loss: 0.696968, acc: 0.515625] [adversarial loss: 0.681034, acc: 0.640625]\n",
      "2628: [discriminator loss: 0.680969, acc: 0.593750] [adversarial loss: 0.937968, acc: 0.078125]\n",
      "2629: [discriminator loss: 0.687540, acc: 0.578125] [adversarial loss: 0.562486, acc: 0.843750]\n",
      "2630: [discriminator loss: 0.698385, acc: 0.539062] [adversarial loss: 0.832258, acc: 0.171875]\n",
      "2631: [discriminator loss: 0.667713, acc: 0.648438] [adversarial loss: 0.662570, acc: 0.593750]\n",
      "2632: [discriminator loss: 0.690521, acc: 0.562500] [adversarial loss: 0.887027, acc: 0.187500]\n",
      "2633: [discriminator loss: 0.701596, acc: 0.593750] [adversarial loss: 0.727212, acc: 0.484375]\n",
      "2634: [discriminator loss: 0.677406, acc: 0.593750] [adversarial loss: 0.876083, acc: 0.187500]\n",
      "2635: [discriminator loss: 0.695345, acc: 0.562500] [adversarial loss: 0.696293, acc: 0.625000]\n",
      "2636: [discriminator loss: 0.736789, acc: 0.476562] [adversarial loss: 0.799037, acc: 0.328125]\n",
      "2637: [discriminator loss: 0.664637, acc: 0.632812] [adversarial loss: 0.767721, acc: 0.343750]\n",
      "2638: [discriminator loss: 0.676487, acc: 0.554688] [adversarial loss: 0.696906, acc: 0.484375]\n",
      "2639: [discriminator loss: 0.697726, acc: 0.476562] [adversarial loss: 0.689372, acc: 0.546875]\n",
      "2640: [discriminator loss: 0.664096, acc: 0.570312] [adversarial loss: 0.751780, acc: 0.359375]\n",
      "2641: [discriminator loss: 0.680637, acc: 0.593750] [adversarial loss: 0.636863, acc: 0.671875]\n",
      "2642: [discriminator loss: 0.721698, acc: 0.500000] [adversarial loss: 0.834848, acc: 0.156250]\n",
      "2643: [discriminator loss: 0.659244, acc: 0.617188] [adversarial loss: 0.728495, acc: 0.453125]\n",
      "2644: [discriminator loss: 0.730147, acc: 0.492188] [adversarial loss: 0.873061, acc: 0.171875]\n",
      "2645: [discriminator loss: 0.665886, acc: 0.585938] [adversarial loss: 0.564365, acc: 0.781250]\n",
      "2646: [discriminator loss: 0.707692, acc: 0.523438] [adversarial loss: 0.852454, acc: 0.218750]\n",
      "2647: [discriminator loss: 0.695400, acc: 0.554688] [adversarial loss: 0.665440, acc: 0.640625]\n",
      "2648: [discriminator loss: 0.696692, acc: 0.601562] [adversarial loss: 0.876261, acc: 0.187500]\n",
      "2649: [discriminator loss: 0.696102, acc: 0.515625] [adversarial loss: 0.730340, acc: 0.375000]\n",
      "2650: [discriminator loss: 0.662964, acc: 0.562500] [adversarial loss: 0.750710, acc: 0.390625]\n",
      "2651: [discriminator loss: 0.673975, acc: 0.570312] [adversarial loss: 0.777947, acc: 0.390625]\n",
      "2652: [discriminator loss: 0.693347, acc: 0.531250] [adversarial loss: 0.776359, acc: 0.296875]\n",
      "2653: [discriminator loss: 0.676246, acc: 0.578125] [adversarial loss: 0.772927, acc: 0.328125]\n",
      "2654: [discriminator loss: 0.678880, acc: 0.601562] [adversarial loss: 0.713785, acc: 0.546875]\n",
      "2655: [discriminator loss: 0.685367, acc: 0.570312] [adversarial loss: 0.864933, acc: 0.234375]\n",
      "2656: [discriminator loss: 0.671420, acc: 0.585938] [adversarial loss: 0.642364, acc: 0.593750]\n",
      "2657: [discriminator loss: 0.693568, acc: 0.570312] [adversarial loss: 0.772558, acc: 0.265625]\n",
      "2658: [discriminator loss: 0.662965, acc: 0.664062] [adversarial loss: 0.763418, acc: 0.406250]\n",
      "2659: [discriminator loss: 0.673930, acc: 0.585938] [adversarial loss: 0.747891, acc: 0.453125]\n",
      "2660: [discriminator loss: 0.670036, acc: 0.570312] [adversarial loss: 0.796386, acc: 0.328125]\n",
      "2661: [discriminator loss: 0.671033, acc: 0.625000] [adversarial loss: 0.874952, acc: 0.203125]\n",
      "2662: [discriminator loss: 0.722083, acc: 0.531250] [adversarial loss: 0.548285, acc: 0.906250]\n",
      "2663: [discriminator loss: 0.710514, acc: 0.500000] [adversarial loss: 1.038729, acc: 0.062500]\n",
      "2664: [discriminator loss: 0.692626, acc: 0.523438] [adversarial loss: 0.648297, acc: 0.656250]\n",
      "2665: [discriminator loss: 0.693112, acc: 0.492188] [adversarial loss: 0.796777, acc: 0.312500]\n",
      "2666: [discriminator loss: 0.656025, acc: 0.601562] [adversarial loss: 0.670572, acc: 0.546875]\n",
      "2667: [discriminator loss: 0.740115, acc: 0.546875] [adversarial loss: 0.747464, acc: 0.406250]\n",
      "2668: [discriminator loss: 0.675508, acc: 0.570312] [adversarial loss: 0.668665, acc: 0.640625]\n",
      "2669: [discriminator loss: 0.694844, acc: 0.500000] [adversarial loss: 0.741321, acc: 0.406250]\n",
      "2670: [discriminator loss: 0.682584, acc: 0.539062] [adversarial loss: 0.781601, acc: 0.234375]\n",
      "2671: [discriminator loss: 0.695463, acc: 0.523438] [adversarial loss: 0.762402, acc: 0.328125]\n",
      "2672: [discriminator loss: 0.684013, acc: 0.578125] [adversarial loss: 0.775407, acc: 0.296875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2673: [discriminator loss: 0.717372, acc: 0.484375] [adversarial loss: 0.841478, acc: 0.265625]\n",
      "2674: [discriminator loss: 0.705680, acc: 0.554688] [adversarial loss: 0.698628, acc: 0.500000]\n",
      "2675: [discriminator loss: 0.690296, acc: 0.500000] [adversarial loss: 0.883322, acc: 0.125000]\n",
      "2676: [discriminator loss: 0.695320, acc: 0.523438] [adversarial loss: 0.621475, acc: 0.781250]\n",
      "2677: [discriminator loss: 0.678086, acc: 0.578125] [adversarial loss: 0.731954, acc: 0.328125]\n",
      "2678: [discriminator loss: 0.670436, acc: 0.578125] [adversarial loss: 0.680001, acc: 0.562500]\n",
      "2679: [discriminator loss: 0.681854, acc: 0.531250] [adversarial loss: 0.822789, acc: 0.187500]\n",
      "2680: [discriminator loss: 0.656519, acc: 0.601562] [adversarial loss: 0.656280, acc: 0.656250]\n",
      "2681: [discriminator loss: 0.661312, acc: 0.601562] [adversarial loss: 0.805539, acc: 0.343750]\n",
      "2682: [discriminator loss: 0.685139, acc: 0.601562] [adversarial loss: 0.712190, acc: 0.406250]\n",
      "2683: [discriminator loss: 0.655044, acc: 0.609375] [adversarial loss: 0.787548, acc: 0.312500]\n",
      "2684: [discriminator loss: 0.690881, acc: 0.554688] [adversarial loss: 0.657349, acc: 0.593750]\n",
      "2685: [discriminator loss: 0.672796, acc: 0.601562] [adversarial loss: 0.894942, acc: 0.187500]\n",
      "2686: [discriminator loss: 0.665774, acc: 0.593750] [adversarial loss: 0.664561, acc: 0.562500]\n",
      "2687: [discriminator loss: 0.687406, acc: 0.593750] [adversarial loss: 0.830914, acc: 0.265625]\n",
      "2688: [discriminator loss: 0.679606, acc: 0.578125] [adversarial loss: 0.763347, acc: 0.453125]\n",
      "2689: [discriminator loss: 0.678108, acc: 0.539062] [adversarial loss: 0.676903, acc: 0.546875]\n",
      "2690: [discriminator loss: 0.651753, acc: 0.625000] [adversarial loss: 0.828458, acc: 0.281250]\n",
      "2691: [discriminator loss: 0.703113, acc: 0.523438] [adversarial loss: 0.589168, acc: 0.781250]\n",
      "2692: [discriminator loss: 0.750335, acc: 0.531250] [adversarial loss: 0.851700, acc: 0.265625]\n",
      "2693: [discriminator loss: 0.698277, acc: 0.539062] [adversarial loss: 0.583552, acc: 0.812500]\n",
      "2694: [discriminator loss: 0.705639, acc: 0.500000] [adversarial loss: 0.854803, acc: 0.218750]\n",
      "2695: [discriminator loss: 0.709706, acc: 0.515625] [adversarial loss: 0.723599, acc: 0.437500]\n",
      "2696: [discriminator loss: 0.658373, acc: 0.648438] [adversarial loss: 0.680476, acc: 0.515625]\n",
      "2697: [discriminator loss: 0.669240, acc: 0.562500] [adversarial loss: 0.648754, acc: 0.625000]\n",
      "2698: [discriminator loss: 0.702264, acc: 0.554688] [adversarial loss: 0.819061, acc: 0.281250]\n",
      "2699: [discriminator loss: 0.689502, acc: 0.500000] [adversarial loss: 0.676952, acc: 0.609375]\n",
      "2700: [discriminator loss: 0.679068, acc: 0.531250] [adversarial loss: 0.925875, acc: 0.281250]\n",
      "2701: [discriminator loss: 0.708459, acc: 0.523438] [adversarial loss: 0.583363, acc: 0.796875]\n",
      "2702: [discriminator loss: 0.728800, acc: 0.507812] [adversarial loss: 0.860062, acc: 0.171875]\n",
      "2703: [discriminator loss: 0.684214, acc: 0.578125] [adversarial loss: 0.689821, acc: 0.546875]\n",
      "2704: [discriminator loss: 0.664738, acc: 0.585938] [adversarial loss: 0.868212, acc: 0.312500]\n",
      "2705: [discriminator loss: 0.703288, acc: 0.539062] [adversarial loss: 0.626771, acc: 0.718750]\n",
      "2706: [discriminator loss: 0.669592, acc: 0.593750] [adversarial loss: 0.806479, acc: 0.265625]\n",
      "2707: [discriminator loss: 0.690549, acc: 0.507812] [adversarial loss: 0.723928, acc: 0.453125]\n",
      "2708: [discriminator loss: 0.672348, acc: 0.617188] [adversarial loss: 0.735124, acc: 0.468750]\n",
      "2709: [discriminator loss: 0.721119, acc: 0.500000] [adversarial loss: 0.744640, acc: 0.359375]\n",
      "2710: [discriminator loss: 0.676712, acc: 0.562500] [adversarial loss: 0.771538, acc: 0.312500]\n",
      "2711: [discriminator loss: 0.663168, acc: 0.570312] [adversarial loss: 0.790050, acc: 0.296875]\n",
      "2712: [discriminator loss: 0.654970, acc: 0.625000] [adversarial loss: 0.811681, acc: 0.234375]\n",
      "2713: [discriminator loss: 0.673674, acc: 0.562500] [adversarial loss: 0.709232, acc: 0.484375]\n",
      "2714: [discriminator loss: 0.682411, acc: 0.578125] [adversarial loss: 0.716898, acc: 0.453125]\n",
      "2715: [discriminator loss: 0.672239, acc: 0.578125] [adversarial loss: 0.678573, acc: 0.593750]\n",
      "2716: [discriminator loss: 0.677216, acc: 0.609375] [adversarial loss: 0.831963, acc: 0.234375]\n",
      "2717: [discriminator loss: 0.678074, acc: 0.601562] [adversarial loss: 0.574696, acc: 0.812500]\n",
      "2718: [discriminator loss: 0.713512, acc: 0.492188] [adversarial loss: 1.021830, acc: 0.093750]\n",
      "2719: [discriminator loss: 0.686743, acc: 0.562500] [adversarial loss: 0.649279, acc: 0.656250]\n",
      "2720: [discriminator loss: 0.728915, acc: 0.460938] [adversarial loss: 0.917376, acc: 0.062500]\n",
      "2721: [discriminator loss: 0.692580, acc: 0.609375] [adversarial loss: 0.721922, acc: 0.468750]\n",
      "2722: [discriminator loss: 0.676976, acc: 0.554688] [adversarial loss: 0.709044, acc: 0.500000]\n",
      "2723: [discriminator loss: 0.684541, acc: 0.546875] [adversarial loss: 0.659041, acc: 0.578125]\n",
      "2724: [discriminator loss: 0.686276, acc: 0.554688] [adversarial loss: 0.803084, acc: 0.281250]\n",
      "2725: [discriminator loss: 0.704991, acc: 0.546875] [adversarial loss: 0.756556, acc: 0.359375]\n",
      "2726: [discriminator loss: 0.693059, acc: 0.554688] [adversarial loss: 0.734381, acc: 0.515625]\n",
      "2727: [discriminator loss: 0.674460, acc: 0.578125] [adversarial loss: 0.740979, acc: 0.500000]\n",
      "2728: [discriminator loss: 0.693965, acc: 0.578125] [adversarial loss: 0.719585, acc: 0.546875]\n",
      "2729: [discriminator loss: 0.666206, acc: 0.609375] [adversarial loss: 0.872245, acc: 0.265625]\n",
      "2730: [discriminator loss: 0.661172, acc: 0.632812] [adversarial loss: 0.707763, acc: 0.484375]\n",
      "2731: [discriminator loss: 0.696283, acc: 0.554688] [adversarial loss: 0.881150, acc: 0.203125]\n",
      "2732: [discriminator loss: 0.719905, acc: 0.453125] [adversarial loss: 0.577357, acc: 0.750000]\n",
      "2733: [discriminator loss: 0.701704, acc: 0.531250] [adversarial loss: 0.922777, acc: 0.125000]\n",
      "2734: [discriminator loss: 0.711048, acc: 0.476562] [adversarial loss: 0.709076, acc: 0.437500]\n",
      "2735: [discriminator loss: 0.680259, acc: 0.585938] [adversarial loss: 0.801299, acc: 0.328125]\n",
      "2736: [discriminator loss: 0.685400, acc: 0.523438] [adversarial loss: 0.763735, acc: 0.281250]\n",
      "2737: [discriminator loss: 0.690030, acc: 0.507812] [adversarial loss: 0.868921, acc: 0.156250]\n",
      "2738: [discriminator loss: 0.682156, acc: 0.531250] [adversarial loss: 0.664660, acc: 0.593750]\n",
      "2739: [discriminator loss: 0.691311, acc: 0.539062] [adversarial loss: 0.892671, acc: 0.109375]\n",
      "2740: [discriminator loss: 0.693433, acc: 0.585938] [adversarial loss: 0.703891, acc: 0.468750]\n",
      "2741: [discriminator loss: 0.702367, acc: 0.460938] [adversarial loss: 0.632122, acc: 0.734375]\n",
      "2742: [discriminator loss: 0.690228, acc: 0.554688] [adversarial loss: 0.839157, acc: 0.187500]\n",
      "2743: [discriminator loss: 0.662753, acc: 0.601562] [adversarial loss: 0.681862, acc: 0.578125]\n",
      "2744: [discriminator loss: 0.685244, acc: 0.531250] [adversarial loss: 0.861950, acc: 0.125000]\n",
      "2745: [discriminator loss: 0.685512, acc: 0.539062] [adversarial loss: 0.632831, acc: 0.609375]\n",
      "2746: [discriminator loss: 0.706293, acc: 0.507812] [adversarial loss: 0.821351, acc: 0.218750]\n",
      "2747: [discriminator loss: 0.681573, acc: 0.546875] [adversarial loss: 0.763367, acc: 0.281250]\n",
      "2748: [discriminator loss: 0.659952, acc: 0.687500] [adversarial loss: 0.725466, acc: 0.500000]\n",
      "2749: [discriminator loss: 0.659939, acc: 0.625000] [adversarial loss: 0.801558, acc: 0.359375]\n",
      "2750: [discriminator loss: 0.706466, acc: 0.507812] [adversarial loss: 0.835077, acc: 0.234375]\n",
      "2751: [discriminator loss: 0.690923, acc: 0.570312] [adversarial loss: 0.588275, acc: 0.750000]\n",
      "2752: [discriminator loss: 0.678743, acc: 0.570312] [adversarial loss: 0.863942, acc: 0.156250]\n",
      "2753: [discriminator loss: 0.697568, acc: 0.523438] [adversarial loss: 0.617025, acc: 0.671875]\n",
      "2754: [discriminator loss: 0.691751, acc: 0.570312] [adversarial loss: 0.766489, acc: 0.265625]\n",
      "2755: [discriminator loss: 0.681857, acc: 0.585938] [adversarial loss: 0.707863, acc: 0.500000]\n",
      "2756: [discriminator loss: 0.695938, acc: 0.531250] [adversarial loss: 0.746077, acc: 0.406250]\n",
      "2757: [discriminator loss: 0.672473, acc: 0.593750] [adversarial loss: 0.668287, acc: 0.671875]\n",
      "2758: [discriminator loss: 0.721424, acc: 0.468750] [adversarial loss: 0.981026, acc: 0.062500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2759: [discriminator loss: 0.679622, acc: 0.539062] [adversarial loss: 0.675423, acc: 0.546875]\n",
      "2760: [discriminator loss: 0.685261, acc: 0.578125] [adversarial loss: 0.783725, acc: 0.281250]\n",
      "2761: [discriminator loss: 0.699403, acc: 0.554688] [adversarial loss: 0.587331, acc: 0.781250]\n",
      "2762: [discriminator loss: 0.678586, acc: 0.562500] [adversarial loss: 0.930715, acc: 0.093750]\n",
      "2763: [discriminator loss: 0.690471, acc: 0.546875] [adversarial loss: 0.581863, acc: 0.781250]\n",
      "2764: [discriminator loss: 0.711832, acc: 0.523438] [adversarial loss: 0.859820, acc: 0.281250]\n",
      "2765: [discriminator loss: 0.679285, acc: 0.570312] [adversarial loss: 0.712560, acc: 0.437500]\n",
      "2766: [discriminator loss: 0.711719, acc: 0.468750] [adversarial loss: 0.735172, acc: 0.359375]\n",
      "2767: [discriminator loss: 0.679692, acc: 0.554688] [adversarial loss: 0.705036, acc: 0.484375]\n",
      "2768: [discriminator loss: 0.683336, acc: 0.578125] [adversarial loss: 0.744510, acc: 0.437500]\n",
      "2769: [discriminator loss: 0.685243, acc: 0.531250] [adversarial loss: 0.774415, acc: 0.375000]\n",
      "2770: [discriminator loss: 0.700758, acc: 0.500000] [adversarial loss: 0.750296, acc: 0.343750]\n",
      "2771: [discriminator loss: 0.659026, acc: 0.585938] [adversarial loss: 0.765120, acc: 0.390625]\n",
      "2772: [discriminator loss: 0.687975, acc: 0.546875] [adversarial loss: 0.661552, acc: 0.562500]\n",
      "2773: [discriminator loss: 0.679363, acc: 0.601562] [adversarial loss: 0.888129, acc: 0.218750]\n",
      "2774: [discriminator loss: 0.695007, acc: 0.523438] [adversarial loss: 0.649986, acc: 0.578125]\n",
      "2775: [discriminator loss: 0.674912, acc: 0.539062] [adversarial loss: 0.746487, acc: 0.437500]\n",
      "2776: [discriminator loss: 0.660376, acc: 0.632812] [adversarial loss: 0.782834, acc: 0.500000]\n",
      "2777: [discriminator loss: 0.693111, acc: 0.562500] [adversarial loss: 0.765592, acc: 0.453125]\n",
      "2778: [discriminator loss: 0.651375, acc: 0.617188] [adversarial loss: 0.809657, acc: 0.375000]\n",
      "2779: [discriminator loss: 0.685693, acc: 0.554688] [adversarial loss: 0.694301, acc: 0.390625]\n",
      "2780: [discriminator loss: 0.675088, acc: 0.625000] [adversarial loss: 0.835032, acc: 0.281250]\n",
      "2781: [discriminator loss: 0.704718, acc: 0.562500] [adversarial loss: 0.606433, acc: 0.703125]\n",
      "2782: [discriminator loss: 0.680750, acc: 0.531250] [adversarial loss: 0.823956, acc: 0.281250]\n",
      "2783: [discriminator loss: 0.668185, acc: 0.656250] [adversarial loss: 0.612685, acc: 0.750000]\n",
      "2784: [discriminator loss: 0.680463, acc: 0.546875] [adversarial loss: 0.796655, acc: 0.250000]\n",
      "2785: [discriminator loss: 0.689556, acc: 0.546875] [adversarial loss: 0.598488, acc: 0.734375]\n",
      "2786: [discriminator loss: 0.715709, acc: 0.523438] [adversarial loss: 0.813885, acc: 0.171875]\n",
      "2787: [discriminator loss: 0.699200, acc: 0.546875] [adversarial loss: 0.643200, acc: 0.625000]\n",
      "2788: [discriminator loss: 0.717531, acc: 0.554688] [adversarial loss: 0.895366, acc: 0.140625]\n",
      "2789: [discriminator loss: 0.672662, acc: 0.531250] [adversarial loss: 0.864702, acc: 0.093750]\n",
      "2790: [discriminator loss: 0.681399, acc: 0.601562] [adversarial loss: 0.711464, acc: 0.515625]\n",
      "2791: [discriminator loss: 0.655337, acc: 0.578125] [adversarial loss: 0.879481, acc: 0.250000]\n",
      "2792: [discriminator loss: 0.680934, acc: 0.570312] [adversarial loss: 0.649772, acc: 0.625000]\n",
      "2793: [discriminator loss: 0.707976, acc: 0.515625] [adversarial loss: 0.781761, acc: 0.281250]\n",
      "2794: [discriminator loss: 0.691092, acc: 0.484375] [adversarial loss: 0.729621, acc: 0.468750]\n",
      "2795: [discriminator loss: 0.691976, acc: 0.546875] [adversarial loss: 0.880643, acc: 0.171875]\n",
      "2796: [discriminator loss: 0.673629, acc: 0.585938] [adversarial loss: 0.709067, acc: 0.500000]\n",
      "2797: [discriminator loss: 0.688161, acc: 0.515625] [adversarial loss: 0.843668, acc: 0.218750]\n",
      "2798: [discriminator loss: 0.678024, acc: 0.593750] [adversarial loss: 0.717755, acc: 0.515625]\n",
      "2799: [discriminator loss: 0.682680, acc: 0.546875] [adversarial loss: 0.767198, acc: 0.421875]\n",
      "2800: [discriminator loss: 0.675774, acc: 0.593750] [adversarial loss: 0.688123, acc: 0.468750]\n",
      "2801: [discriminator loss: 0.688965, acc: 0.476562] [adversarial loss: 0.698621, acc: 0.468750]\n",
      "2802: [discriminator loss: 0.666078, acc: 0.570312] [adversarial loss: 0.837371, acc: 0.265625]\n",
      "2803: [discriminator loss: 0.686407, acc: 0.531250] [adversarial loss: 0.725241, acc: 0.515625]\n",
      "2804: [discriminator loss: 0.684294, acc: 0.531250] [adversarial loss: 0.783729, acc: 0.296875]\n",
      "2805: [discriminator loss: 0.684924, acc: 0.539062] [adversarial loss: 0.682685, acc: 0.593750]\n",
      "2806: [discriminator loss: 0.687207, acc: 0.515625] [adversarial loss: 0.836153, acc: 0.062500]\n",
      "2807: [discriminator loss: 0.681575, acc: 0.562500] [adversarial loss: 0.586242, acc: 0.812500]\n",
      "2808: [discriminator loss: 0.700681, acc: 0.546875] [adversarial loss: 0.862058, acc: 0.093750]\n",
      "2809: [discriminator loss: 0.644302, acc: 0.664062] [adversarial loss: 0.744182, acc: 0.500000]\n",
      "2810: [discriminator loss: 0.677881, acc: 0.578125] [adversarial loss: 0.820485, acc: 0.343750]\n",
      "2811: [discriminator loss: 0.717948, acc: 0.476562] [adversarial loss: 0.690694, acc: 0.593750]\n",
      "2812: [discriminator loss: 0.663711, acc: 0.625000] [adversarial loss: 0.801128, acc: 0.343750]\n",
      "2813: [discriminator loss: 0.663095, acc: 0.671875] [adversarial loss: 0.674001, acc: 0.546875]\n",
      "2814: [discriminator loss: 0.756435, acc: 0.484375] [adversarial loss: 0.808519, acc: 0.296875]\n",
      "2815: [discriminator loss: 0.686988, acc: 0.546875] [adversarial loss: 0.769989, acc: 0.343750]\n",
      "2816: [discriminator loss: 0.658859, acc: 0.593750] [adversarial loss: 0.897328, acc: 0.203125]\n",
      "2817: [discriminator loss: 0.706933, acc: 0.531250] [adversarial loss: 0.753603, acc: 0.390625]\n",
      "2818: [discriminator loss: 0.690343, acc: 0.515625] [adversarial loss: 0.744584, acc: 0.312500]\n",
      "2819: [discriminator loss: 0.695640, acc: 0.546875] [adversarial loss: 0.751764, acc: 0.296875]\n",
      "2820: [discriminator loss: 0.663757, acc: 0.625000] [adversarial loss: 0.714769, acc: 0.484375]\n",
      "2821: [discriminator loss: 0.708960, acc: 0.515625] [adversarial loss: 0.776729, acc: 0.265625]\n",
      "2822: [discriminator loss: 0.688800, acc: 0.585938] [adversarial loss: 0.605367, acc: 0.875000]\n",
      "2823: [discriminator loss: 0.677594, acc: 0.570312] [adversarial loss: 0.836208, acc: 0.281250]\n",
      "2824: [discriminator loss: 0.662434, acc: 0.617188] [adversarial loss: 0.713980, acc: 0.546875]\n",
      "2825: [discriminator loss: 0.702236, acc: 0.484375] [adversarial loss: 0.884599, acc: 0.093750]\n",
      "2826: [discriminator loss: 0.687559, acc: 0.546875] [adversarial loss: 0.605724, acc: 0.703125]\n",
      "2827: [discriminator loss: 0.687861, acc: 0.562500] [adversarial loss: 0.903824, acc: 0.203125]\n",
      "2828: [discriminator loss: 0.702609, acc: 0.539062] [adversarial loss: 0.686101, acc: 0.546875]\n",
      "2829: [discriminator loss: 0.706666, acc: 0.531250] [adversarial loss: 0.837105, acc: 0.156250]\n",
      "2830: [discriminator loss: 0.683766, acc: 0.500000] [adversarial loss: 0.713781, acc: 0.390625]\n",
      "2831: [discriminator loss: 0.686179, acc: 0.546875] [adversarial loss: 0.742525, acc: 0.421875]\n",
      "2832: [discriminator loss: 0.696046, acc: 0.507812] [adversarial loss: 0.653315, acc: 0.671875]\n",
      "2833: [discriminator loss: 0.667650, acc: 0.585938] [adversarial loss: 0.780640, acc: 0.281250]\n",
      "2834: [discriminator loss: 0.680626, acc: 0.570312] [adversarial loss: 0.683394, acc: 0.609375]\n",
      "2835: [discriminator loss: 0.667189, acc: 0.562500] [adversarial loss: 0.853113, acc: 0.296875]\n",
      "2836: [discriminator loss: 0.695769, acc: 0.500000] [adversarial loss: 0.764108, acc: 0.328125]\n",
      "2837: [discriminator loss: 0.691490, acc: 0.507812] [adversarial loss: 0.742516, acc: 0.406250]\n",
      "2838: [discriminator loss: 0.687968, acc: 0.492188] [adversarial loss: 0.787870, acc: 0.203125]\n",
      "2839: [discriminator loss: 0.670543, acc: 0.531250] [adversarial loss: 0.709450, acc: 0.484375]\n",
      "2840: [discriminator loss: 0.692442, acc: 0.507812] [adversarial loss: 0.865585, acc: 0.281250]\n",
      "2841: [discriminator loss: 0.693656, acc: 0.554688] [adversarial loss: 0.696694, acc: 0.515625]\n",
      "2842: [discriminator loss: 0.691193, acc: 0.546875] [adversarial loss: 0.679609, acc: 0.531250]\n",
      "2843: [discriminator loss: 0.700766, acc: 0.484375] [adversarial loss: 0.715116, acc: 0.500000]\n",
      "2844: [discriminator loss: 0.671255, acc: 0.593750] [adversarial loss: 0.909687, acc: 0.187500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2845: [discriminator loss: 0.684892, acc: 0.601562] [adversarial loss: 0.703150, acc: 0.484375]\n",
      "2846: [discriminator loss: 0.706108, acc: 0.500000] [adversarial loss: 0.865045, acc: 0.171875]\n",
      "2847: [discriminator loss: 0.706127, acc: 0.515625] [adversarial loss: 0.580231, acc: 0.765625]\n",
      "2848: [discriminator loss: 0.690364, acc: 0.546875] [adversarial loss: 0.788763, acc: 0.265625]\n",
      "2849: [discriminator loss: 0.675454, acc: 0.585938] [adversarial loss: 0.733051, acc: 0.406250]\n",
      "2850: [discriminator loss: 0.679112, acc: 0.531250] [adversarial loss: 0.732332, acc: 0.437500]\n",
      "2851: [discriminator loss: 0.688199, acc: 0.554688] [adversarial loss: 0.847305, acc: 0.203125]\n",
      "2852: [discriminator loss: 0.681883, acc: 0.546875] [adversarial loss: 0.695818, acc: 0.453125]\n",
      "2853: [discriminator loss: 0.683652, acc: 0.585938] [adversarial loss: 1.016002, acc: 0.093750]\n",
      "2854: [discriminator loss: 0.727171, acc: 0.523438] [adversarial loss: 0.578232, acc: 0.796875]\n",
      "2855: [discriminator loss: 0.710078, acc: 0.531250] [adversarial loss: 0.847201, acc: 0.156250]\n",
      "2856: [discriminator loss: 0.687736, acc: 0.523438] [adversarial loss: 0.665138, acc: 0.531250]\n",
      "2857: [discriminator loss: 0.678510, acc: 0.609375] [adversarial loss: 0.779056, acc: 0.312500]\n",
      "2858: [discriminator loss: 0.693460, acc: 0.515625] [adversarial loss: 0.662747, acc: 0.593750]\n",
      "2859: [discriminator loss: 0.679782, acc: 0.578125] [adversarial loss: 0.689620, acc: 0.562500]\n",
      "2860: [discriminator loss: 0.670916, acc: 0.617188] [adversarial loss: 0.839392, acc: 0.250000]\n",
      "2861: [discriminator loss: 0.679026, acc: 0.593750] [adversarial loss: 0.709652, acc: 0.484375]\n",
      "2862: [discriminator loss: 0.698504, acc: 0.523438] [adversarial loss: 0.843051, acc: 0.187500]\n",
      "2863: [discriminator loss: 0.683563, acc: 0.562500] [adversarial loss: 0.578308, acc: 0.859375]\n",
      "2864: [discriminator loss: 0.669168, acc: 0.578125] [adversarial loss: 0.873387, acc: 0.171875]\n",
      "2865: [discriminator loss: 0.708131, acc: 0.437500] [adversarial loss: 0.685653, acc: 0.578125]\n",
      "2866: [discriminator loss: 0.688355, acc: 0.546875] [adversarial loss: 0.804263, acc: 0.375000]\n",
      "2867: [discriminator loss: 0.665452, acc: 0.601562] [adversarial loss: 0.797962, acc: 0.421875]\n",
      "2868: [discriminator loss: 0.690571, acc: 0.539062] [adversarial loss: 0.769344, acc: 0.359375]\n",
      "2869: [discriminator loss: 0.676637, acc: 0.593750] [adversarial loss: 0.781472, acc: 0.312500]\n",
      "2870: [discriminator loss: 0.679638, acc: 0.585938] [adversarial loss: 0.620850, acc: 0.671875]\n",
      "2871: [discriminator loss: 0.650557, acc: 0.593750] [adversarial loss: 0.768291, acc: 0.343750]\n",
      "2872: [discriminator loss: 0.713388, acc: 0.468750] [adversarial loss: 0.661217, acc: 0.562500]\n",
      "2873: [discriminator loss: 0.692296, acc: 0.500000] [adversarial loss: 0.766579, acc: 0.343750]\n",
      "2874: [discriminator loss: 0.659133, acc: 0.625000] [adversarial loss: 0.621957, acc: 0.609375]\n",
      "2875: [discriminator loss: 0.701373, acc: 0.523438] [adversarial loss: 0.872383, acc: 0.156250]\n",
      "2876: [discriminator loss: 0.680370, acc: 0.585938] [adversarial loss: 0.668750, acc: 0.531250]\n",
      "2877: [discriminator loss: 0.683863, acc: 0.539062] [adversarial loss: 0.857448, acc: 0.234375]\n",
      "2878: [discriminator loss: 0.677921, acc: 0.578125] [adversarial loss: 0.652050, acc: 0.671875]\n",
      "2879: [discriminator loss: 0.690543, acc: 0.531250] [adversarial loss: 0.883173, acc: 0.156250]\n",
      "2880: [discriminator loss: 0.678988, acc: 0.593750] [adversarial loss: 0.713787, acc: 0.484375]\n",
      "2881: [discriminator loss: 0.740218, acc: 0.515625] [adversarial loss: 0.911091, acc: 0.031250]\n",
      "2882: [discriminator loss: 0.680903, acc: 0.585938] [adversarial loss: 0.702466, acc: 0.453125]\n",
      "2883: [discriminator loss: 0.673867, acc: 0.570312] [adversarial loss: 0.789247, acc: 0.375000]\n",
      "2884: [discriminator loss: 0.683742, acc: 0.562500] [adversarial loss: 0.620190, acc: 0.796875]\n",
      "2885: [discriminator loss: 0.687220, acc: 0.546875] [adversarial loss: 0.807367, acc: 0.296875]\n",
      "2886: [discriminator loss: 0.687949, acc: 0.523438] [adversarial loss: 0.666062, acc: 0.671875]\n",
      "2887: [discriminator loss: 0.703855, acc: 0.515625] [adversarial loss: 0.854766, acc: 0.187500]\n",
      "2888: [discriminator loss: 0.681282, acc: 0.585938] [adversarial loss: 0.681344, acc: 0.546875]\n",
      "2889: [discriminator loss: 0.704054, acc: 0.523438] [adversarial loss: 0.830624, acc: 0.156250]\n",
      "2890: [discriminator loss: 0.699404, acc: 0.468750] [adversarial loss: 0.668173, acc: 0.593750]\n",
      "2891: [discriminator loss: 0.692917, acc: 0.453125] [adversarial loss: 0.761676, acc: 0.265625]\n",
      "2892: [discriminator loss: 0.678606, acc: 0.593750] [adversarial loss: 0.659989, acc: 0.578125]\n",
      "2893: [discriminator loss: 0.691407, acc: 0.562500] [adversarial loss: 0.732598, acc: 0.390625]\n",
      "2894: [discriminator loss: 0.663408, acc: 0.585938] [adversarial loss: 0.797140, acc: 0.296875]\n",
      "2895: [discriminator loss: 0.684875, acc: 0.515625] [adversarial loss: 0.742395, acc: 0.296875]\n",
      "2896: [discriminator loss: 0.706639, acc: 0.468750] [adversarial loss: 0.799099, acc: 0.312500]\n",
      "2897: [discriminator loss: 0.695065, acc: 0.554688] [adversarial loss: 0.697190, acc: 0.515625]\n",
      "2898: [discriminator loss: 0.681128, acc: 0.554688] [adversarial loss: 0.772185, acc: 0.328125]\n",
      "2899: [discriminator loss: 0.684835, acc: 0.539062] [adversarial loss: 0.808805, acc: 0.328125]\n",
      "2900: [discriminator loss: 0.670558, acc: 0.640625] [adversarial loss: 0.723899, acc: 0.500000]\n",
      "2901: [discriminator loss: 0.689476, acc: 0.578125] [adversarial loss: 0.716366, acc: 0.453125]\n",
      "2902: [discriminator loss: 0.658908, acc: 0.609375] [adversarial loss: 0.683820, acc: 0.546875]\n",
      "2903: [discriminator loss: 0.675074, acc: 0.539062] [adversarial loss: 0.646398, acc: 0.656250]\n",
      "2904: [discriminator loss: 0.669021, acc: 0.593750] [adversarial loss: 0.788853, acc: 0.265625]\n",
      "2905: [discriminator loss: 0.669151, acc: 0.593750] [adversarial loss: 0.744007, acc: 0.296875]\n",
      "2906: [discriminator loss: 0.697429, acc: 0.531250] [adversarial loss: 0.815430, acc: 0.328125]\n",
      "2907: [discriminator loss: 0.703789, acc: 0.492188] [adversarial loss: 0.713515, acc: 0.531250]\n",
      "2908: [discriminator loss: 0.665036, acc: 0.593750] [adversarial loss: 0.981516, acc: 0.125000]\n",
      "2909: [discriminator loss: 0.702718, acc: 0.539062] [adversarial loss: 0.536999, acc: 0.906250]\n",
      "2910: [discriminator loss: 0.705829, acc: 0.484375] [adversarial loss: 0.886085, acc: 0.093750]\n",
      "2911: [discriminator loss: 0.706027, acc: 0.562500] [adversarial loss: 0.605007, acc: 0.796875]\n",
      "2912: [discriminator loss: 0.682699, acc: 0.500000] [adversarial loss: 0.731434, acc: 0.421875]\n",
      "2913: [discriminator loss: 0.666120, acc: 0.617188] [adversarial loss: 0.727850, acc: 0.437500]\n",
      "2914: [discriminator loss: 0.696240, acc: 0.585938] [adversarial loss: 0.832332, acc: 0.140625]\n",
      "2915: [discriminator loss: 0.682337, acc: 0.554688] [adversarial loss: 0.655560, acc: 0.625000]\n",
      "2916: [discriminator loss: 0.679227, acc: 0.578125] [adversarial loss: 0.863864, acc: 0.062500]\n",
      "2917: [discriminator loss: 0.687452, acc: 0.593750] [adversarial loss: 0.659819, acc: 0.640625]\n",
      "2918: [discriminator loss: 0.686022, acc: 0.507812] [adversarial loss: 0.638738, acc: 0.640625]\n",
      "2919: [discriminator loss: 0.683110, acc: 0.539062] [adversarial loss: 0.825649, acc: 0.250000]\n",
      "2920: [discriminator loss: 0.699944, acc: 0.507812] [adversarial loss: 0.714370, acc: 0.484375]\n",
      "2921: [discriminator loss: 0.712224, acc: 0.484375] [adversarial loss: 0.700534, acc: 0.453125]\n",
      "2922: [discriminator loss: 0.679728, acc: 0.609375] [adversarial loss: 0.913692, acc: 0.093750]\n",
      "2923: [discriminator loss: 0.695322, acc: 0.539062] [adversarial loss: 0.596214, acc: 0.859375]\n",
      "2924: [discriminator loss: 0.716304, acc: 0.515625] [adversarial loss: 0.875138, acc: 0.078125]\n",
      "2925: [discriminator loss: 0.697411, acc: 0.492188] [adversarial loss: 0.757682, acc: 0.421875]\n",
      "2926: [discriminator loss: 0.672177, acc: 0.570312] [adversarial loss: 0.809807, acc: 0.296875]\n",
      "2927: [discriminator loss: 0.695143, acc: 0.531250] [adversarial loss: 0.727999, acc: 0.421875]\n",
      "2928: [discriminator loss: 0.681863, acc: 0.570312] [adversarial loss: 0.820273, acc: 0.156250]\n",
      "2929: [discriminator loss: 0.683403, acc: 0.625000] [adversarial loss: 0.702670, acc: 0.484375]\n",
      "2930: [discriminator loss: 0.665788, acc: 0.609375] [adversarial loss: 0.844314, acc: 0.250000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2931: [discriminator loss: 0.694689, acc: 0.562500] [adversarial loss: 0.617082, acc: 0.734375]\n",
      "2932: [discriminator loss: 0.675779, acc: 0.578125] [adversarial loss: 0.841726, acc: 0.203125]\n",
      "2933: [discriminator loss: 0.680237, acc: 0.554688] [adversarial loss: 0.674102, acc: 0.625000]\n",
      "2934: [discriminator loss: 0.689422, acc: 0.531250] [adversarial loss: 0.782452, acc: 0.281250]\n",
      "2935: [discriminator loss: 0.675724, acc: 0.546875] [adversarial loss: 0.692385, acc: 0.468750]\n",
      "2936: [discriminator loss: 0.685419, acc: 0.531250] [adversarial loss: 0.747465, acc: 0.390625]\n",
      "2937: [discriminator loss: 0.699127, acc: 0.476562] [adversarial loss: 0.659274, acc: 0.656250]\n",
      "2938: [discriminator loss: 0.677712, acc: 0.578125] [adversarial loss: 0.778265, acc: 0.359375]\n",
      "2939: [discriminator loss: 0.665916, acc: 0.648438] [adversarial loss: 0.682836, acc: 0.468750]\n",
      "2940: [discriminator loss: 0.681011, acc: 0.570312] [adversarial loss: 0.809821, acc: 0.296875]\n",
      "2941: [discriminator loss: 0.685682, acc: 0.562500] [adversarial loss: 0.730841, acc: 0.328125]\n",
      "2942: [discriminator loss: 0.672975, acc: 0.523438] [adversarial loss: 0.697553, acc: 0.515625]\n",
      "2943: [discriminator loss: 0.698684, acc: 0.515625] [adversarial loss: 0.786577, acc: 0.250000]\n",
      "2944: [discriminator loss: 0.668172, acc: 0.554688] [adversarial loss: 0.682469, acc: 0.468750]\n",
      "2945: [discriminator loss: 0.692386, acc: 0.546875] [adversarial loss: 0.881337, acc: 0.140625]\n",
      "2946: [discriminator loss: 0.706824, acc: 0.531250] [adversarial loss: 0.655870, acc: 0.656250]\n",
      "2947: [discriminator loss: 0.662918, acc: 0.593750] [adversarial loss: 0.739789, acc: 0.296875]\n",
      "2948: [discriminator loss: 0.678368, acc: 0.609375] [adversarial loss: 0.645941, acc: 0.609375]\n",
      "2949: [discriminator loss: 0.652076, acc: 0.601562] [adversarial loss: 0.830277, acc: 0.171875]\n",
      "2950: [discriminator loss: 0.685291, acc: 0.554688] [adversarial loss: 0.781480, acc: 0.437500]\n",
      "2951: [discriminator loss: 0.674595, acc: 0.570312] [adversarial loss: 0.912213, acc: 0.203125]\n",
      "2952: [discriminator loss: 0.715315, acc: 0.539062] [adversarial loss: 0.606701, acc: 0.718750]\n",
      "2953: [discriminator loss: 0.683291, acc: 0.546875] [adversarial loss: 0.922867, acc: 0.218750]\n",
      "2954: [discriminator loss: 0.699451, acc: 0.539062] [adversarial loss: 0.569655, acc: 0.859375]\n",
      "2955: [discriminator loss: 0.696518, acc: 0.523438] [adversarial loss: 0.911361, acc: 0.078125]\n",
      "2956: [discriminator loss: 0.696453, acc: 0.539062] [adversarial loss: 0.631293, acc: 0.765625]\n",
      "2957: [discriminator loss: 0.667515, acc: 0.601562] [adversarial loss: 0.778364, acc: 0.406250]\n",
      "2958: [discriminator loss: 0.642892, acc: 0.625000] [adversarial loss: 0.654746, acc: 0.703125]\n",
      "2959: [discriminator loss: 0.709454, acc: 0.546875] [adversarial loss: 0.872350, acc: 0.265625]\n",
      "2960: [discriminator loss: 0.712255, acc: 0.515625] [adversarial loss: 0.684647, acc: 0.515625]\n",
      "2961: [discriminator loss: 0.706530, acc: 0.523438] [adversarial loss: 0.797124, acc: 0.281250]\n",
      "2962: [discriminator loss: 0.667514, acc: 0.640625] [adversarial loss: 0.698785, acc: 0.500000]\n",
      "2963: [discriminator loss: 0.689374, acc: 0.507812] [adversarial loss: 0.763336, acc: 0.343750]\n",
      "2964: [discriminator loss: 0.685381, acc: 0.554688] [adversarial loss: 0.839281, acc: 0.203125]\n",
      "2965: [discriminator loss: 0.691223, acc: 0.570312] [adversarial loss: 0.699360, acc: 0.484375]\n",
      "2966: [discriminator loss: 0.658398, acc: 0.601562] [adversarial loss: 0.818439, acc: 0.265625]\n",
      "2967: [discriminator loss: 0.712819, acc: 0.546875] [adversarial loss: 0.668027, acc: 0.578125]\n",
      "2968: [discriminator loss: 0.702569, acc: 0.500000] [adversarial loss: 0.855045, acc: 0.171875]\n",
      "2969: [discriminator loss: 0.650614, acc: 0.656250] [adversarial loss: 0.718023, acc: 0.484375]\n",
      "2970: [discriminator loss: 0.698376, acc: 0.585938] [adversarial loss: 0.734193, acc: 0.421875]\n",
      "2971: [discriminator loss: 0.690113, acc: 0.531250] [adversarial loss: 0.801190, acc: 0.328125]\n",
      "2972: [discriminator loss: 0.733143, acc: 0.500000] [adversarial loss: 0.640952, acc: 0.687500]\n",
      "2973: [discriminator loss: 0.691830, acc: 0.585938] [adversarial loss: 0.828436, acc: 0.250000]\n",
      "2974: [discriminator loss: 0.687806, acc: 0.515625] [adversarial loss: 0.673646, acc: 0.578125]\n",
      "2975: [discriminator loss: 0.676051, acc: 0.562500] [adversarial loss: 0.761769, acc: 0.453125]\n",
      "2976: [discriminator loss: 0.678712, acc: 0.585938] [adversarial loss: 0.723301, acc: 0.406250]\n",
      "2977: [discriminator loss: 0.694421, acc: 0.531250] [adversarial loss: 0.733676, acc: 0.453125]\n",
      "2978: [discriminator loss: 0.679981, acc: 0.570312] [adversarial loss: 0.675946, acc: 0.625000]\n",
      "2979: [discriminator loss: 0.673202, acc: 0.617188] [adversarial loss: 0.749903, acc: 0.390625]\n",
      "2980: [discriminator loss: 0.677305, acc: 0.585938] [adversarial loss: 0.742667, acc: 0.437500]\n",
      "2981: [discriminator loss: 0.676019, acc: 0.601562] [adversarial loss: 0.787617, acc: 0.312500]\n",
      "2982: [discriminator loss: 0.691719, acc: 0.539062] [adversarial loss: 0.687984, acc: 0.546875]\n",
      "2983: [discriminator loss: 0.714761, acc: 0.531250] [adversarial loss: 0.788718, acc: 0.250000]\n",
      "2984: [discriminator loss: 0.675836, acc: 0.593750] [adversarial loss: 0.653261, acc: 0.640625]\n",
      "2985: [discriminator loss: 0.682958, acc: 0.539062] [adversarial loss: 0.827094, acc: 0.250000]\n",
      "2986: [discriminator loss: 0.680349, acc: 0.578125] [adversarial loss: 0.698519, acc: 0.468750]\n",
      "2987: [discriminator loss: 0.689314, acc: 0.578125] [adversarial loss: 0.842291, acc: 0.203125]\n",
      "2988: [discriminator loss: 0.682120, acc: 0.554688] [adversarial loss: 0.602084, acc: 0.687500]\n",
      "2989: [discriminator loss: 0.706513, acc: 0.515625] [adversarial loss: 0.810046, acc: 0.250000]\n",
      "2990: [discriminator loss: 0.680018, acc: 0.531250] [adversarial loss: 0.619133, acc: 0.734375]\n",
      "2991: [discriminator loss: 0.660542, acc: 0.593750] [adversarial loss: 0.837436, acc: 0.375000]\n",
      "2992: [discriminator loss: 0.690873, acc: 0.585938] [adversarial loss: 0.636670, acc: 0.656250]\n",
      "2993: [discriminator loss: 0.701069, acc: 0.570312] [adversarial loss: 0.825583, acc: 0.125000]\n",
      "2994: [discriminator loss: 0.684281, acc: 0.546875] [adversarial loss: 0.595165, acc: 0.765625]\n",
      "2995: [discriminator loss: 0.707093, acc: 0.546875] [adversarial loss: 0.794906, acc: 0.203125]\n",
      "2996: [discriminator loss: 0.676621, acc: 0.539062] [adversarial loss: 0.701697, acc: 0.546875]\n",
      "2997: [discriminator loss: 0.702452, acc: 0.515625] [adversarial loss: 0.637808, acc: 0.718750]\n",
      "2998: [discriminator loss: 0.707300, acc: 0.507812] [adversarial loss: 0.829976, acc: 0.125000]\n",
      "2999: [discriminator loss: 0.679050, acc: 0.578125] [adversarial loss: 0.641328, acc: 0.703125]\n",
      "cgan_mnist  labels for generated images:  [0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5]\n",
      "3000: [discriminator loss: 0.694376, acc: 0.546875] [adversarial loss: 0.796546, acc: 0.281250]\n",
      "3001: [discriminator loss: 0.691232, acc: 0.484375] [adversarial loss: 0.649598, acc: 0.671875]\n",
      "3002: [discriminator loss: 0.692697, acc: 0.500000] [adversarial loss: 0.886658, acc: 0.265625]\n",
      "3003: [discriminator loss: 0.639019, acc: 0.640625] [adversarial loss: 0.703310, acc: 0.500000]\n",
      "3004: [discriminator loss: 0.736450, acc: 0.484375] [adversarial loss: 0.727870, acc: 0.484375]\n",
      "3005: [discriminator loss: 0.671027, acc: 0.585938] [adversarial loss: 0.760240, acc: 0.390625]\n",
      "3006: [discriminator loss: 0.679267, acc: 0.554688] [adversarial loss: 0.809114, acc: 0.296875]\n",
      "3007: [discriminator loss: 0.696681, acc: 0.539062] [adversarial loss: 0.701382, acc: 0.531250]\n",
      "3008: [discriminator loss: 0.701502, acc: 0.507812] [adversarial loss: 0.801588, acc: 0.218750]\n",
      "3009: [discriminator loss: 0.701359, acc: 0.539062] [adversarial loss: 0.655177, acc: 0.687500]\n",
      "3010: [discriminator loss: 0.699455, acc: 0.484375] [adversarial loss: 0.815515, acc: 0.203125]\n",
      "3011: [discriminator loss: 0.685553, acc: 0.539062] [adversarial loss: 0.725291, acc: 0.406250]\n",
      "3012: [discriminator loss: 0.669855, acc: 0.593750] [adversarial loss: 0.683360, acc: 0.531250]\n",
      "3013: [discriminator loss: 0.673185, acc: 0.578125] [adversarial loss: 0.728264, acc: 0.375000]\n",
      "3014: [discriminator loss: 0.680367, acc: 0.585938] [adversarial loss: 0.674457, acc: 0.500000]\n",
      "3015: [discriminator loss: 0.687449, acc: 0.593750] [adversarial loss: 0.779297, acc: 0.296875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3016: [discriminator loss: 0.673329, acc: 0.570312] [adversarial loss: 0.689501, acc: 0.562500]\n",
      "3017: [discriminator loss: 0.659431, acc: 0.593750] [adversarial loss: 0.951599, acc: 0.250000]\n",
      "3018: [discriminator loss: 0.701522, acc: 0.554688] [adversarial loss: 0.589624, acc: 0.734375]\n",
      "3019: [discriminator loss: 0.678681, acc: 0.570312] [adversarial loss: 0.890952, acc: 0.218750]\n",
      "3020: [discriminator loss: 0.706809, acc: 0.515625] [adversarial loss: 0.617430, acc: 0.796875]\n",
      "3021: [discriminator loss: 0.706708, acc: 0.523438] [adversarial loss: 0.865265, acc: 0.156250]\n",
      "3022: [discriminator loss: 0.671622, acc: 0.609375] [adversarial loss: 0.706119, acc: 0.531250]\n",
      "3023: [discriminator loss: 0.678372, acc: 0.554688] [adversarial loss: 0.732263, acc: 0.406250]\n",
      "3024: [discriminator loss: 0.699874, acc: 0.515625] [adversarial loss: 0.676052, acc: 0.593750]\n",
      "3025: [discriminator loss: 0.676021, acc: 0.570312] [adversarial loss: 0.811588, acc: 0.187500]\n",
      "3026: [discriminator loss: 0.673787, acc: 0.562500] [adversarial loss: 0.760822, acc: 0.375000]\n",
      "3027: [discriminator loss: 0.669332, acc: 0.609375] [adversarial loss: 0.730978, acc: 0.421875]\n",
      "3028: [discriminator loss: 0.703129, acc: 0.500000] [adversarial loss: 0.786870, acc: 0.359375]\n",
      "3029: [discriminator loss: 0.699861, acc: 0.562500] [adversarial loss: 0.692656, acc: 0.578125]\n",
      "3030: [discriminator loss: 0.706380, acc: 0.468750] [adversarial loss: 0.765264, acc: 0.359375]\n",
      "3031: [discriminator loss: 0.704025, acc: 0.523438] [adversarial loss: 0.791839, acc: 0.218750]\n",
      "3032: [discriminator loss: 0.681932, acc: 0.570312] [adversarial loss: 0.678815, acc: 0.531250]\n",
      "3033: [discriminator loss: 0.699064, acc: 0.507812] [adversarial loss: 0.764528, acc: 0.515625]\n",
      "3034: [discriminator loss: 0.690433, acc: 0.531250] [adversarial loss: 0.867455, acc: 0.218750]\n",
      "3035: [discriminator loss: 0.729053, acc: 0.492188] [adversarial loss: 0.617209, acc: 0.718750]\n",
      "3036: [discriminator loss: 0.668615, acc: 0.593750] [adversarial loss: 0.791840, acc: 0.265625]\n",
      "3037: [discriminator loss: 0.674043, acc: 0.578125] [adversarial loss: 0.698852, acc: 0.515625]\n",
      "3038: [discriminator loss: 0.676376, acc: 0.539062] [adversarial loss: 0.756036, acc: 0.375000]\n",
      "3039: [discriminator loss: 0.674489, acc: 0.601562] [adversarial loss: 0.792323, acc: 0.343750]\n",
      "3040: [discriminator loss: 0.696346, acc: 0.476562] [adversarial loss: 0.642033, acc: 0.546875]\n",
      "3041: [discriminator loss: 0.682014, acc: 0.500000] [adversarial loss: 0.768971, acc: 0.453125]\n",
      "3042: [discriminator loss: 0.697583, acc: 0.554688] [adversarial loss: 0.684083, acc: 0.515625]\n",
      "3043: [discriminator loss: 0.683950, acc: 0.562500] [adversarial loss: 0.775509, acc: 0.390625]\n",
      "3044: [discriminator loss: 0.658803, acc: 0.609375] [adversarial loss: 0.815595, acc: 0.296875]\n",
      "3045: [discriminator loss: 0.702137, acc: 0.500000] [adversarial loss: 0.697824, acc: 0.484375]\n",
      "3046: [discriminator loss: 0.717100, acc: 0.484375] [adversarial loss: 0.872605, acc: 0.171875]\n",
      "3047: [discriminator loss: 0.684220, acc: 0.570312] [adversarial loss: 0.636370, acc: 0.703125]\n",
      "3048: [discriminator loss: 0.673372, acc: 0.625000] [adversarial loss: 0.761606, acc: 0.421875]\n",
      "3049: [discriminator loss: 0.705473, acc: 0.523438] [adversarial loss: 0.711825, acc: 0.468750]\n",
      "3050: [discriminator loss: 0.695133, acc: 0.515625] [adversarial loss: 0.895516, acc: 0.109375]\n",
      "3051: [discriminator loss: 0.682692, acc: 0.531250] [adversarial loss: 0.598019, acc: 0.765625]\n",
      "3052: [discriminator loss: 0.686306, acc: 0.593750] [adversarial loss: 0.908669, acc: 0.093750]\n",
      "3053: [discriminator loss: 0.691673, acc: 0.554688] [adversarial loss: 0.648684, acc: 0.625000]\n",
      "3054: [discriminator loss: 0.665741, acc: 0.609375] [adversarial loss: 0.799787, acc: 0.296875]\n",
      "3055: [discriminator loss: 0.672786, acc: 0.554688] [adversarial loss: 0.750186, acc: 0.375000]\n",
      "3056: [discriminator loss: 0.680552, acc: 0.554688] [adversarial loss: 0.604925, acc: 0.781250]\n",
      "3057: [discriminator loss: 0.702148, acc: 0.562500] [adversarial loss: 0.793833, acc: 0.359375]\n",
      "3058: [discriminator loss: 0.678320, acc: 0.585938] [adversarial loss: 0.654892, acc: 0.765625]\n",
      "3059: [discriminator loss: 0.684171, acc: 0.531250] [adversarial loss: 0.926185, acc: 0.109375]\n",
      "3060: [discriminator loss: 0.715155, acc: 0.523438] [adversarial loss: 0.732650, acc: 0.437500]\n",
      "3061: [discriminator loss: 0.681361, acc: 0.570312] [adversarial loss: 0.723124, acc: 0.406250]\n",
      "3062: [discriminator loss: 0.697976, acc: 0.507812] [adversarial loss: 0.631047, acc: 0.750000]\n",
      "3063: [discriminator loss: 0.710060, acc: 0.460938] [adversarial loss: 0.869662, acc: 0.109375]\n",
      "3064: [discriminator loss: 0.681483, acc: 0.578125] [adversarial loss: 0.696244, acc: 0.515625]\n",
      "3065: [discriminator loss: 0.663530, acc: 0.601562] [adversarial loss: 0.723033, acc: 0.515625]\n",
      "3066: [discriminator loss: 0.677122, acc: 0.570312] [adversarial loss: 0.664437, acc: 0.625000]\n",
      "3067: [discriminator loss: 0.687216, acc: 0.570312] [adversarial loss: 0.807178, acc: 0.359375]\n",
      "3068: [discriminator loss: 0.692043, acc: 0.531250] [adversarial loss: 0.653518, acc: 0.546875]\n",
      "3069: [discriminator loss: 0.696975, acc: 0.578125] [adversarial loss: 0.772212, acc: 0.343750]\n",
      "3070: [discriminator loss: 0.697713, acc: 0.539062] [adversarial loss: 0.707907, acc: 0.484375]\n",
      "3071: [discriminator loss: 0.689883, acc: 0.539062] [adversarial loss: 0.772427, acc: 0.296875]\n",
      "3072: [discriminator loss: 0.711108, acc: 0.523438] [adversarial loss: 0.837439, acc: 0.125000]\n",
      "3073: [discriminator loss: 0.653756, acc: 0.640625] [adversarial loss: 0.693332, acc: 0.421875]\n",
      "3074: [discriminator loss: 0.718365, acc: 0.515625] [adversarial loss: 0.778528, acc: 0.328125]\n",
      "3075: [discriminator loss: 0.689151, acc: 0.546875] [adversarial loss: 0.710276, acc: 0.421875]\n",
      "3076: [discriminator loss: 0.691573, acc: 0.523438] [adversarial loss: 0.798373, acc: 0.218750]\n",
      "3077: [discriminator loss: 0.698312, acc: 0.476562] [adversarial loss: 0.647162, acc: 0.671875]\n",
      "3078: [discriminator loss: 0.711808, acc: 0.460938] [adversarial loss: 0.813860, acc: 0.140625]\n",
      "3079: [discriminator loss: 0.673760, acc: 0.570312] [adversarial loss: 0.760698, acc: 0.343750]\n",
      "3080: [discriminator loss: 0.708478, acc: 0.460938] [adversarial loss: 0.788043, acc: 0.234375]\n",
      "3081: [discriminator loss: 0.678340, acc: 0.578125] [adversarial loss: 0.654974, acc: 0.671875]\n",
      "3082: [discriminator loss: 0.693503, acc: 0.531250] [adversarial loss: 0.772094, acc: 0.328125]\n",
      "3083: [discriminator loss: 0.659356, acc: 0.632812] [adversarial loss: 0.651270, acc: 0.625000]\n",
      "3084: [discriminator loss: 0.684246, acc: 0.578125] [adversarial loss: 0.789897, acc: 0.343750]\n",
      "3085: [discriminator loss: 0.707199, acc: 0.515625] [adversarial loss: 0.657630, acc: 0.656250]\n",
      "3086: [discriminator loss: 0.684625, acc: 0.554688] [adversarial loss: 0.756913, acc: 0.421875]\n",
      "3087: [discriminator loss: 0.714437, acc: 0.531250] [adversarial loss: 0.757405, acc: 0.312500]\n",
      "3088: [discriminator loss: 0.672749, acc: 0.585938] [adversarial loss: 0.762031, acc: 0.406250]\n",
      "3089: [discriminator loss: 0.688269, acc: 0.562500] [adversarial loss: 0.797021, acc: 0.343750]\n",
      "3090: [discriminator loss: 0.696473, acc: 0.546875] [adversarial loss: 0.691621, acc: 0.484375]\n",
      "3091: [discriminator loss: 0.672417, acc: 0.570312] [adversarial loss: 0.809855, acc: 0.343750]\n",
      "3092: [discriminator loss: 0.696007, acc: 0.578125] [adversarial loss: 0.724427, acc: 0.515625]\n",
      "3093: [discriminator loss: 0.683969, acc: 0.562500] [adversarial loss: 0.726725, acc: 0.437500]\n",
      "3094: [discriminator loss: 0.686433, acc: 0.500000] [adversarial loss: 0.700124, acc: 0.484375]\n",
      "3095: [discriminator loss: 0.671606, acc: 0.585938] [adversarial loss: 0.630064, acc: 0.734375]\n",
      "3096: [discriminator loss: 0.691590, acc: 0.484375] [adversarial loss: 0.833035, acc: 0.218750]\n",
      "3097: [discriminator loss: 0.713139, acc: 0.500000] [adversarial loss: 0.609714, acc: 0.734375]\n",
      "3098: [discriminator loss: 0.697118, acc: 0.492188] [adversarial loss: 0.871890, acc: 0.078125]\n",
      "3099: [discriminator loss: 0.694835, acc: 0.570312] [adversarial loss: 0.625641, acc: 0.718750]\n",
      "3100: [discriminator loss: 0.691172, acc: 0.554688] [adversarial loss: 0.812316, acc: 0.312500]\n",
      "3101: [discriminator loss: 0.738952, acc: 0.476562] [adversarial loss: 0.663397, acc: 0.640625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3102: [discriminator loss: 0.682256, acc: 0.507812] [adversarial loss: 0.821218, acc: 0.171875]\n",
      "3103: [discriminator loss: 0.675981, acc: 0.546875] [adversarial loss: 0.694160, acc: 0.484375]\n",
      "3104: [discriminator loss: 0.670532, acc: 0.562500] [adversarial loss: 0.807441, acc: 0.296875]\n",
      "3105: [discriminator loss: 0.709628, acc: 0.492188] [adversarial loss: 0.661325, acc: 0.640625]\n",
      "3106: [discriminator loss: 0.701423, acc: 0.500000] [adversarial loss: 0.798036, acc: 0.156250]\n",
      "3107: [discriminator loss: 0.682849, acc: 0.570312] [adversarial loss: 0.675290, acc: 0.578125]\n",
      "3108: [discriminator loss: 0.698911, acc: 0.539062] [adversarial loss: 0.801183, acc: 0.156250]\n",
      "3109: [discriminator loss: 0.673552, acc: 0.578125] [adversarial loss: 0.699055, acc: 0.468750]\n",
      "3110: [discriminator loss: 0.684700, acc: 0.554688] [adversarial loss: 0.788426, acc: 0.437500]\n",
      "3111: [discriminator loss: 0.660454, acc: 0.648438] [adversarial loss: 0.748745, acc: 0.484375]\n",
      "3112: [discriminator loss: 0.672505, acc: 0.578125] [adversarial loss: 0.704851, acc: 0.531250]\n",
      "3113: [discriminator loss: 0.704887, acc: 0.500000] [adversarial loss: 0.815657, acc: 0.187500]\n",
      "3114: [discriminator loss: 0.668750, acc: 0.601562] [adversarial loss: 0.656394, acc: 0.703125]\n",
      "3115: [discriminator loss: 0.692113, acc: 0.507812] [adversarial loss: 0.736075, acc: 0.437500]\n",
      "3116: [discriminator loss: 0.688682, acc: 0.546875] [adversarial loss: 0.691789, acc: 0.578125]\n",
      "3117: [discriminator loss: 0.663912, acc: 0.609375] [adversarial loss: 0.800173, acc: 0.312500]\n",
      "3118: [discriminator loss: 0.697831, acc: 0.515625] [adversarial loss: 0.811134, acc: 0.265625]\n",
      "3119: [discriminator loss: 0.656525, acc: 0.593750] [adversarial loss: 0.736188, acc: 0.375000]\n",
      "3120: [discriminator loss: 0.699603, acc: 0.500000] [adversarial loss: 0.820607, acc: 0.375000]\n",
      "3121: [discriminator loss: 0.682005, acc: 0.585938] [adversarial loss: 0.722560, acc: 0.515625]\n",
      "3122: [discriminator loss: 0.686865, acc: 0.531250] [adversarial loss: 0.681890, acc: 0.609375]\n",
      "3123: [discriminator loss: 0.667610, acc: 0.585938] [adversarial loss: 0.807165, acc: 0.296875]\n",
      "3124: [discriminator loss: 0.707806, acc: 0.515625] [adversarial loss: 0.552972, acc: 0.875000]\n",
      "3125: [discriminator loss: 0.737421, acc: 0.500000] [adversarial loss: 0.810731, acc: 0.171875]\n",
      "3126: [discriminator loss: 0.701610, acc: 0.507812] [adversarial loss: 0.612419, acc: 0.765625]\n",
      "3127: [discriminator loss: 0.693291, acc: 0.585938] [adversarial loss: 0.757151, acc: 0.359375]\n",
      "3128: [discriminator loss: 0.680824, acc: 0.578125] [adversarial loss: 0.734979, acc: 0.437500]\n",
      "3129: [discriminator loss: 0.695311, acc: 0.515625] [adversarial loss: 0.790641, acc: 0.218750]\n",
      "3130: [discriminator loss: 0.691472, acc: 0.515625] [adversarial loss: 0.824517, acc: 0.156250]\n",
      "3131: [discriminator loss: 0.698331, acc: 0.515625] [adversarial loss: 0.621165, acc: 0.750000]\n",
      "3132: [discriminator loss: 0.707938, acc: 0.500000] [adversarial loss: 0.826558, acc: 0.203125]\n",
      "3133: [discriminator loss: 0.691877, acc: 0.554688] [adversarial loss: 0.629995, acc: 0.687500]\n",
      "3134: [discriminator loss: 0.685658, acc: 0.546875] [adversarial loss: 0.802368, acc: 0.234375]\n",
      "3135: [discriminator loss: 0.689712, acc: 0.531250] [adversarial loss: 0.696134, acc: 0.562500]\n",
      "3136: [discriminator loss: 0.691792, acc: 0.539062] [adversarial loss: 0.783775, acc: 0.171875]\n",
      "3137: [discriminator loss: 0.685021, acc: 0.562500] [adversarial loss: 0.816354, acc: 0.281250]\n",
      "3138: [discriminator loss: 0.693228, acc: 0.562500] [adversarial loss: 0.650228, acc: 0.671875]\n",
      "3139: [discriminator loss: 0.689185, acc: 0.546875] [adversarial loss: 0.772344, acc: 0.218750]\n",
      "3140: [discriminator loss: 0.670696, acc: 0.585938] [adversarial loss: 0.728050, acc: 0.453125]\n",
      "3141: [discriminator loss: 0.683764, acc: 0.601562] [adversarial loss: 0.766846, acc: 0.328125]\n",
      "3142: [discriminator loss: 0.684273, acc: 0.578125] [adversarial loss: 0.728181, acc: 0.500000]\n",
      "3143: [discriminator loss: 0.683636, acc: 0.539062] [adversarial loss: 0.749075, acc: 0.328125]\n",
      "3144: [discriminator loss: 0.667518, acc: 0.601562] [adversarial loss: 0.751814, acc: 0.421875]\n",
      "3145: [discriminator loss: 0.680452, acc: 0.578125] [adversarial loss: 0.676722, acc: 0.578125]\n",
      "3146: [discriminator loss: 0.695244, acc: 0.515625] [adversarial loss: 0.812000, acc: 0.140625]\n",
      "3147: [discriminator loss: 0.684756, acc: 0.570312] [adversarial loss: 0.771506, acc: 0.390625]\n",
      "3148: [discriminator loss: 0.660748, acc: 0.609375] [adversarial loss: 0.981306, acc: 0.171875]\n",
      "3149: [discriminator loss: 0.734345, acc: 0.476562] [adversarial loss: 0.535263, acc: 0.828125]\n",
      "3150: [discriminator loss: 0.734770, acc: 0.484375] [adversarial loss: 0.914029, acc: 0.140625]\n",
      "3151: [discriminator loss: 0.707496, acc: 0.531250] [adversarial loss: 0.643793, acc: 0.687500]\n",
      "3152: [discriminator loss: 0.683145, acc: 0.578125] [adversarial loss: 0.716343, acc: 0.453125]\n",
      "3153: [discriminator loss: 0.668790, acc: 0.640625] [adversarial loss: 0.725242, acc: 0.375000]\n",
      "3154: [discriminator loss: 0.652191, acc: 0.656250] [adversarial loss: 0.744108, acc: 0.468750]\n",
      "3155: [discriminator loss: 0.733250, acc: 0.445312] [adversarial loss: 0.782714, acc: 0.421875]\n",
      "3156: [discriminator loss: 0.706603, acc: 0.468750] [adversarial loss: 0.697405, acc: 0.593750]\n",
      "3157: [discriminator loss: 0.681319, acc: 0.523438] [adversarial loss: 0.735271, acc: 0.406250]\n",
      "3158: [discriminator loss: 0.705170, acc: 0.554688] [adversarial loss: 0.639673, acc: 0.640625]\n",
      "3159: [discriminator loss: 0.684686, acc: 0.523438] [adversarial loss: 0.659920, acc: 0.656250]\n",
      "3160: [discriminator loss: 0.664181, acc: 0.601562] [adversarial loss: 0.710629, acc: 0.500000]\n",
      "3161: [discriminator loss: 0.697100, acc: 0.531250] [adversarial loss: 0.687816, acc: 0.531250]\n",
      "3162: [discriminator loss: 0.681017, acc: 0.585938] [adversarial loss: 0.694163, acc: 0.531250]\n",
      "3163: [discriminator loss: 0.685647, acc: 0.554688] [adversarial loss: 0.760180, acc: 0.390625]\n",
      "3164: [discriminator loss: 0.706498, acc: 0.492188] [adversarial loss: 0.729212, acc: 0.453125]\n",
      "3165: [discriminator loss: 0.696014, acc: 0.539062] [adversarial loss: 0.775668, acc: 0.250000]\n",
      "3166: [discriminator loss: 0.689869, acc: 0.523438] [adversarial loss: 0.851514, acc: 0.109375]\n",
      "3167: [discriminator loss: 0.694981, acc: 0.515625] [adversarial loss: 0.642921, acc: 0.625000]\n",
      "3168: [discriminator loss: 0.687645, acc: 0.523438] [adversarial loss: 0.780012, acc: 0.234375]\n",
      "3169: [discriminator loss: 0.678163, acc: 0.601562] [adversarial loss: 0.704366, acc: 0.390625]\n",
      "3170: [discriminator loss: 0.687069, acc: 0.617188] [adversarial loss: 0.746780, acc: 0.328125]\n",
      "3171: [discriminator loss: 0.682768, acc: 0.546875] [adversarial loss: 0.665769, acc: 0.671875]\n",
      "3172: [discriminator loss: 0.693067, acc: 0.539062] [adversarial loss: 0.796470, acc: 0.218750]\n",
      "3173: [discriminator loss: 0.693402, acc: 0.515625] [adversarial loss: 0.703464, acc: 0.406250]\n",
      "3174: [discriminator loss: 0.670737, acc: 0.609375] [adversarial loss: 0.738385, acc: 0.375000]\n",
      "3175: [discriminator loss: 0.693261, acc: 0.531250] [adversarial loss: 0.710375, acc: 0.515625]\n",
      "3176: [discriminator loss: 0.708716, acc: 0.476562] [adversarial loss: 0.777008, acc: 0.234375]\n",
      "3177: [discriminator loss: 0.691794, acc: 0.523438] [adversarial loss: 0.692406, acc: 0.484375]\n",
      "3178: [discriminator loss: 0.709459, acc: 0.492188] [adversarial loss: 0.782531, acc: 0.250000]\n",
      "3179: [discriminator loss: 0.665305, acc: 0.640625] [adversarial loss: 0.716913, acc: 0.468750]\n",
      "3180: [discriminator loss: 0.700878, acc: 0.484375] [adversarial loss: 0.823278, acc: 0.265625]\n",
      "3181: [discriminator loss: 0.683796, acc: 0.562500] [adversarial loss: 0.634803, acc: 0.656250]\n",
      "3182: [discriminator loss: 0.687431, acc: 0.515625] [adversarial loss: 0.762808, acc: 0.328125]\n",
      "3183: [discriminator loss: 0.697324, acc: 0.507812] [adversarial loss: 0.681182, acc: 0.515625]\n",
      "3184: [discriminator loss: 0.666262, acc: 0.601562] [adversarial loss: 0.832825, acc: 0.281250]\n",
      "3185: [discriminator loss: 0.698787, acc: 0.546875] [adversarial loss: 0.682540, acc: 0.578125]\n",
      "3186: [discriminator loss: 0.682273, acc: 0.578125] [adversarial loss: 0.708737, acc: 0.468750]\n",
      "3187: [discriminator loss: 0.671612, acc: 0.578125] [adversarial loss: 0.718791, acc: 0.500000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3188: [discriminator loss: 0.689523, acc: 0.539062] [adversarial loss: 0.630702, acc: 0.703125]\n",
      "3189: [discriminator loss: 0.690026, acc: 0.523438] [adversarial loss: 0.722316, acc: 0.437500]\n",
      "3190: [discriminator loss: 0.666826, acc: 0.578125] [adversarial loss: 0.808372, acc: 0.265625]\n",
      "3191: [discriminator loss: 0.681492, acc: 0.570312] [adversarial loss: 0.791252, acc: 0.312500]\n",
      "3192: [discriminator loss: 0.655931, acc: 0.601562] [adversarial loss: 0.802679, acc: 0.312500]\n",
      "3193: [discriminator loss: 0.686527, acc: 0.562500] [adversarial loss: 0.645480, acc: 0.656250]\n",
      "3194: [discriminator loss: 0.695796, acc: 0.507812] [adversarial loss: 0.910890, acc: 0.109375]\n",
      "3195: [discriminator loss: 0.710296, acc: 0.554688] [adversarial loss: 0.575859, acc: 0.859375]\n",
      "3196: [discriminator loss: 0.690129, acc: 0.500000] [adversarial loss: 0.899055, acc: 0.156250]\n",
      "3197: [discriminator loss: 0.706327, acc: 0.492188] [adversarial loss: 0.601203, acc: 0.781250]\n",
      "3198: [discriminator loss: 0.705832, acc: 0.507812] [adversarial loss: 0.789452, acc: 0.234375]\n",
      "3199: [discriminator loss: 0.670769, acc: 0.625000] [adversarial loss: 0.666946, acc: 0.578125]\n",
      "3200: [discriminator loss: 0.712095, acc: 0.468750] [adversarial loss: 0.651466, acc: 0.640625]\n",
      "3201: [discriminator loss: 0.706238, acc: 0.523438] [adversarial loss: 0.750825, acc: 0.359375]\n",
      "3202: [discriminator loss: 0.682176, acc: 0.562500] [adversarial loss: 0.690899, acc: 0.609375]\n",
      "3203: [discriminator loss: 0.691468, acc: 0.570312] [adversarial loss: 0.719451, acc: 0.453125]\n",
      "3204: [discriminator loss: 0.677157, acc: 0.562500] [adversarial loss: 0.741754, acc: 0.312500]\n",
      "3205: [discriminator loss: 0.667435, acc: 0.625000] [adversarial loss: 0.699180, acc: 0.531250]\n",
      "3206: [discriminator loss: 0.716000, acc: 0.500000] [adversarial loss: 0.726389, acc: 0.390625]\n",
      "3207: [discriminator loss: 0.685465, acc: 0.562500] [adversarial loss: 0.775395, acc: 0.406250]\n",
      "3208: [discriminator loss: 0.665258, acc: 0.578125] [adversarial loss: 0.856737, acc: 0.234375]\n",
      "3209: [discriminator loss: 0.716239, acc: 0.531250] [adversarial loss: 0.640855, acc: 0.765625]\n",
      "3210: [discriminator loss: 0.695558, acc: 0.484375] [adversarial loss: 0.918692, acc: 0.109375]\n",
      "3211: [discriminator loss: 0.701069, acc: 0.507812] [adversarial loss: 0.645146, acc: 0.718750]\n",
      "3212: [discriminator loss: 0.709443, acc: 0.476562] [adversarial loss: 0.729924, acc: 0.453125]\n",
      "3213: [discriminator loss: 0.685256, acc: 0.523438] [adversarial loss: 0.823426, acc: 0.109375]\n",
      "3214: [discriminator loss: 0.670789, acc: 0.609375] [adversarial loss: 0.681826, acc: 0.593750]\n",
      "3215: [discriminator loss: 0.696681, acc: 0.500000] [adversarial loss: 0.731722, acc: 0.359375]\n",
      "3216: [discriminator loss: 0.676641, acc: 0.570312] [adversarial loss: 0.699259, acc: 0.468750]\n",
      "3217: [discriminator loss: 0.695019, acc: 0.484375] [adversarial loss: 0.840996, acc: 0.281250]\n",
      "3218: [discriminator loss: 0.704095, acc: 0.445312] [adversarial loss: 0.720898, acc: 0.421875]\n",
      "3219: [discriminator loss: 0.686116, acc: 0.570312] [adversarial loss: 0.811242, acc: 0.218750]\n",
      "3220: [discriminator loss: 0.670775, acc: 0.562500] [adversarial loss: 0.688175, acc: 0.531250]\n",
      "3221: [discriminator loss: 0.669075, acc: 0.625000] [adversarial loss: 0.747293, acc: 0.359375]\n",
      "3222: [discriminator loss: 0.723200, acc: 0.453125] [adversarial loss: 0.689470, acc: 0.531250]\n",
      "3223: [discriminator loss: 0.660549, acc: 0.601562] [adversarial loss: 0.778517, acc: 0.265625]\n",
      "3224: [discriminator loss: 0.694430, acc: 0.539062] [adversarial loss: 0.732815, acc: 0.406250]\n",
      "3225: [discriminator loss: 0.666326, acc: 0.617188] [adversarial loss: 0.775918, acc: 0.343750]\n",
      "3226: [discriminator loss: 0.675665, acc: 0.593750] [adversarial loss: 0.580050, acc: 0.828125]\n",
      "3227: [discriminator loss: 0.691889, acc: 0.578125] [adversarial loss: 0.812373, acc: 0.218750]\n",
      "3228: [discriminator loss: 0.688259, acc: 0.539062] [adversarial loss: 0.742761, acc: 0.406250]\n",
      "3229: [discriminator loss: 0.718232, acc: 0.515625] [adversarial loss: 0.730712, acc: 0.421875]\n",
      "3230: [discriminator loss: 0.696023, acc: 0.546875] [adversarial loss: 0.803295, acc: 0.265625]\n",
      "3231: [discriminator loss: 0.706879, acc: 0.500000] [adversarial loss: 0.649621, acc: 0.703125]\n",
      "3232: [discriminator loss: 0.696902, acc: 0.515625] [adversarial loss: 0.773452, acc: 0.250000]\n",
      "3233: [discriminator loss: 0.687572, acc: 0.570312] [adversarial loss: 0.655716, acc: 0.656250]\n",
      "3234: [discriminator loss: 0.668984, acc: 0.609375] [adversarial loss: 0.746412, acc: 0.453125]\n",
      "3235: [discriminator loss: 0.684976, acc: 0.562500] [adversarial loss: 0.700622, acc: 0.437500]\n",
      "3236: [discriminator loss: 0.684933, acc: 0.554688] [adversarial loss: 0.603221, acc: 0.718750]\n",
      "3237: [discriminator loss: 0.692635, acc: 0.492188] [adversarial loss: 0.848535, acc: 0.109375]\n",
      "3238: [discriminator loss: 0.706120, acc: 0.500000] [adversarial loss: 0.626378, acc: 0.703125]\n",
      "3239: [discriminator loss: 0.690935, acc: 0.546875] [adversarial loss: 0.738780, acc: 0.406250]\n",
      "3240: [discriminator loss: 0.680052, acc: 0.546875] [adversarial loss: 0.693735, acc: 0.593750]\n",
      "3241: [discriminator loss: 0.668241, acc: 0.593750] [adversarial loss: 0.764496, acc: 0.390625]\n",
      "3242: [discriminator loss: 0.667939, acc: 0.609375] [adversarial loss: 0.737823, acc: 0.437500]\n",
      "3243: [discriminator loss: 0.669360, acc: 0.585938] [adversarial loss: 0.700254, acc: 0.484375]\n",
      "3244: [discriminator loss: 0.708944, acc: 0.507812] [adversarial loss: 0.762173, acc: 0.453125]\n",
      "3245: [discriminator loss: 0.717016, acc: 0.507812] [adversarial loss: 0.607206, acc: 0.781250]\n",
      "3246: [discriminator loss: 0.703756, acc: 0.546875] [adversarial loss: 0.808703, acc: 0.156250]\n",
      "3247: [discriminator loss: 0.684885, acc: 0.554688] [adversarial loss: 0.653189, acc: 0.625000]\n",
      "3248: [discriminator loss: 0.672385, acc: 0.578125] [adversarial loss: 0.795877, acc: 0.328125]\n",
      "3249: [discriminator loss: 0.685353, acc: 0.554688] [adversarial loss: 0.664393, acc: 0.593750]\n",
      "3250: [discriminator loss: 0.732670, acc: 0.406250] [adversarial loss: 0.789015, acc: 0.187500]\n",
      "3251: [discriminator loss: 0.681489, acc: 0.570312] [adversarial loss: 0.695585, acc: 0.453125]\n",
      "3252: [discriminator loss: 0.674190, acc: 0.554688] [adversarial loss: 0.780358, acc: 0.343750]\n",
      "3253: [discriminator loss: 0.669929, acc: 0.617188] [adversarial loss: 0.764483, acc: 0.484375]\n",
      "3254: [discriminator loss: 0.690397, acc: 0.539062] [adversarial loss: 0.743562, acc: 0.421875]\n",
      "3255: [discriminator loss: 0.674453, acc: 0.593750] [adversarial loss: 0.741163, acc: 0.421875]\n",
      "3256: [discriminator loss: 0.686493, acc: 0.523438] [adversarial loss: 0.654056, acc: 0.625000]\n",
      "3257: [discriminator loss: 0.696526, acc: 0.531250] [adversarial loss: 0.886424, acc: 0.062500]\n",
      "3258: [discriminator loss: 0.691458, acc: 0.578125] [adversarial loss: 0.640208, acc: 0.656250]\n",
      "3259: [discriminator loss: 0.676516, acc: 0.562500] [adversarial loss: 0.852826, acc: 0.187500]\n",
      "3260: [discriminator loss: 0.685738, acc: 0.570312] [adversarial loss: 0.698562, acc: 0.500000]\n",
      "3261: [discriminator loss: 0.700629, acc: 0.515625] [adversarial loss: 0.775204, acc: 0.265625]\n",
      "3262: [discriminator loss: 0.671012, acc: 0.601562] [adversarial loss: 0.735699, acc: 0.281250]\n",
      "3263: [discriminator loss: 0.663430, acc: 0.640625] [adversarial loss: 0.719195, acc: 0.468750]\n",
      "3264: [discriminator loss: 0.702680, acc: 0.531250] [adversarial loss: 0.729088, acc: 0.375000]\n",
      "3265: [discriminator loss: 0.677365, acc: 0.515625] [adversarial loss: 0.793777, acc: 0.281250]\n",
      "3266: [discriminator loss: 0.693273, acc: 0.523438] [adversarial loss: 0.698654, acc: 0.468750]\n",
      "3267: [discriminator loss: 0.695616, acc: 0.539062] [adversarial loss: 0.736584, acc: 0.406250]\n",
      "3268: [discriminator loss: 0.724437, acc: 0.445312] [adversarial loss: 0.610371, acc: 0.828125]\n",
      "3269: [discriminator loss: 0.679842, acc: 0.546875] [adversarial loss: 0.750034, acc: 0.312500]\n",
      "3270: [discriminator loss: 0.662531, acc: 0.648438] [adversarial loss: 0.767591, acc: 0.468750]\n",
      "3271: [discriminator loss: 0.716453, acc: 0.500000] [adversarial loss: 0.704672, acc: 0.484375]\n",
      "3272: [discriminator loss: 0.702027, acc: 0.484375] [adversarial loss: 0.711081, acc: 0.468750]\n",
      "3273: [discriminator loss: 0.683839, acc: 0.531250] [adversarial loss: 0.771379, acc: 0.375000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3274: [discriminator loss: 0.658560, acc: 0.671875] [adversarial loss: 0.651278, acc: 0.640625]\n",
      "3275: [discriminator loss: 0.683289, acc: 0.562500] [adversarial loss: 0.816925, acc: 0.218750]\n",
      "3276: [discriminator loss: 0.680471, acc: 0.593750] [adversarial loss: 0.626065, acc: 0.671875]\n",
      "3277: [discriminator loss: 0.735813, acc: 0.500000] [adversarial loss: 0.859620, acc: 0.078125]\n",
      "3278: [discriminator loss: 0.714560, acc: 0.484375] [adversarial loss: 0.610609, acc: 0.796875]\n",
      "3279: [discriminator loss: 0.690292, acc: 0.539062] [adversarial loss: 0.857286, acc: 0.078125]\n",
      "3280: [discriminator loss: 0.680132, acc: 0.554688] [adversarial loss: 0.644495, acc: 0.718750]\n",
      "3281: [discriminator loss: 0.701706, acc: 0.476562] [adversarial loss: 0.733805, acc: 0.390625]\n",
      "3282: [discriminator loss: 0.676237, acc: 0.585938] [adversarial loss: 0.775726, acc: 0.343750]\n",
      "3283: [discriminator loss: 0.701965, acc: 0.523438] [adversarial loss: 0.710615, acc: 0.437500]\n",
      "3284: [discriminator loss: 0.680586, acc: 0.625000] [adversarial loss: 0.715274, acc: 0.453125]\n",
      "3285: [discriminator loss: 0.684279, acc: 0.546875] [adversarial loss: 0.761414, acc: 0.328125]\n",
      "3286: [discriminator loss: 0.680417, acc: 0.585938] [adversarial loss: 0.795704, acc: 0.421875]\n",
      "3287: [discriminator loss: 0.675317, acc: 0.554688] [adversarial loss: 0.630221, acc: 0.703125]\n",
      "3288: [discriminator loss: 0.704453, acc: 0.546875] [adversarial loss: 0.772186, acc: 0.421875]\n",
      "3289: [discriminator loss: 0.706788, acc: 0.507812] [adversarial loss: 0.683859, acc: 0.546875]\n",
      "3290: [discriminator loss: 0.698383, acc: 0.492188] [adversarial loss: 0.818244, acc: 0.156250]\n",
      "3291: [discriminator loss: 0.690081, acc: 0.554688] [adversarial loss: 0.641732, acc: 0.781250]\n",
      "3292: [discriminator loss: 0.700074, acc: 0.500000] [adversarial loss: 0.769142, acc: 0.265625]\n",
      "3293: [discriminator loss: 0.686347, acc: 0.554688] [adversarial loss: 0.637260, acc: 0.687500]\n",
      "3294: [discriminator loss: 0.685727, acc: 0.546875] [adversarial loss: 0.757775, acc: 0.265625]\n",
      "3295: [discriminator loss: 0.704535, acc: 0.421875] [adversarial loss: 0.611869, acc: 0.796875]\n",
      "3296: [discriminator loss: 0.692868, acc: 0.500000] [adversarial loss: 0.843749, acc: 0.109375]\n",
      "3297: [discriminator loss: 0.678696, acc: 0.523438] [adversarial loss: 0.690662, acc: 0.578125]\n",
      "3298: [discriminator loss: 0.696771, acc: 0.453125] [adversarial loss: 0.740990, acc: 0.328125]\n",
      "3299: [discriminator loss: 0.676279, acc: 0.593750] [adversarial loss: 0.689996, acc: 0.500000]\n",
      "3300: [discriminator loss: 0.701392, acc: 0.507812] [adversarial loss: 0.720471, acc: 0.500000]\n",
      "3301: [discriminator loss: 0.694921, acc: 0.562500] [adversarial loss: 0.796266, acc: 0.171875]\n",
      "3302: [discriminator loss: 0.697107, acc: 0.562500] [adversarial loss: 0.605561, acc: 0.796875]\n",
      "3303: [discriminator loss: 0.690586, acc: 0.546875] [adversarial loss: 0.744557, acc: 0.312500]\n",
      "3304: [discriminator loss: 0.678989, acc: 0.578125] [adversarial loss: 0.714573, acc: 0.500000]\n",
      "3305: [discriminator loss: 0.664585, acc: 0.609375] [adversarial loss: 0.735784, acc: 0.375000]\n",
      "3306: [discriminator loss: 0.690752, acc: 0.546875] [adversarial loss: 0.733825, acc: 0.406250]\n",
      "3307: [discriminator loss: 0.689959, acc: 0.546875] [adversarial loss: 0.698306, acc: 0.546875]\n",
      "3308: [discriminator loss: 0.687768, acc: 0.500000] [adversarial loss: 0.737628, acc: 0.343750]\n",
      "3309: [discriminator loss: 0.679323, acc: 0.570312] [adversarial loss: 0.589495, acc: 0.828125]\n",
      "3310: [discriminator loss: 0.721001, acc: 0.468750] [adversarial loss: 0.778232, acc: 0.171875]\n",
      "3311: [discriminator loss: 0.680530, acc: 0.570312] [adversarial loss: 0.621789, acc: 0.812500]\n",
      "3312: [discriminator loss: 0.676544, acc: 0.570312] [adversarial loss: 0.865448, acc: 0.109375]\n",
      "3313: [discriminator loss: 0.689423, acc: 0.531250] [adversarial loss: 0.682656, acc: 0.625000]\n",
      "3314: [discriminator loss: 0.707522, acc: 0.500000] [adversarial loss: 0.801084, acc: 0.296875]\n",
      "3315: [discriminator loss: 0.699442, acc: 0.531250] [adversarial loss: 0.654660, acc: 0.734375]\n",
      "3316: [discriminator loss: 0.661619, acc: 0.617188] [adversarial loss: 0.742398, acc: 0.609375]\n",
      "3317: [discriminator loss: 0.721366, acc: 0.515625] [adversarial loss: 0.647103, acc: 0.734375]\n",
      "3318: [discriminator loss: 0.683072, acc: 0.546875] [adversarial loss: 0.781314, acc: 0.203125]\n",
      "3319: [discriminator loss: 0.686616, acc: 0.539062] [adversarial loss: 0.643111, acc: 0.625000]\n",
      "3320: [discriminator loss: 0.674295, acc: 0.554688] [adversarial loss: 0.840747, acc: 0.187500]\n",
      "3321: [discriminator loss: 0.716453, acc: 0.500000] [adversarial loss: 0.729459, acc: 0.421875]\n",
      "3322: [discriminator loss: 0.674679, acc: 0.570312] [adversarial loss: 0.687256, acc: 0.500000]\n",
      "3323: [discriminator loss: 0.674461, acc: 0.601562] [adversarial loss: 0.754374, acc: 0.359375]\n",
      "3324: [discriminator loss: 0.706612, acc: 0.500000] [adversarial loss: 0.652121, acc: 0.687500]\n",
      "3325: [discriminator loss: 0.694363, acc: 0.539062] [adversarial loss: 0.721366, acc: 0.406250]\n",
      "3326: [discriminator loss: 0.737683, acc: 0.453125] [adversarial loss: 0.940173, acc: 0.109375]\n",
      "3327: [discriminator loss: 0.677110, acc: 0.554688] [adversarial loss: 0.613600, acc: 0.718750]\n",
      "3328: [discriminator loss: 0.674812, acc: 0.601562] [adversarial loss: 0.833379, acc: 0.140625]\n",
      "3329: [discriminator loss: 0.699099, acc: 0.523438] [adversarial loss: 0.695332, acc: 0.593750]\n",
      "3330: [discriminator loss: 0.675401, acc: 0.531250] [adversarial loss: 0.626670, acc: 0.718750]\n",
      "3331: [discriminator loss: 0.709985, acc: 0.484375] [adversarial loss: 0.752029, acc: 0.250000]\n",
      "3332: [discriminator loss: 0.689689, acc: 0.554688] [adversarial loss: 0.618373, acc: 0.796875]\n",
      "3333: [discriminator loss: 0.698214, acc: 0.531250] [adversarial loss: 0.676804, acc: 0.546875]\n",
      "3334: [discriminator loss: 0.682955, acc: 0.500000] [adversarial loss: 0.665616, acc: 0.593750]\n",
      "3335: [discriminator loss: 0.687413, acc: 0.546875] [adversarial loss: 0.709208, acc: 0.453125]\n",
      "3336: [discriminator loss: 0.675650, acc: 0.593750] [adversarial loss: 0.847343, acc: 0.156250]\n",
      "3337: [discriminator loss: 0.700578, acc: 0.531250] [adversarial loss: 0.566581, acc: 0.765625]\n",
      "3338: [discriminator loss: 0.741703, acc: 0.476562] [adversarial loss: 0.849983, acc: 0.203125]\n",
      "3339: [discriminator loss: 0.708859, acc: 0.453125] [adversarial loss: 0.634878, acc: 0.718750]\n",
      "3340: [discriminator loss: 0.678665, acc: 0.570312] [adversarial loss: 0.832592, acc: 0.140625]\n",
      "3341: [discriminator loss: 0.713718, acc: 0.484375] [adversarial loss: 0.698158, acc: 0.515625]\n",
      "3342: [discriminator loss: 0.672876, acc: 0.617188] [adversarial loss: 0.716094, acc: 0.468750]\n",
      "3343: [discriminator loss: 0.671798, acc: 0.578125] [adversarial loss: 0.780700, acc: 0.312500]\n",
      "3344: [discriminator loss: 0.668246, acc: 0.617188] [adversarial loss: 0.785238, acc: 0.312500]\n",
      "3345: [discriminator loss: 0.691530, acc: 0.531250] [adversarial loss: 0.694412, acc: 0.515625]\n",
      "3346: [discriminator loss: 0.695554, acc: 0.562500] [adversarial loss: 0.720423, acc: 0.437500]\n",
      "3347: [discriminator loss: 0.688836, acc: 0.554688] [adversarial loss: 0.736643, acc: 0.343750]\n",
      "3348: [discriminator loss: 0.706874, acc: 0.492188] [adversarial loss: 0.711767, acc: 0.500000]\n",
      "3349: [discriminator loss: 0.693215, acc: 0.515625] [adversarial loss: 0.712447, acc: 0.500000]\n",
      "3350: [discriminator loss: 0.690110, acc: 0.539062] [adversarial loss: 0.878241, acc: 0.140625]\n",
      "3351: [discriminator loss: 0.683895, acc: 0.539062] [adversarial loss: 0.705452, acc: 0.453125]\n",
      "3352: [discriminator loss: 0.672760, acc: 0.554688] [adversarial loss: 0.647301, acc: 0.671875]\n",
      "3353: [discriminator loss: 0.695876, acc: 0.562500] [adversarial loss: 0.822643, acc: 0.375000]\n",
      "3354: [discriminator loss: 0.717502, acc: 0.515625] [adversarial loss: 0.729909, acc: 0.359375]\n",
      "3355: [discriminator loss: 0.688831, acc: 0.523438] [adversarial loss: 0.662038, acc: 0.578125]\n",
      "3356: [discriminator loss: 0.703339, acc: 0.500000] [adversarial loss: 0.797754, acc: 0.281250]\n",
      "3357: [discriminator loss: 0.675287, acc: 0.585938] [adversarial loss: 0.674062, acc: 0.625000]\n",
      "3358: [discriminator loss: 0.672387, acc: 0.578125] [adversarial loss: 0.786649, acc: 0.406250]\n",
      "3359: [discriminator loss: 0.706681, acc: 0.492188] [adversarial loss: 0.733623, acc: 0.437500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3360: [discriminator loss: 0.691340, acc: 0.484375] [adversarial loss: 0.834525, acc: 0.093750]\n",
      "3361: [discriminator loss: 0.697279, acc: 0.515625] [adversarial loss: 0.637047, acc: 0.765625]\n",
      "3362: [discriminator loss: 0.690215, acc: 0.562500] [adversarial loss: 0.768167, acc: 0.312500]\n",
      "3363: [discriminator loss: 0.671062, acc: 0.609375] [adversarial loss: 0.695450, acc: 0.453125]\n",
      "3364: [discriminator loss: 0.695579, acc: 0.546875] [adversarial loss: 0.731938, acc: 0.375000]\n",
      "3365: [discriminator loss: 0.664244, acc: 0.617188] [adversarial loss: 0.690341, acc: 0.609375]\n",
      "3366: [discriminator loss: 0.705646, acc: 0.484375] [adversarial loss: 0.811831, acc: 0.234375]\n",
      "3367: [discriminator loss: 0.683866, acc: 0.601562] [adversarial loss: 0.613152, acc: 0.718750]\n",
      "3368: [discriminator loss: 0.715497, acc: 0.468750] [adversarial loss: 0.770144, acc: 0.265625]\n",
      "3369: [discriminator loss: 0.689509, acc: 0.515625] [adversarial loss: 0.775540, acc: 0.234375]\n",
      "3370: [discriminator loss: 0.706930, acc: 0.507812] [adversarial loss: 0.722442, acc: 0.390625]\n",
      "3371: [discriminator loss: 0.688203, acc: 0.578125] [adversarial loss: 0.720261, acc: 0.421875]\n",
      "3372: [discriminator loss: 0.705506, acc: 0.453125] [adversarial loss: 0.711391, acc: 0.468750]\n",
      "3373: [discriminator loss: 0.689462, acc: 0.554688] [adversarial loss: 0.666436, acc: 0.656250]\n",
      "3374: [discriminator loss: 0.685637, acc: 0.562500] [adversarial loss: 0.786277, acc: 0.156250]\n",
      "3375: [discriminator loss: 0.685115, acc: 0.554688] [adversarial loss: 0.663117, acc: 0.656250]\n",
      "3376: [discriminator loss: 0.664748, acc: 0.609375] [adversarial loss: 0.757142, acc: 0.328125]\n",
      "3377: [discriminator loss: 0.681696, acc: 0.578125] [adversarial loss: 0.656927, acc: 0.625000]\n",
      "3378: [discriminator loss: 0.692823, acc: 0.515625] [adversarial loss: 0.911906, acc: 0.140625]\n",
      "3379: [discriminator loss: 0.701833, acc: 0.570312] [adversarial loss: 0.615782, acc: 0.781250]\n",
      "3380: [discriminator loss: 0.702549, acc: 0.429688] [adversarial loss: 0.889385, acc: 0.093750]\n",
      "3381: [discriminator loss: 0.693119, acc: 0.570312] [adversarial loss: 0.640584, acc: 0.687500]\n",
      "3382: [discriminator loss: 0.684054, acc: 0.546875] [adversarial loss: 0.704280, acc: 0.437500]\n",
      "3383: [discriminator loss: 0.696971, acc: 0.539062] [adversarial loss: 0.688427, acc: 0.578125]\n",
      "3384: [discriminator loss: 0.681148, acc: 0.601562] [adversarial loss: 0.775766, acc: 0.250000]\n",
      "3385: [discriminator loss: 0.679673, acc: 0.546875] [adversarial loss: 0.740394, acc: 0.312500]\n",
      "3386: [discriminator loss: 0.690234, acc: 0.570312] [adversarial loss: 0.722856, acc: 0.484375]\n",
      "3387: [discriminator loss: 0.678225, acc: 0.601562] [adversarial loss: 0.847246, acc: 0.218750]\n",
      "3388: [discriminator loss: 0.685145, acc: 0.539062] [adversarial loss: 0.587600, acc: 0.703125]\n",
      "3389: [discriminator loss: 0.701042, acc: 0.492188] [adversarial loss: 0.823108, acc: 0.093750]\n",
      "3390: [discriminator loss: 0.705557, acc: 0.500000] [adversarial loss: 0.645427, acc: 0.656250]\n",
      "3391: [discriminator loss: 0.696524, acc: 0.562500] [adversarial loss: 0.824537, acc: 0.250000]\n",
      "3392: [discriminator loss: 0.691396, acc: 0.554688] [adversarial loss: 0.679880, acc: 0.531250]\n",
      "3393: [discriminator loss: 0.668212, acc: 0.625000] [adversarial loss: 0.718896, acc: 0.421875]\n",
      "3394: [discriminator loss: 0.701226, acc: 0.484375] [adversarial loss: 0.673456, acc: 0.656250]\n",
      "3395: [discriminator loss: 0.701140, acc: 0.507812] [adversarial loss: 0.678318, acc: 0.562500]\n",
      "3396: [discriminator loss: 0.691950, acc: 0.515625] [adversarial loss: 0.825955, acc: 0.109375]\n",
      "3397: [discriminator loss: 0.669046, acc: 0.617188] [adversarial loss: 0.661250, acc: 0.562500]\n",
      "3398: [discriminator loss: 0.727296, acc: 0.468750] [adversarial loss: 0.812089, acc: 0.109375]\n",
      "3399: [discriminator loss: 0.688974, acc: 0.546875] [adversarial loss: 0.777472, acc: 0.375000]\n",
      "3400: [discriminator loss: 0.678930, acc: 0.585938] [adversarial loss: 0.744439, acc: 0.437500]\n",
      "3401: [discriminator loss: 0.694326, acc: 0.539062] [adversarial loss: 0.647461, acc: 0.687500]\n",
      "3402: [discriminator loss: 0.700691, acc: 0.531250] [adversarial loss: 0.738966, acc: 0.359375]\n",
      "3403: [discriminator loss: 0.698132, acc: 0.476562] [adversarial loss: 0.674444, acc: 0.500000]\n",
      "3404: [discriminator loss: 0.692866, acc: 0.554688] [adversarial loss: 0.902714, acc: 0.109375]\n",
      "3405: [discriminator loss: 0.682212, acc: 0.507812] [adversarial loss: 0.666282, acc: 0.656250]\n",
      "3406: [discriminator loss: 0.695943, acc: 0.515625] [adversarial loss: 0.788081, acc: 0.265625]\n",
      "3407: [discriminator loss: 0.725209, acc: 0.492188] [adversarial loss: 0.644379, acc: 0.796875]\n",
      "3408: [discriminator loss: 0.667278, acc: 0.593750] [adversarial loss: 0.717869, acc: 0.453125]\n",
      "3409: [discriminator loss: 0.692002, acc: 0.500000] [adversarial loss: 0.750795, acc: 0.281250]\n",
      "3410: [discriminator loss: 0.679780, acc: 0.546875] [adversarial loss: 0.681895, acc: 0.656250]\n",
      "3411: [discriminator loss: 0.683554, acc: 0.601562] [adversarial loss: 0.726029, acc: 0.437500]\n",
      "3412: [discriminator loss: 0.692437, acc: 0.585938] [adversarial loss: 0.741067, acc: 0.328125]\n",
      "3413: [discriminator loss: 0.680075, acc: 0.593750] [adversarial loss: 0.716533, acc: 0.406250]\n",
      "3414: [discriminator loss: 0.691342, acc: 0.554688] [adversarial loss: 0.739386, acc: 0.375000]\n",
      "3415: [discriminator loss: 0.690287, acc: 0.546875] [adversarial loss: 0.801326, acc: 0.203125]\n",
      "3416: [discriminator loss: 0.699812, acc: 0.500000] [adversarial loss: 0.617212, acc: 0.687500]\n",
      "3417: [discriminator loss: 0.697527, acc: 0.539062] [adversarial loss: 0.776517, acc: 0.187500]\n",
      "3418: [discriminator loss: 0.686602, acc: 0.515625] [adversarial loss: 0.682847, acc: 0.609375]\n",
      "3419: [discriminator loss: 0.700289, acc: 0.476562] [adversarial loss: 0.705798, acc: 0.406250]\n",
      "3420: [discriminator loss: 0.705979, acc: 0.476562] [adversarial loss: 0.876821, acc: 0.015625]\n",
      "3421: [discriminator loss: 0.691823, acc: 0.523438] [adversarial loss: 0.619425, acc: 0.796875]\n",
      "3422: [discriminator loss: 0.712405, acc: 0.476562] [adversarial loss: 0.810391, acc: 0.187500]\n",
      "3423: [discriminator loss: 0.678590, acc: 0.523438] [adversarial loss: 0.688288, acc: 0.593750]\n",
      "3424: [discriminator loss: 0.689297, acc: 0.523438] [adversarial loss: 0.792821, acc: 0.109375]\n",
      "3425: [discriminator loss: 0.689853, acc: 0.546875] [adversarial loss: 0.649860, acc: 0.671875]\n",
      "3426: [discriminator loss: 0.681867, acc: 0.546875] [adversarial loss: 0.735595, acc: 0.406250]\n",
      "3427: [discriminator loss: 0.700226, acc: 0.453125] [adversarial loss: 0.618824, acc: 0.796875]\n",
      "3428: [discriminator loss: 0.683030, acc: 0.570312] [adversarial loss: 0.845549, acc: 0.125000]\n",
      "3429: [discriminator loss: 0.704107, acc: 0.546875] [adversarial loss: 0.688550, acc: 0.562500]\n",
      "3430: [discriminator loss: 0.680149, acc: 0.593750] [adversarial loss: 0.734798, acc: 0.437500]\n",
      "3431: [discriminator loss: 0.680676, acc: 0.531250] [adversarial loss: 0.687881, acc: 0.546875]\n",
      "3432: [discriminator loss: 0.698308, acc: 0.507812] [adversarial loss: 0.793947, acc: 0.218750]\n",
      "3433: [discriminator loss: 0.683353, acc: 0.539062] [adversarial loss: 0.724642, acc: 0.421875]\n",
      "3434: [discriminator loss: 0.706330, acc: 0.484375] [adversarial loss: 0.757338, acc: 0.406250]\n",
      "3435: [discriminator loss: 0.695960, acc: 0.531250] [adversarial loss: 0.695211, acc: 0.531250]\n",
      "3436: [discriminator loss: 0.692685, acc: 0.484375] [adversarial loss: 0.705190, acc: 0.531250]\n",
      "3437: [discriminator loss: 0.696200, acc: 0.515625] [adversarial loss: 0.679635, acc: 0.484375]\n",
      "3438: [discriminator loss: 0.707995, acc: 0.468750] [adversarial loss: 0.763883, acc: 0.281250]\n",
      "3439: [discriminator loss: 0.688501, acc: 0.539062] [adversarial loss: 0.701845, acc: 0.500000]\n",
      "3440: [discriminator loss: 0.672421, acc: 0.562500] [adversarial loss: 0.761442, acc: 0.234375]\n",
      "3441: [discriminator loss: 0.702091, acc: 0.492188] [adversarial loss: 0.641596, acc: 0.703125]\n",
      "3442: [discriminator loss: 0.672971, acc: 0.609375] [adversarial loss: 0.776208, acc: 0.390625]\n",
      "3443: [discriminator loss: 0.691351, acc: 0.554688] [adversarial loss: 0.682653, acc: 0.515625]\n",
      "3444: [discriminator loss: 0.692883, acc: 0.562500] [adversarial loss: 0.731654, acc: 0.421875]\n",
      "3445: [discriminator loss: 0.726132, acc: 0.468750] [adversarial loss: 0.726331, acc: 0.406250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3446: [discriminator loss: 0.676981, acc: 0.554688] [adversarial loss: 0.788275, acc: 0.265625]\n",
      "3447: [discriminator loss: 0.682320, acc: 0.531250] [adversarial loss: 0.682602, acc: 0.531250]\n",
      "3448: [discriminator loss: 0.696959, acc: 0.507812] [adversarial loss: 0.767475, acc: 0.250000]\n",
      "3449: [discriminator loss: 0.693601, acc: 0.523438] [adversarial loss: 0.660970, acc: 0.593750]\n",
      "3450: [discriminator loss: 0.698203, acc: 0.515625] [adversarial loss: 0.832764, acc: 0.140625]\n",
      "3451: [discriminator loss: 0.717769, acc: 0.445312] [adversarial loss: 0.630022, acc: 0.796875]\n",
      "3452: [discriminator loss: 0.685495, acc: 0.546875] [adversarial loss: 0.785599, acc: 0.250000]\n",
      "3453: [discriminator loss: 0.715424, acc: 0.476562] [adversarial loss: 0.658662, acc: 0.640625]\n",
      "3454: [discriminator loss: 0.703946, acc: 0.453125] [adversarial loss: 0.763504, acc: 0.265625]\n",
      "3455: [discriminator loss: 0.693830, acc: 0.500000] [adversarial loss: 0.662915, acc: 0.718750]\n",
      "3456: [discriminator loss: 0.690172, acc: 0.546875] [adversarial loss: 0.799307, acc: 0.156250]\n",
      "3457: [discriminator loss: 0.707366, acc: 0.523438] [adversarial loss: 0.663461, acc: 0.703125]\n",
      "3458: [discriminator loss: 0.692548, acc: 0.546875] [adversarial loss: 0.691657, acc: 0.578125]\n",
      "3459: [discriminator loss: 0.685676, acc: 0.546875] [adversarial loss: 0.738584, acc: 0.328125]\n",
      "3460: [discriminator loss: 0.700193, acc: 0.484375] [adversarial loss: 0.674125, acc: 0.625000]\n",
      "3461: [discriminator loss: 0.693867, acc: 0.578125] [adversarial loss: 0.806488, acc: 0.093750]\n",
      "3462: [discriminator loss: 0.686421, acc: 0.500000] [adversarial loss: 0.701109, acc: 0.609375]\n",
      "3463: [discriminator loss: 0.678223, acc: 0.531250] [adversarial loss: 0.835503, acc: 0.125000]\n",
      "3464: [discriminator loss: 0.694270, acc: 0.570312] [adversarial loss: 0.612628, acc: 0.750000]\n",
      "3465: [discriminator loss: 0.685490, acc: 0.507812] [adversarial loss: 0.760522, acc: 0.281250]\n",
      "3466: [discriminator loss: 0.696302, acc: 0.562500] [adversarial loss: 0.615518, acc: 0.718750]\n",
      "3467: [discriminator loss: 0.695853, acc: 0.515625] [adversarial loss: 0.807828, acc: 0.203125]\n",
      "3468: [discriminator loss: 0.691015, acc: 0.601562] [adversarial loss: 0.600476, acc: 0.765625]\n",
      "3469: [discriminator loss: 0.691937, acc: 0.570312] [adversarial loss: 0.786018, acc: 0.187500]\n",
      "3470: [discriminator loss: 0.689798, acc: 0.546875] [adversarial loss: 0.637926, acc: 0.750000]\n",
      "3471: [discriminator loss: 0.687484, acc: 0.539062] [adversarial loss: 0.723599, acc: 0.359375]\n",
      "3472: [discriminator loss: 0.706175, acc: 0.507812] [adversarial loss: 0.711216, acc: 0.437500]\n",
      "3473: [discriminator loss: 0.695368, acc: 0.507812] [adversarial loss: 0.689796, acc: 0.546875]\n",
      "3474: [discriminator loss: 0.686045, acc: 0.546875] [adversarial loss: 0.780419, acc: 0.140625]\n",
      "3475: [discriminator loss: 0.701666, acc: 0.546875] [adversarial loss: 0.625032, acc: 0.796875]\n",
      "3476: [discriminator loss: 0.695828, acc: 0.484375] [adversarial loss: 0.770031, acc: 0.125000]\n",
      "3477: [discriminator loss: 0.682958, acc: 0.601562] [adversarial loss: 0.714695, acc: 0.437500]\n",
      "3478: [discriminator loss: 0.693081, acc: 0.484375] [adversarial loss: 0.685142, acc: 0.578125]\n",
      "3479: [discriminator loss: 0.676179, acc: 0.617188] [adversarial loss: 0.736788, acc: 0.312500]\n",
      "3480: [discriminator loss: 0.683099, acc: 0.593750] [adversarial loss: 0.712968, acc: 0.406250]\n",
      "3481: [discriminator loss: 0.686860, acc: 0.593750] [adversarial loss: 0.738603, acc: 0.343750]\n",
      "3482: [discriminator loss: 0.692710, acc: 0.507812] [adversarial loss: 0.720487, acc: 0.437500]\n",
      "3483: [discriminator loss: 0.688757, acc: 0.546875] [adversarial loss: 0.706195, acc: 0.453125]\n",
      "3484: [discriminator loss: 0.692395, acc: 0.523438] [adversarial loss: 0.664327, acc: 0.593750]\n",
      "3485: [discriminator loss: 0.688197, acc: 0.539062] [adversarial loss: 0.880475, acc: 0.125000]\n",
      "3486: [discriminator loss: 0.742957, acc: 0.484375] [adversarial loss: 0.669449, acc: 0.640625]\n",
      "3487: [discriminator loss: 0.679876, acc: 0.585938] [adversarial loss: 0.693589, acc: 0.531250]\n",
      "3488: [discriminator loss: 0.699105, acc: 0.484375] [adversarial loss: 0.757539, acc: 0.187500]\n",
      "3489: [discriminator loss: 0.684197, acc: 0.578125] [adversarial loss: 0.654627, acc: 0.703125]\n",
      "3490: [discriminator loss: 0.698229, acc: 0.453125] [adversarial loss: 0.810177, acc: 0.171875]\n",
      "3491: [discriminator loss: 0.697014, acc: 0.546875] [adversarial loss: 0.624081, acc: 0.750000]\n",
      "3492: [discriminator loss: 0.683286, acc: 0.539062] [adversarial loss: 0.742753, acc: 0.312500]\n",
      "3493: [discriminator loss: 0.687649, acc: 0.500000] [adversarial loss: 0.632535, acc: 0.718750]\n",
      "3494: [discriminator loss: 0.707648, acc: 0.476562] [adversarial loss: 0.724003, acc: 0.375000]\n",
      "3495: [discriminator loss: 0.694577, acc: 0.531250] [adversarial loss: 0.810480, acc: 0.140625]\n",
      "3496: [discriminator loss: 0.692048, acc: 0.539062] [adversarial loss: 0.637741, acc: 0.765625]\n",
      "3497: [discriminator loss: 0.678630, acc: 0.609375] [adversarial loss: 0.733336, acc: 0.390625]\n",
      "3498: [discriminator loss: 0.693431, acc: 0.500000] [adversarial loss: 0.626862, acc: 0.765625]\n",
      "3499: [discriminator loss: 0.711304, acc: 0.484375] [adversarial loss: 0.710958, acc: 0.484375]\n",
      "cgan_mnist  labels for generated images:  [0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5]\n",
      "3500: [discriminator loss: 0.702469, acc: 0.468750] [adversarial loss: 0.674712, acc: 0.609375]\n",
      "3501: [discriminator loss: 0.677648, acc: 0.554688] [adversarial loss: 0.715390, acc: 0.406250]\n",
      "3502: [discriminator loss: 0.682138, acc: 0.570312] [adversarial loss: 0.718815, acc: 0.437500]\n",
      "3503: [discriminator loss: 0.685224, acc: 0.546875] [adversarial loss: 0.703990, acc: 0.484375]\n",
      "3504: [discriminator loss: 0.679307, acc: 0.554688] [adversarial loss: 0.840387, acc: 0.062500]\n",
      "3505: [discriminator loss: 0.708553, acc: 0.523438] [adversarial loss: 0.638828, acc: 0.750000]\n",
      "3506: [discriminator loss: 0.699526, acc: 0.515625] [adversarial loss: 0.735596, acc: 0.375000]\n",
      "3507: [discriminator loss: 0.678837, acc: 0.562500] [adversarial loss: 0.629445, acc: 0.671875]\n",
      "3508: [discriminator loss: 0.691841, acc: 0.546875] [adversarial loss: 0.723746, acc: 0.359375]\n",
      "3509: [discriminator loss: 0.690563, acc: 0.539062] [adversarial loss: 0.624735, acc: 0.828125]\n",
      "3510: [discriminator loss: 0.664034, acc: 0.632812] [adversarial loss: 0.687144, acc: 0.500000]\n",
      "3511: [discriminator loss: 0.703582, acc: 0.468750] [adversarial loss: 0.843051, acc: 0.078125]\n",
      "3512: [discriminator loss: 0.709437, acc: 0.507812] [adversarial loss: 0.677869, acc: 0.546875]\n",
      "3513: [discriminator loss: 0.703101, acc: 0.546875] [adversarial loss: 0.836766, acc: 0.234375]\n",
      "3514: [discriminator loss: 0.683981, acc: 0.578125] [adversarial loss: 0.710279, acc: 0.437500]\n",
      "3515: [discriminator loss: 0.685901, acc: 0.578125] [adversarial loss: 0.814776, acc: 0.187500]\n",
      "3516: [discriminator loss: 0.695425, acc: 0.476562] [adversarial loss: 0.642998, acc: 0.734375]\n",
      "3517: [discriminator loss: 0.694479, acc: 0.507812] [adversarial loss: 0.729723, acc: 0.328125]\n",
      "3518: [discriminator loss: 0.685187, acc: 0.523438] [adversarial loss: 0.669160, acc: 0.671875]\n",
      "3519: [discriminator loss: 0.686051, acc: 0.601562] [adversarial loss: 0.597399, acc: 0.796875]\n",
      "3520: [discriminator loss: 0.691841, acc: 0.554688] [adversarial loss: 0.803271, acc: 0.125000]\n",
      "3521: [discriminator loss: 0.688225, acc: 0.500000] [adversarial loss: 0.653926, acc: 0.656250]\n",
      "3522: [discriminator loss: 0.692332, acc: 0.531250] [adversarial loss: 0.793422, acc: 0.312500]\n",
      "3523: [discriminator loss: 0.692968, acc: 0.539062] [adversarial loss: 0.732775, acc: 0.421875]\n",
      "3524: [discriminator loss: 0.701929, acc: 0.476562] [adversarial loss: 0.758720, acc: 0.265625]\n",
      "3525: [discriminator loss: 0.690373, acc: 0.484375] [adversarial loss: 0.625436, acc: 0.828125]\n",
      "3526: [discriminator loss: 0.682006, acc: 0.578125] [adversarial loss: 0.849200, acc: 0.046875]\n",
      "3527: [discriminator loss: 0.687187, acc: 0.531250] [adversarial loss: 0.636847, acc: 0.687500]\n",
      "3528: [discriminator loss: 0.708350, acc: 0.460938] [adversarial loss: 0.894848, acc: 0.078125]\n",
      "3529: [discriminator loss: 0.697787, acc: 0.492188] [adversarial loss: 0.584565, acc: 0.859375]\n",
      "3530: [discriminator loss: 0.696107, acc: 0.531250] [adversarial loss: 0.914498, acc: 0.109375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3531: [discriminator loss: 0.713482, acc: 0.437500] [adversarial loss: 0.645623, acc: 0.781250]\n",
      "3532: [discriminator loss: 0.697747, acc: 0.539062] [adversarial loss: 0.750934, acc: 0.203125]\n",
      "3533: [discriminator loss: 0.681017, acc: 0.562500] [adversarial loss: 0.663412, acc: 0.656250]\n",
      "3534: [discriminator loss: 0.689638, acc: 0.546875] [adversarial loss: 0.778194, acc: 0.234375]\n",
      "3535: [discriminator loss: 0.671773, acc: 0.562500] [adversarial loss: 0.701515, acc: 0.546875]\n",
      "3536: [discriminator loss: 0.713244, acc: 0.460938] [adversarial loss: 0.759096, acc: 0.312500]\n",
      "3537: [discriminator loss: 0.688953, acc: 0.578125] [adversarial loss: 0.701584, acc: 0.546875]\n",
      "3538: [discriminator loss: 0.676785, acc: 0.562500] [adversarial loss: 0.754319, acc: 0.390625]\n",
      "3539: [discriminator loss: 0.690458, acc: 0.539062] [adversarial loss: 0.693080, acc: 0.546875]\n",
      "3540: [discriminator loss: 0.670080, acc: 0.625000] [adversarial loss: 0.715079, acc: 0.531250]\n",
      "3541: [discriminator loss: 0.690535, acc: 0.562500] [adversarial loss: 0.686317, acc: 0.484375]\n",
      "3542: [discriminator loss: 0.684088, acc: 0.531250] [adversarial loss: 0.753854, acc: 0.312500]\n",
      "3543: [discriminator loss: 0.704431, acc: 0.515625] [adversarial loss: 0.678699, acc: 0.609375]\n",
      "3544: [discriminator loss: 0.695464, acc: 0.539062] [adversarial loss: 0.752519, acc: 0.218750]\n",
      "3545: [discriminator loss: 0.685936, acc: 0.593750] [adversarial loss: 0.751708, acc: 0.328125]\n",
      "3546: [discriminator loss: 0.695875, acc: 0.507812] [adversarial loss: 0.671916, acc: 0.625000]\n",
      "3547: [discriminator loss: 0.678446, acc: 0.546875] [adversarial loss: 0.801907, acc: 0.187500]\n",
      "3548: [discriminator loss: 0.666796, acc: 0.617188] [adversarial loss: 0.623477, acc: 0.718750]\n",
      "3549: [discriminator loss: 0.682786, acc: 0.562500] [adversarial loss: 0.776292, acc: 0.203125]\n",
      "3550: [discriminator loss: 0.690926, acc: 0.515625] [adversarial loss: 0.652035, acc: 0.609375]\n",
      "3551: [discriminator loss: 0.695979, acc: 0.476562] [adversarial loss: 0.855838, acc: 0.062500]\n",
      "3552: [discriminator loss: 0.708530, acc: 0.492188] [adversarial loss: 0.659920, acc: 0.656250]\n",
      "3553: [discriminator loss: 0.692182, acc: 0.531250] [adversarial loss: 0.780364, acc: 0.187500]\n",
      "3554: [discriminator loss: 0.680424, acc: 0.609375] [adversarial loss: 0.681698, acc: 0.593750]\n",
      "3555: [discriminator loss: 0.673077, acc: 0.617188] [adversarial loss: 0.806853, acc: 0.187500]\n",
      "3556: [discriminator loss: 0.700697, acc: 0.531250] [adversarial loss: 0.613596, acc: 0.859375]\n",
      "3557: [discriminator loss: 0.685096, acc: 0.625000] [adversarial loss: 0.772111, acc: 0.250000]\n",
      "3558: [discriminator loss: 0.681194, acc: 0.546875] [adversarial loss: 0.688698, acc: 0.546875]\n",
      "3559: [discriminator loss: 0.687406, acc: 0.539062] [adversarial loss: 0.726933, acc: 0.421875]\n",
      "3560: [discriminator loss: 0.695058, acc: 0.523438] [adversarial loss: 0.702567, acc: 0.531250]\n",
      "3561: [discriminator loss: 0.693563, acc: 0.531250] [adversarial loss: 0.756747, acc: 0.281250]\n",
      "3562: [discriminator loss: 0.707473, acc: 0.507812] [adversarial loss: 0.685924, acc: 0.515625]\n",
      "3563: [discriminator loss: 0.675781, acc: 0.554688] [adversarial loss: 0.796978, acc: 0.265625]\n",
      "3564: [discriminator loss: 0.684797, acc: 0.570312] [adversarial loss: 0.697091, acc: 0.515625]\n",
      "3565: [discriminator loss: 0.680259, acc: 0.554688] [adversarial loss: 0.881323, acc: 0.093750]\n",
      "3566: [discriminator loss: 0.718250, acc: 0.500000] [adversarial loss: 0.563210, acc: 0.937500]\n",
      "3567: [discriminator loss: 0.698582, acc: 0.523438] [adversarial loss: 0.813561, acc: 0.078125]\n",
      "3568: [discriminator loss: 0.689958, acc: 0.523438] [adversarial loss: 0.587699, acc: 0.937500]\n",
      "3569: [discriminator loss: 0.691778, acc: 0.515625] [adversarial loss: 0.791262, acc: 0.156250]\n",
      "3570: [discriminator loss: 0.685068, acc: 0.492188] [adversarial loss: 0.669081, acc: 0.718750]\n",
      "3571: [discriminator loss: 0.693882, acc: 0.507812] [adversarial loss: 0.760217, acc: 0.296875]\n",
      "3572: [discriminator loss: 0.685305, acc: 0.562500] [adversarial loss: 0.731898, acc: 0.328125]\n",
      "3573: [discriminator loss: 0.690376, acc: 0.546875] [adversarial loss: 0.749428, acc: 0.328125]\n",
      "3574: [discriminator loss: 0.680337, acc: 0.601562] [adversarial loss: 0.732328, acc: 0.421875]\n",
      "3575: [discriminator loss: 0.688359, acc: 0.539062] [adversarial loss: 0.633821, acc: 0.734375]\n",
      "3576: [discriminator loss: 0.717125, acc: 0.453125] [adversarial loss: 0.729382, acc: 0.328125]\n",
      "3577: [discriminator loss: 0.688545, acc: 0.570312] [adversarial loss: 0.766403, acc: 0.250000]\n",
      "3578: [discriminator loss: 0.683954, acc: 0.546875] [adversarial loss: 0.701321, acc: 0.500000]\n",
      "3579: [discriminator loss: 0.696075, acc: 0.515625] [adversarial loss: 0.762087, acc: 0.234375]\n",
      "3580: [discriminator loss: 0.696972, acc: 0.500000] [adversarial loss: 0.701273, acc: 0.437500]\n",
      "3581: [discriminator loss: 0.681698, acc: 0.515625] [adversarial loss: 0.778727, acc: 0.218750]\n",
      "3582: [discriminator loss: 0.704029, acc: 0.484375] [adversarial loss: 0.664000, acc: 0.656250]\n",
      "3583: [discriminator loss: 0.693514, acc: 0.546875] [adversarial loss: 0.738477, acc: 0.312500]\n",
      "3584: [discriminator loss: 0.676980, acc: 0.609375] [adversarial loss: 0.716784, acc: 0.406250]\n",
      "3585: [discriminator loss: 0.695844, acc: 0.468750] [adversarial loss: 0.723602, acc: 0.390625]\n",
      "3586: [discriminator loss: 0.696970, acc: 0.515625] [adversarial loss: 0.770009, acc: 0.171875]\n",
      "3587: [discriminator loss: 0.685705, acc: 0.570312] [adversarial loss: 0.727053, acc: 0.328125]\n",
      "3588: [discriminator loss: 0.694265, acc: 0.468750] [adversarial loss: 0.782767, acc: 0.171875]\n",
      "3589: [discriminator loss: 0.685545, acc: 0.539062] [adversarial loss: 0.664044, acc: 0.609375]\n",
      "3590: [discriminator loss: 0.687967, acc: 0.546875] [adversarial loss: 0.734469, acc: 0.484375]\n",
      "3591: [discriminator loss: 0.663949, acc: 0.656250] [adversarial loss: 0.745447, acc: 0.328125]\n",
      "3592: [discriminator loss: 0.695835, acc: 0.531250] [adversarial loss: 0.561846, acc: 0.921875]\n",
      "3593: [discriminator loss: 0.697951, acc: 0.523438] [adversarial loss: 0.756828, acc: 0.343750]\n",
      "3594: [discriminator loss: 0.687562, acc: 0.539062] [adversarial loss: 0.671685, acc: 0.515625]\n",
      "3595: [discriminator loss: 0.691796, acc: 0.546875] [adversarial loss: 0.747774, acc: 0.343750]\n",
      "3596: [discriminator loss: 0.697596, acc: 0.453125] [adversarial loss: 0.776972, acc: 0.234375]\n",
      "3597: [discriminator loss: 0.682144, acc: 0.562500] [adversarial loss: 0.723657, acc: 0.484375]\n",
      "3598: [discriminator loss: 0.710009, acc: 0.468750] [adversarial loss: 0.709476, acc: 0.390625]\n",
      "3599: [discriminator loss: 0.679292, acc: 0.570312] [adversarial loss: 0.767661, acc: 0.156250]\n",
      "3600: [discriminator loss: 0.684021, acc: 0.585938] [adversarial loss: 0.759352, acc: 0.234375]\n",
      "3601: [discriminator loss: 0.684519, acc: 0.562500] [adversarial loss: 0.682435, acc: 0.562500]\n",
      "3602: [discriminator loss: 0.693980, acc: 0.484375] [adversarial loss: 0.729548, acc: 0.375000]\n",
      "3603: [discriminator loss: 0.682612, acc: 0.593750] [adversarial loss: 0.687347, acc: 0.562500]\n",
      "3604: [discriminator loss: 0.701755, acc: 0.476562] [adversarial loss: 0.670664, acc: 0.656250]\n",
      "3605: [discriminator loss: 0.679280, acc: 0.554688] [adversarial loss: 0.784510, acc: 0.218750]\n",
      "3606: [discriminator loss: 0.686551, acc: 0.601562] [adversarial loss: 0.672531, acc: 0.484375]\n",
      "3607: [discriminator loss: 0.708989, acc: 0.476562] [adversarial loss: 0.811742, acc: 0.125000]\n",
      "3608: [discriminator loss: 0.682332, acc: 0.601562] [adversarial loss: 0.692088, acc: 0.421875]\n",
      "3609: [discriminator loss: 0.700966, acc: 0.484375] [adversarial loss: 0.732469, acc: 0.343750]\n",
      "3610: [discriminator loss: 0.686549, acc: 0.562500] [adversarial loss: 0.687352, acc: 0.546875]\n",
      "3611: [discriminator loss: 0.679573, acc: 0.585938] [adversarial loss: 0.771697, acc: 0.421875]\n",
      "3612: [discriminator loss: 0.712206, acc: 0.523438] [adversarial loss: 0.702377, acc: 0.515625]\n",
      "3613: [discriminator loss: 0.685713, acc: 0.546875] [adversarial loss: 0.700983, acc: 0.437500]\n",
      "3614: [discriminator loss: 0.696034, acc: 0.460938] [adversarial loss: 0.707379, acc: 0.406250]\n",
      "3615: [discriminator loss: 0.690582, acc: 0.523438] [adversarial loss: 0.686398, acc: 0.578125]\n",
      "3616: [discriminator loss: 0.704293, acc: 0.531250] [adversarial loss: 0.723836, acc: 0.437500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3617: [discriminator loss: 0.694085, acc: 0.476562] [adversarial loss: 0.701543, acc: 0.500000]\n",
      "3618: [discriminator loss: 0.680297, acc: 0.585938] [adversarial loss: 0.786378, acc: 0.359375]\n",
      "3619: [discriminator loss: 0.713474, acc: 0.531250] [adversarial loss: 0.599320, acc: 0.843750]\n",
      "3620: [discriminator loss: 0.708972, acc: 0.484375] [adversarial loss: 0.758913, acc: 0.234375]\n",
      "3621: [discriminator loss: 0.687628, acc: 0.562500] [adversarial loss: 0.682446, acc: 0.500000]\n",
      "3622: [discriminator loss: 0.710081, acc: 0.437500] [adversarial loss: 0.751417, acc: 0.328125]\n",
      "3623: [discriminator loss: 0.694073, acc: 0.539062] [adversarial loss: 0.731367, acc: 0.281250]\n",
      "3624: [discriminator loss: 0.678392, acc: 0.554688] [adversarial loss: 0.693750, acc: 0.515625]\n",
      "3625: [discriminator loss: 0.701809, acc: 0.476562] [adversarial loss: 0.786709, acc: 0.171875]\n",
      "3626: [discriminator loss: 0.682593, acc: 0.578125] [adversarial loss: 0.734910, acc: 0.312500]\n",
      "3627: [discriminator loss: 0.674562, acc: 0.570312] [adversarial loss: 0.741974, acc: 0.281250]\n",
      "3628: [discriminator loss: 0.678167, acc: 0.562500] [adversarial loss: 0.683362, acc: 0.500000]\n",
      "3629: [discriminator loss: 0.696054, acc: 0.523438] [adversarial loss: 0.872052, acc: 0.140625]\n",
      "3630: [discriminator loss: 0.704928, acc: 0.484375] [adversarial loss: 0.624135, acc: 0.781250]\n",
      "3631: [discriminator loss: 0.691858, acc: 0.523438] [adversarial loss: 0.781271, acc: 0.281250]\n",
      "3632: [discriminator loss: 0.720783, acc: 0.484375] [adversarial loss: 0.656477, acc: 0.687500]\n",
      "3633: [discriminator loss: 0.690737, acc: 0.515625] [adversarial loss: 0.738452, acc: 0.312500]\n",
      "3634: [discriminator loss: 0.689345, acc: 0.523438] [adversarial loss: 0.654624, acc: 0.671875]\n",
      "3635: [discriminator loss: 0.675892, acc: 0.601562] [adversarial loss: 0.763008, acc: 0.390625]\n",
      "3636: [discriminator loss: 0.680338, acc: 0.593750] [adversarial loss: 0.690900, acc: 0.484375]\n",
      "3637: [discriminator loss: 0.688381, acc: 0.539062] [adversarial loss: 0.778183, acc: 0.296875]\n",
      "3638: [discriminator loss: 0.710702, acc: 0.523438] [adversarial loss: 0.667934, acc: 0.562500]\n",
      "3639: [discriminator loss: 0.701815, acc: 0.531250] [adversarial loss: 0.701519, acc: 0.421875]\n",
      "3640: [discriminator loss: 0.687999, acc: 0.554688] [adversarial loss: 0.758927, acc: 0.296875]\n",
      "3641: [discriminator loss: 0.693930, acc: 0.500000] [adversarial loss: 0.633011, acc: 0.687500]\n",
      "3642: [discriminator loss: 0.689795, acc: 0.554688] [adversarial loss: 0.722781, acc: 0.390625]\n",
      "3643: [discriminator loss: 0.684926, acc: 0.546875] [adversarial loss: 0.686507, acc: 0.531250]\n",
      "3644: [discriminator loss: 0.692869, acc: 0.484375] [adversarial loss: 0.748351, acc: 0.234375]\n",
      "3645: [discriminator loss: 0.699809, acc: 0.515625] [adversarial loss: 0.695104, acc: 0.484375]\n",
      "3646: [discriminator loss: 0.683553, acc: 0.539062] [adversarial loss: 0.810112, acc: 0.234375]\n",
      "3647: [discriminator loss: 0.707386, acc: 0.460938] [adversarial loss: 0.587890, acc: 0.937500]\n",
      "3648: [discriminator loss: 0.695661, acc: 0.507812] [adversarial loss: 0.763465, acc: 0.203125]\n",
      "3649: [discriminator loss: 0.687248, acc: 0.523438] [adversarial loss: 0.613527, acc: 0.781250]\n",
      "3650: [discriminator loss: 0.691613, acc: 0.507812] [adversarial loss: 0.766981, acc: 0.265625]\n",
      "3651: [discriminator loss: 0.675291, acc: 0.609375] [adversarial loss: 0.682853, acc: 0.609375]\n",
      "3652: [discriminator loss: 0.679130, acc: 0.554688] [adversarial loss: 0.673446, acc: 0.609375]\n",
      "3653: [discriminator loss: 0.701281, acc: 0.468750] [adversarial loss: 0.701692, acc: 0.546875]\n",
      "3654: [discriminator loss: 0.699740, acc: 0.429688] [adversarial loss: 0.717206, acc: 0.500000]\n",
      "3655: [discriminator loss: 0.698925, acc: 0.492188] [adversarial loss: 0.716114, acc: 0.468750]\n",
      "3656: [discriminator loss: 0.691728, acc: 0.515625] [adversarial loss: 0.700305, acc: 0.515625]\n",
      "3657: [discriminator loss: 0.694018, acc: 0.460938] [adversarial loss: 0.732491, acc: 0.390625]\n",
      "3658: [discriminator loss: 0.674279, acc: 0.632812] [adversarial loss: 0.717704, acc: 0.390625]\n",
      "3659: [discriminator loss: 0.705853, acc: 0.476562] [adversarial loss: 0.665282, acc: 0.687500]\n",
      "3660: [discriminator loss: 0.700581, acc: 0.515625] [adversarial loss: 0.662003, acc: 0.562500]\n",
      "3661: [discriminator loss: 0.708619, acc: 0.453125] [adversarial loss: 0.752827, acc: 0.250000]\n",
      "3662: [discriminator loss: 0.690193, acc: 0.539062] [adversarial loss: 0.703501, acc: 0.437500]\n",
      "3663: [discriminator loss: 0.697164, acc: 0.468750] [adversarial loss: 0.814072, acc: 0.046875]\n",
      "3664: [discriminator loss: 0.692887, acc: 0.562500] [adversarial loss: 0.644824, acc: 0.828125]\n",
      "3665: [discriminator loss: 0.699714, acc: 0.484375] [adversarial loss: 0.781892, acc: 0.109375]\n",
      "3666: [discriminator loss: 0.679161, acc: 0.578125] [adversarial loss: 0.706691, acc: 0.484375]\n",
      "3667: [discriminator loss: 0.682137, acc: 0.554688] [adversarial loss: 0.790273, acc: 0.140625]\n",
      "3668: [discriminator loss: 0.696203, acc: 0.562500] [adversarial loss: 0.603284, acc: 0.781250]\n",
      "3669: [discriminator loss: 0.690782, acc: 0.562500] [adversarial loss: 0.835200, acc: 0.171875]\n",
      "3670: [discriminator loss: 0.701339, acc: 0.531250] [adversarial loss: 0.612435, acc: 0.781250]\n",
      "3671: [discriminator loss: 0.688259, acc: 0.554688] [adversarial loss: 0.691553, acc: 0.468750]\n",
      "3672: [discriminator loss: 0.680022, acc: 0.593750] [adversarial loss: 0.753909, acc: 0.250000]\n",
      "3673: [discriminator loss: 0.692633, acc: 0.539062] [adversarial loss: 0.708144, acc: 0.437500]\n",
      "3674: [discriminator loss: 0.683258, acc: 0.562500] [adversarial loss: 0.772226, acc: 0.218750]\n",
      "3675: [discriminator loss: 0.689056, acc: 0.515625] [adversarial loss: 0.756686, acc: 0.218750]\n",
      "3676: [discriminator loss: 0.684561, acc: 0.562500] [adversarial loss: 0.725473, acc: 0.359375]\n",
      "3677: [discriminator loss: 0.697262, acc: 0.476562] [adversarial loss: 0.668614, acc: 0.687500]\n",
      "3678: [discriminator loss: 0.699036, acc: 0.500000] [adversarial loss: 0.758746, acc: 0.312500]\n",
      "3679: [discriminator loss: 0.690650, acc: 0.531250] [adversarial loss: 0.680016, acc: 0.609375]\n",
      "3680: [discriminator loss: 0.697940, acc: 0.515625] [adversarial loss: 0.864881, acc: 0.156250]\n",
      "3681: [discriminator loss: 0.700742, acc: 0.515625] [adversarial loss: 0.615512, acc: 0.781250]\n",
      "3682: [discriminator loss: 0.708776, acc: 0.507812] [adversarial loss: 0.736287, acc: 0.328125]\n",
      "3683: [discriminator loss: 0.691766, acc: 0.546875] [adversarial loss: 0.654316, acc: 0.640625]\n",
      "3684: [discriminator loss: 0.684998, acc: 0.625000] [adversarial loss: 0.680065, acc: 0.609375]\n",
      "3685: [discriminator loss: 0.689166, acc: 0.515625] [adversarial loss: 0.674656, acc: 0.640625]\n",
      "3686: [discriminator loss: 0.686910, acc: 0.609375] [adversarial loss: 0.726995, acc: 0.453125]\n",
      "3687: [discriminator loss: 0.686317, acc: 0.562500] [adversarial loss: 0.673320, acc: 0.578125]\n",
      "3688: [discriminator loss: 0.699860, acc: 0.523438] [adversarial loss: 0.774727, acc: 0.187500]\n",
      "3689: [discriminator loss: 0.678827, acc: 0.523438] [adversarial loss: 0.705160, acc: 0.437500]\n",
      "3690: [discriminator loss: 0.689137, acc: 0.570312] [adversarial loss: 0.767461, acc: 0.234375]\n",
      "3691: [discriminator loss: 0.685025, acc: 0.554688] [adversarial loss: 0.734159, acc: 0.312500]\n",
      "3692: [discriminator loss: 0.690069, acc: 0.546875] [adversarial loss: 0.638909, acc: 0.781250]\n",
      "3693: [discriminator loss: 0.706490, acc: 0.476562] [adversarial loss: 0.754425, acc: 0.281250]\n",
      "3694: [discriminator loss: 0.683496, acc: 0.601562] [adversarial loss: 0.705352, acc: 0.500000]\n",
      "3695: [discriminator loss: 0.705148, acc: 0.492188] [adversarial loss: 0.672614, acc: 0.531250]\n",
      "3696: [discriminator loss: 0.693785, acc: 0.539062] [adversarial loss: 0.751542, acc: 0.265625]\n",
      "3697: [discriminator loss: 0.680469, acc: 0.570312] [adversarial loss: 0.728979, acc: 0.312500]\n",
      "3698: [discriminator loss: 0.714417, acc: 0.453125] [adversarial loss: 0.626714, acc: 0.812500]\n",
      "3699: [discriminator loss: 0.684989, acc: 0.539062] [adversarial loss: 0.750900, acc: 0.343750]\n",
      "3700: [discriminator loss: 0.700116, acc: 0.507812] [adversarial loss: 0.706331, acc: 0.531250]\n",
      "3701: [discriminator loss: 0.675865, acc: 0.570312] [adversarial loss: 0.690389, acc: 0.515625]\n",
      "3702: [discriminator loss: 0.682020, acc: 0.523438] [adversarial loss: 0.729032, acc: 0.343750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3703: [discriminator loss: 0.680047, acc: 0.601562] [adversarial loss: 0.713547, acc: 0.500000]\n",
      "3704: [discriminator loss: 0.685728, acc: 0.523438] [adversarial loss: 0.761787, acc: 0.218750]\n",
      "3705: [discriminator loss: 0.698415, acc: 0.476562] [adversarial loss: 0.676680, acc: 0.500000]\n",
      "3706: [discriminator loss: 0.699102, acc: 0.531250] [adversarial loss: 0.851987, acc: 0.046875]\n",
      "3707: [discriminator loss: 0.695946, acc: 0.492188] [adversarial loss: 0.610826, acc: 0.859375]\n",
      "3708: [discriminator loss: 0.672531, acc: 0.554688] [adversarial loss: 0.804286, acc: 0.218750]\n",
      "3709: [discriminator loss: 0.678020, acc: 0.585938] [adversarial loss: 0.593897, acc: 0.750000]\n",
      "3710: [discriminator loss: 0.702495, acc: 0.507812] [adversarial loss: 0.707324, acc: 0.515625]\n",
      "3711: [discriminator loss: 0.700756, acc: 0.507812] [adversarial loss: 0.711611, acc: 0.406250]\n",
      "3712: [discriminator loss: 0.693503, acc: 0.531250] [adversarial loss: 0.694844, acc: 0.500000]\n",
      "3713: [discriminator loss: 0.697585, acc: 0.523438] [adversarial loss: 0.711408, acc: 0.406250]\n",
      "3714: [discriminator loss: 0.700597, acc: 0.507812] [adversarial loss: 0.799362, acc: 0.156250]\n",
      "3715: [discriminator loss: 0.686795, acc: 0.523438] [adversarial loss: 0.649513, acc: 0.765625]\n",
      "3716: [discriminator loss: 0.682420, acc: 0.539062] [adversarial loss: 0.724441, acc: 0.359375]\n",
      "3717: [discriminator loss: 0.668933, acc: 0.570312] [adversarial loss: 0.695218, acc: 0.593750]\n",
      "3718: [discriminator loss: 0.698611, acc: 0.484375] [adversarial loss: 0.702716, acc: 0.468750]\n",
      "3719: [discriminator loss: 0.678888, acc: 0.578125] [adversarial loss: 0.731324, acc: 0.406250]\n",
      "3720: [discriminator loss: 0.687591, acc: 0.523438] [adversarial loss: 0.654331, acc: 0.718750]\n",
      "3721: [discriminator loss: 0.692744, acc: 0.539062] [adversarial loss: 0.785100, acc: 0.250000]\n",
      "3722: [discriminator loss: 0.710579, acc: 0.492188] [adversarial loss: 0.603882, acc: 0.812500]\n",
      "3723: [discriminator loss: 0.713923, acc: 0.484375] [adversarial loss: 0.775691, acc: 0.125000]\n",
      "3724: [discriminator loss: 0.697633, acc: 0.476562] [adversarial loss: 0.703585, acc: 0.484375]\n",
      "3725: [discriminator loss: 0.684017, acc: 0.601562] [adversarial loss: 0.685325, acc: 0.546875]\n",
      "3726: [discriminator loss: 0.686277, acc: 0.539062] [adversarial loss: 0.709200, acc: 0.484375]\n",
      "3727: [discriminator loss: 0.691998, acc: 0.523438] [adversarial loss: 0.732839, acc: 0.468750]\n",
      "3728: [discriminator loss: 0.708764, acc: 0.460938] [adversarial loss: 0.671764, acc: 0.640625]\n",
      "3729: [discriminator loss: 0.688045, acc: 0.617188] [adversarial loss: 0.854911, acc: 0.109375]\n",
      "3730: [discriminator loss: 0.694989, acc: 0.507812] [adversarial loss: 0.653838, acc: 0.671875]\n",
      "3731: [discriminator loss: 0.684000, acc: 0.554688] [adversarial loss: 0.732799, acc: 0.406250]\n",
      "3732: [discriminator loss: 0.709784, acc: 0.492188] [adversarial loss: 0.737274, acc: 0.265625]\n",
      "3733: [discriminator loss: 0.694481, acc: 0.484375] [adversarial loss: 0.686604, acc: 0.500000]\n",
      "3734: [discriminator loss: 0.687811, acc: 0.554688] [adversarial loss: 0.737013, acc: 0.265625]\n",
      "3735: [discriminator loss: 0.685518, acc: 0.515625] [adversarial loss: 0.726806, acc: 0.390625]\n",
      "3736: [discriminator loss: 0.692289, acc: 0.515625] [adversarial loss: 0.730106, acc: 0.359375]\n",
      "3737: [discriminator loss: 0.698193, acc: 0.492188] [adversarial loss: 0.721078, acc: 0.437500]\n",
      "3738: [discriminator loss: 0.688537, acc: 0.539062] [adversarial loss: 0.766109, acc: 0.312500]\n",
      "3739: [discriminator loss: 0.677744, acc: 0.609375] [adversarial loss: 0.734886, acc: 0.265625]\n",
      "3740: [discriminator loss: 0.695047, acc: 0.531250] [adversarial loss: 0.690212, acc: 0.453125]\n",
      "3741: [discriminator loss: 0.684969, acc: 0.570312] [adversarial loss: 0.695771, acc: 0.437500]\n",
      "3742: [discriminator loss: 0.689882, acc: 0.492188] [adversarial loss: 0.764945, acc: 0.296875]\n",
      "3743: [discriminator loss: 0.670147, acc: 0.593750] [adversarial loss: 0.710952, acc: 0.546875]\n",
      "3744: [discriminator loss: 0.679672, acc: 0.593750] [adversarial loss: 0.761551, acc: 0.343750]\n",
      "3745: [discriminator loss: 0.696675, acc: 0.484375] [adversarial loss: 0.567490, acc: 0.859375]\n",
      "3746: [discriminator loss: 0.707864, acc: 0.476562] [adversarial loss: 0.868490, acc: 0.109375]\n",
      "3747: [discriminator loss: 0.703123, acc: 0.515625] [adversarial loss: 0.606555, acc: 0.875000]\n",
      "3748: [discriminator loss: 0.684229, acc: 0.523438] [adversarial loss: 0.772049, acc: 0.187500]\n",
      "3749: [discriminator loss: 0.698753, acc: 0.492188] [adversarial loss: 0.676503, acc: 0.593750]\n",
      "3750: [discriminator loss: 0.694959, acc: 0.562500] [adversarial loss: 0.703266, acc: 0.546875]\n",
      "3751: [discriminator loss: 0.681506, acc: 0.609375] [adversarial loss: 0.664458, acc: 0.640625]\n",
      "3752: [discriminator loss: 0.712703, acc: 0.476562] [adversarial loss: 0.761302, acc: 0.218750]\n",
      "3753: [discriminator loss: 0.706206, acc: 0.484375] [adversarial loss: 0.717097, acc: 0.453125]\n",
      "3754: [discriminator loss: 0.677382, acc: 0.625000] [adversarial loss: 0.774098, acc: 0.375000]\n",
      "3755: [discriminator loss: 0.711502, acc: 0.484375] [adversarial loss: 0.656018, acc: 0.765625]\n",
      "3756: [discriminator loss: 0.681645, acc: 0.562500] [adversarial loss: 0.739498, acc: 0.281250]\n",
      "3757: [discriminator loss: 0.680880, acc: 0.539062] [adversarial loss: 0.754887, acc: 0.250000]\n",
      "3758: [discriminator loss: 0.677578, acc: 0.539062] [adversarial loss: 0.658533, acc: 0.640625]\n",
      "3759: [discriminator loss: 0.687630, acc: 0.562500] [adversarial loss: 0.628582, acc: 0.859375]\n",
      "3760: [discriminator loss: 0.695400, acc: 0.484375] [adversarial loss: 0.793415, acc: 0.125000]\n",
      "3761: [discriminator loss: 0.686300, acc: 0.523438] [adversarial loss: 0.690421, acc: 0.515625]\n",
      "3762: [discriminator loss: 0.686553, acc: 0.546875] [adversarial loss: 0.761843, acc: 0.203125]\n",
      "3763: [discriminator loss: 0.680949, acc: 0.578125] [adversarial loss: 0.693344, acc: 0.437500]\n",
      "3764: [discriminator loss: 0.695515, acc: 0.539062] [adversarial loss: 0.676948, acc: 0.515625]\n",
      "3765: [discriminator loss: 0.689518, acc: 0.562500] [adversarial loss: 0.806625, acc: 0.203125]\n",
      "3766: [discriminator loss: 0.695210, acc: 0.500000] [adversarial loss: 0.682472, acc: 0.593750]\n",
      "3767: [discriminator loss: 0.682371, acc: 0.578125] [adversarial loss: 0.760087, acc: 0.375000]\n",
      "3768: [discriminator loss: 0.664174, acc: 0.570312] [adversarial loss: 0.703748, acc: 0.593750]\n",
      "3769: [discriminator loss: 0.727049, acc: 0.453125] [adversarial loss: 0.751250, acc: 0.312500]\n",
      "3770: [discriminator loss: 0.702433, acc: 0.484375] [adversarial loss: 0.654112, acc: 0.703125]\n",
      "3771: [discriminator loss: 0.682403, acc: 0.570312] [adversarial loss: 0.799896, acc: 0.296875]\n",
      "3772: [discriminator loss: 0.686436, acc: 0.570312] [adversarial loss: 0.646459, acc: 0.734375]\n",
      "3773: [discriminator loss: 0.690293, acc: 0.507812] [adversarial loss: 0.789872, acc: 0.140625]\n",
      "3774: [discriminator loss: 0.674594, acc: 0.562500] [adversarial loss: 0.707291, acc: 0.421875]\n",
      "3775: [discriminator loss: 0.688706, acc: 0.554688] [adversarial loss: 0.643617, acc: 0.703125]\n",
      "3776: [discriminator loss: 0.703791, acc: 0.562500] [adversarial loss: 0.738538, acc: 0.265625]\n",
      "3777: [discriminator loss: 0.686324, acc: 0.562500] [adversarial loss: 0.656025, acc: 0.640625]\n",
      "3778: [discriminator loss: 0.710863, acc: 0.492188] [adversarial loss: 0.726850, acc: 0.375000]\n",
      "3779: [discriminator loss: 0.684737, acc: 0.578125] [adversarial loss: 0.742772, acc: 0.359375]\n",
      "3780: [discriminator loss: 0.703391, acc: 0.492188] [adversarial loss: 0.746881, acc: 0.281250]\n",
      "3781: [discriminator loss: 0.696025, acc: 0.523438] [adversarial loss: 0.671978, acc: 0.609375]\n",
      "3782: [discriminator loss: 0.691160, acc: 0.515625] [adversarial loss: 0.713446, acc: 0.375000]\n",
      "3783: [discriminator loss: 0.686775, acc: 0.546875] [adversarial loss: 0.745073, acc: 0.296875]\n",
      "3784: [discriminator loss: 0.674452, acc: 0.593750] [adversarial loss: 0.771671, acc: 0.328125]\n",
      "3785: [discriminator loss: 0.702060, acc: 0.523438] [adversarial loss: 0.654092, acc: 0.703125]\n",
      "3786: [discriminator loss: 0.695746, acc: 0.484375] [adversarial loss: 0.761391, acc: 0.250000]\n",
      "3787: [discriminator loss: 0.689903, acc: 0.546875] [adversarial loss: 0.662768, acc: 0.781250]\n",
      "3788: [discriminator loss: 0.690146, acc: 0.546875] [adversarial loss: 0.800765, acc: 0.140625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3789: [discriminator loss: 0.690871, acc: 0.523438] [adversarial loss: 0.615432, acc: 0.859375]\n",
      "3790: [discriminator loss: 0.704133, acc: 0.515625] [adversarial loss: 0.784679, acc: 0.203125]\n",
      "3791: [discriminator loss: 0.692244, acc: 0.554688] [adversarial loss: 0.625761, acc: 0.796875]\n",
      "3792: [discriminator loss: 0.689321, acc: 0.515625] [adversarial loss: 0.782980, acc: 0.140625]\n",
      "3793: [discriminator loss: 0.699501, acc: 0.460938] [adversarial loss: 0.621092, acc: 0.781250]\n",
      "3794: [discriminator loss: 0.687032, acc: 0.531250] [adversarial loss: 0.812749, acc: 0.078125]\n",
      "3795: [discriminator loss: 0.691606, acc: 0.468750] [adversarial loss: 0.689092, acc: 0.562500]\n",
      "3796: [discriminator loss: 0.711553, acc: 0.507812] [adversarial loss: 0.704516, acc: 0.375000]\n",
      "3797: [discriminator loss: 0.704319, acc: 0.468750] [adversarial loss: 0.788170, acc: 0.171875]\n",
      "3798: [discriminator loss: 0.692058, acc: 0.531250] [adversarial loss: 0.635332, acc: 0.750000]\n",
      "3799: [discriminator loss: 0.700080, acc: 0.515625] [adversarial loss: 0.814162, acc: 0.093750]\n",
      "3800: [discriminator loss: 0.697139, acc: 0.500000] [adversarial loss: 0.646943, acc: 0.734375]\n",
      "3801: [discriminator loss: 0.692824, acc: 0.539062] [adversarial loss: 0.780762, acc: 0.156250]\n",
      "3802: [discriminator loss: 0.692475, acc: 0.539062] [adversarial loss: 0.675428, acc: 0.593750]\n",
      "3803: [discriminator loss: 0.696141, acc: 0.523438] [adversarial loss: 0.690359, acc: 0.500000]\n",
      "3804: [discriminator loss: 0.711717, acc: 0.414062] [adversarial loss: 0.716771, acc: 0.421875]\n",
      "3805: [discriminator loss: 0.687781, acc: 0.585938] [adversarial loss: 0.586639, acc: 0.921875]\n",
      "3806: [discriminator loss: 0.698965, acc: 0.507812] [adversarial loss: 0.833908, acc: 0.171875]\n",
      "3807: [discriminator loss: 0.700767, acc: 0.500000] [adversarial loss: 0.656886, acc: 0.656250]\n",
      "3808: [discriminator loss: 0.697394, acc: 0.484375] [adversarial loss: 0.722173, acc: 0.390625]\n",
      "3809: [discriminator loss: 0.682962, acc: 0.546875] [adversarial loss: 0.674896, acc: 0.578125]\n",
      "3810: [discriminator loss: 0.691856, acc: 0.515625] [adversarial loss: 0.777476, acc: 0.171875]\n",
      "3811: [discriminator loss: 0.689175, acc: 0.492188] [adversarial loss: 0.696375, acc: 0.531250]\n",
      "3812: [discriminator loss: 0.695527, acc: 0.531250] [adversarial loss: 0.704002, acc: 0.437500]\n",
      "3813: [discriminator loss: 0.692295, acc: 0.523438] [adversarial loss: 0.805638, acc: 0.171875]\n",
      "3814: [discriminator loss: 0.704836, acc: 0.445312] [adversarial loss: 0.641501, acc: 0.734375]\n",
      "3815: [discriminator loss: 0.690337, acc: 0.539062] [adversarial loss: 0.748303, acc: 0.312500]\n",
      "3816: [discriminator loss: 0.684543, acc: 0.539062] [adversarial loss: 0.753687, acc: 0.390625]\n",
      "3817: [discriminator loss: 0.689395, acc: 0.578125] [adversarial loss: 0.710026, acc: 0.437500]\n",
      "3818: [discriminator loss: 0.702563, acc: 0.445312] [adversarial loss: 0.706759, acc: 0.453125]\n",
      "3819: [discriminator loss: 0.686198, acc: 0.554688] [adversarial loss: 0.782041, acc: 0.156250]\n",
      "3820: [discriminator loss: 0.688172, acc: 0.539062] [adversarial loss: 0.686919, acc: 0.531250]\n",
      "3821: [discriminator loss: 0.690710, acc: 0.546875] [adversarial loss: 0.784660, acc: 0.250000]\n",
      "3822: [discriminator loss: 0.689992, acc: 0.570312] [adversarial loss: 0.642352, acc: 0.750000]\n",
      "3823: [discriminator loss: 0.698383, acc: 0.476562] [adversarial loss: 0.785917, acc: 0.062500]\n",
      "3824: [discriminator loss: 0.693579, acc: 0.484375] [adversarial loss: 0.675414, acc: 0.593750]\n",
      "3825: [discriminator loss: 0.699673, acc: 0.453125] [adversarial loss: 0.675236, acc: 0.640625]\n",
      "3826: [discriminator loss: 0.701609, acc: 0.468750] [adversarial loss: 0.756194, acc: 0.140625]\n",
      "3827: [discriminator loss: 0.682006, acc: 0.601562] [adversarial loss: 0.732563, acc: 0.453125]\n",
      "3828: [discriminator loss: 0.706245, acc: 0.515625] [adversarial loss: 0.662580, acc: 0.656250]\n",
      "3829: [discriminator loss: 0.692004, acc: 0.539062] [adversarial loss: 0.683299, acc: 0.578125]\n",
      "3830: [discriminator loss: 0.697334, acc: 0.515625] [adversarial loss: 0.664440, acc: 0.671875]\n",
      "3831: [discriminator loss: 0.695099, acc: 0.523438] [adversarial loss: 0.799716, acc: 0.156250]\n",
      "3832: [discriminator loss: 0.708515, acc: 0.429688] [adversarial loss: 0.665452, acc: 0.703125]\n",
      "3833: [discriminator loss: 0.692421, acc: 0.546875] [adversarial loss: 0.713994, acc: 0.328125]\n",
      "3834: [discriminator loss: 0.697668, acc: 0.445312] [adversarial loss: 0.669690, acc: 0.562500]\n",
      "3835: [discriminator loss: 0.685649, acc: 0.562500] [adversarial loss: 0.736485, acc: 0.281250]\n",
      "3836: [discriminator loss: 0.689550, acc: 0.539062] [adversarial loss: 0.756977, acc: 0.187500]\n",
      "3837: [discriminator loss: 0.698015, acc: 0.523438] [adversarial loss: 0.678390, acc: 0.578125]\n",
      "3838: [discriminator loss: 0.694515, acc: 0.484375] [adversarial loss: 0.768917, acc: 0.343750]\n",
      "3839: [discriminator loss: 0.698692, acc: 0.539062] [adversarial loss: 0.662924, acc: 0.625000]\n",
      "3840: [discriminator loss: 0.681294, acc: 0.531250] [adversarial loss: 0.796360, acc: 0.125000]\n",
      "3841: [discriminator loss: 0.701915, acc: 0.476562] [adversarial loss: 0.694262, acc: 0.531250]\n",
      "3842: [discriminator loss: 0.696839, acc: 0.492188] [adversarial loss: 0.707639, acc: 0.390625]\n",
      "3843: [discriminator loss: 0.683861, acc: 0.570312] [adversarial loss: 0.759195, acc: 0.265625]\n",
      "3844: [discriminator loss: 0.704853, acc: 0.476562] [adversarial loss: 0.672177, acc: 0.671875]\n",
      "3845: [discriminator loss: 0.705036, acc: 0.421875] [adversarial loss: 0.728157, acc: 0.218750]\n",
      "3846: [discriminator loss: 0.679664, acc: 0.593750] [adversarial loss: 0.703265, acc: 0.500000]\n",
      "3847: [discriminator loss: 0.685283, acc: 0.601562] [adversarial loss: 0.690192, acc: 0.531250]\n",
      "3848: [discriminator loss: 0.713021, acc: 0.476562] [adversarial loss: 0.698083, acc: 0.500000]\n",
      "3849: [discriminator loss: 0.684138, acc: 0.570312] [adversarial loss: 0.769074, acc: 0.109375]\n",
      "3850: [discriminator loss: 0.700215, acc: 0.492188] [adversarial loss: 0.644310, acc: 0.875000]\n",
      "3851: [discriminator loss: 0.687368, acc: 0.546875] [adversarial loss: 0.762525, acc: 0.109375]\n",
      "3852: [discriminator loss: 0.688022, acc: 0.523438] [adversarial loss: 0.657082, acc: 0.765625]\n",
      "3853: [discriminator loss: 0.680265, acc: 0.546875] [adversarial loss: 0.770241, acc: 0.250000]\n",
      "3854: [discriminator loss: 0.700377, acc: 0.500000] [adversarial loss: 0.654477, acc: 0.703125]\n",
      "3855: [discriminator loss: 0.679940, acc: 0.601562] [adversarial loss: 0.736887, acc: 0.390625]\n",
      "3856: [discriminator loss: 0.708272, acc: 0.421875] [adversarial loss: 0.699579, acc: 0.484375]\n",
      "3857: [discriminator loss: 0.682991, acc: 0.546875] [adversarial loss: 0.784125, acc: 0.125000]\n",
      "3858: [discriminator loss: 0.702110, acc: 0.460938] [adversarial loss: 0.650951, acc: 0.750000]\n",
      "3859: [discriminator loss: 0.694713, acc: 0.523438] [adversarial loss: 0.787066, acc: 0.171875]\n",
      "3860: [discriminator loss: 0.696516, acc: 0.507812] [adversarial loss: 0.607946, acc: 0.921875]\n",
      "3861: [discriminator loss: 0.698830, acc: 0.539062] [adversarial loss: 0.779672, acc: 0.250000]\n",
      "3862: [discriminator loss: 0.705426, acc: 0.492188] [adversarial loss: 0.612550, acc: 0.828125]\n",
      "3863: [discriminator loss: 0.701898, acc: 0.437500] [adversarial loss: 0.823508, acc: 0.125000]\n",
      "3864: [discriminator loss: 0.702570, acc: 0.492188] [adversarial loss: 0.697873, acc: 0.406250]\n",
      "3865: [discriminator loss: 0.689203, acc: 0.578125] [adversarial loss: 0.723879, acc: 0.328125]\n",
      "3866: [discriminator loss: 0.693640, acc: 0.539062] [adversarial loss: 0.642222, acc: 0.718750]\n",
      "3867: [discriminator loss: 0.697940, acc: 0.523438] [adversarial loss: 0.751816, acc: 0.250000]\n",
      "3868: [discriminator loss: 0.682896, acc: 0.523438] [adversarial loss: 0.650821, acc: 0.640625]\n",
      "3869: [discriminator loss: 0.695427, acc: 0.515625] [adversarial loss: 0.749880, acc: 0.234375]\n",
      "3870: [discriminator loss: 0.694100, acc: 0.507812] [adversarial loss: 0.664074, acc: 0.734375]\n",
      "3871: [discriminator loss: 0.705725, acc: 0.445312] [adversarial loss: 0.723776, acc: 0.312500]\n",
      "3872: [discriminator loss: 0.685036, acc: 0.531250] [adversarial loss: 0.734561, acc: 0.390625]\n",
      "3873: [discriminator loss: 0.689323, acc: 0.507812] [adversarial loss: 0.754307, acc: 0.296875]\n",
      "3874: [discriminator loss: 0.682798, acc: 0.585938] [adversarial loss: 0.689403, acc: 0.500000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3875: [discriminator loss: 0.698426, acc: 0.546875] [adversarial loss: 0.703375, acc: 0.437500]\n",
      "3876: [discriminator loss: 0.688232, acc: 0.531250] [adversarial loss: 0.647941, acc: 0.671875]\n",
      "3877: [discriminator loss: 0.693726, acc: 0.523438] [adversarial loss: 0.790397, acc: 0.156250]\n",
      "3878: [discriminator loss: 0.698751, acc: 0.562500] [adversarial loss: 0.654360, acc: 0.578125]\n",
      "3879: [discriminator loss: 0.697633, acc: 0.523438] [adversarial loss: 0.729999, acc: 0.328125]\n",
      "3880: [discriminator loss: 0.698711, acc: 0.453125] [adversarial loss: 0.680148, acc: 0.640625]\n",
      "3881: [discriminator loss: 0.693321, acc: 0.531250] [adversarial loss: 0.745283, acc: 0.281250]\n",
      "3882: [discriminator loss: 0.681847, acc: 0.531250] [adversarial loss: 0.752529, acc: 0.359375]\n",
      "3883: [discriminator loss: 0.693475, acc: 0.531250] [adversarial loss: 0.753753, acc: 0.203125]\n",
      "3884: [discriminator loss: 0.711651, acc: 0.406250] [adversarial loss: 0.662815, acc: 0.671875]\n",
      "3885: [discriminator loss: 0.688837, acc: 0.492188] [adversarial loss: 0.717440, acc: 0.328125]\n",
      "3886: [discriminator loss: 0.688914, acc: 0.578125] [adversarial loss: 0.606634, acc: 0.859375]\n",
      "3887: [discriminator loss: 0.705900, acc: 0.507812] [adversarial loss: 0.785843, acc: 0.203125]\n",
      "3888: [discriminator loss: 0.689768, acc: 0.546875] [adversarial loss: 0.638557, acc: 0.796875]\n",
      "3889: [discriminator loss: 0.676173, acc: 0.578125] [adversarial loss: 0.789883, acc: 0.234375]\n",
      "3890: [discriminator loss: 0.696880, acc: 0.492188] [adversarial loss: 0.715695, acc: 0.421875]\n",
      "3891: [discriminator loss: 0.680761, acc: 0.578125] [adversarial loss: 0.769199, acc: 0.265625]\n",
      "3892: [discriminator loss: 0.705201, acc: 0.539062] [adversarial loss: 0.681631, acc: 0.546875]\n",
      "3893: [discriminator loss: 0.691385, acc: 0.460938] [adversarial loss: 0.751285, acc: 0.265625]\n",
      "3894: [discriminator loss: 0.690530, acc: 0.531250] [adversarial loss: 0.691592, acc: 0.484375]\n",
      "3895: [discriminator loss: 0.694386, acc: 0.507812] [adversarial loss: 0.695468, acc: 0.453125]\n",
      "3896: [discriminator loss: 0.691381, acc: 0.539062] [adversarial loss: 0.687744, acc: 0.500000]\n",
      "3897: [discriminator loss: 0.694456, acc: 0.507812] [adversarial loss: 0.802884, acc: 0.093750]\n",
      "3898: [discriminator loss: 0.694871, acc: 0.500000] [adversarial loss: 0.663778, acc: 0.671875]\n",
      "3899: [discriminator loss: 0.688878, acc: 0.531250] [adversarial loss: 0.739421, acc: 0.421875]\n",
      "3900: [discriminator loss: 0.676676, acc: 0.609375] [adversarial loss: 0.814416, acc: 0.109375]\n",
      "3901: [discriminator loss: 0.709547, acc: 0.500000] [adversarial loss: 0.652828, acc: 0.781250]\n",
      "3902: [discriminator loss: 0.696945, acc: 0.562500] [adversarial loss: 0.780819, acc: 0.093750]\n",
      "3903: [discriminator loss: 0.695374, acc: 0.515625] [adversarial loss: 0.627849, acc: 0.812500]\n",
      "3904: [discriminator loss: 0.705052, acc: 0.507812] [adversarial loss: 0.765707, acc: 0.218750]\n",
      "3905: [discriminator loss: 0.699781, acc: 0.460938] [adversarial loss: 0.679845, acc: 0.625000]\n",
      "3906: [discriminator loss: 0.689886, acc: 0.523438] [adversarial loss: 0.763799, acc: 0.218750]\n",
      "3907: [discriminator loss: 0.689044, acc: 0.507812] [adversarial loss: 0.680063, acc: 0.500000]\n",
      "3908: [discriminator loss: 0.700074, acc: 0.445312] [adversarial loss: 0.670025, acc: 0.671875]\n",
      "3909: [discriminator loss: 0.690789, acc: 0.531250] [adversarial loss: 0.762811, acc: 0.250000]\n",
      "3910: [discriminator loss: 0.682673, acc: 0.578125] [adversarial loss: 0.695522, acc: 0.546875]\n",
      "3911: [discriminator loss: 0.702565, acc: 0.453125] [adversarial loss: 0.796005, acc: 0.078125]\n",
      "3912: [discriminator loss: 0.685697, acc: 0.507812] [adversarial loss: 0.672268, acc: 0.609375]\n",
      "3913: [discriminator loss: 0.676677, acc: 0.601562] [adversarial loss: 0.801662, acc: 0.187500]\n",
      "3914: [discriminator loss: 0.680890, acc: 0.562500] [adversarial loss: 0.620284, acc: 0.703125]\n",
      "3915: [discriminator loss: 0.698855, acc: 0.531250] [adversarial loss: 0.709303, acc: 0.453125]\n",
      "3916: [discriminator loss: 0.681540, acc: 0.601562] [adversarial loss: 0.761411, acc: 0.265625]\n",
      "3917: [discriminator loss: 0.703686, acc: 0.484375] [adversarial loss: 0.720551, acc: 0.343750]\n",
      "3918: [discriminator loss: 0.690901, acc: 0.492188] [adversarial loss: 0.677883, acc: 0.578125]\n",
      "3919: [discriminator loss: 0.702294, acc: 0.484375] [adversarial loss: 0.804624, acc: 0.140625]\n",
      "3920: [discriminator loss: 0.695118, acc: 0.468750] [adversarial loss: 0.678191, acc: 0.578125]\n",
      "3921: [discriminator loss: 0.696590, acc: 0.492188] [adversarial loss: 0.786987, acc: 0.140625]\n",
      "3922: [discriminator loss: 0.704068, acc: 0.500000] [adversarial loss: 0.635920, acc: 0.781250]\n",
      "3923: [discriminator loss: 0.690278, acc: 0.531250] [adversarial loss: 0.753753, acc: 0.250000]\n",
      "3924: [discriminator loss: 0.700704, acc: 0.476562] [adversarial loss: 0.621899, acc: 0.937500]\n",
      "3925: [discriminator loss: 0.694944, acc: 0.531250] [adversarial loss: 0.760822, acc: 0.187500]\n",
      "3926: [discriminator loss: 0.682077, acc: 0.625000] [adversarial loss: 0.687265, acc: 0.562500]\n",
      "3927: [discriminator loss: 0.701650, acc: 0.523438] [adversarial loss: 0.739833, acc: 0.218750]\n",
      "3928: [discriminator loss: 0.688887, acc: 0.507812] [adversarial loss: 0.677011, acc: 0.609375]\n",
      "3929: [discriminator loss: 0.695460, acc: 0.523438] [adversarial loss: 0.731189, acc: 0.296875]\n",
      "3930: [discriminator loss: 0.684101, acc: 0.585938] [adversarial loss: 0.654889, acc: 0.765625]\n",
      "3931: [discriminator loss: 0.692640, acc: 0.492188] [adversarial loss: 0.788313, acc: 0.140625]\n",
      "3932: [discriminator loss: 0.714553, acc: 0.484375] [adversarial loss: 0.607228, acc: 0.875000]\n",
      "3933: [discriminator loss: 0.713501, acc: 0.492188] [adversarial loss: 0.833792, acc: 0.078125]\n",
      "3934: [discriminator loss: 0.695953, acc: 0.476562] [adversarial loss: 0.585126, acc: 0.984375]\n",
      "3935: [discriminator loss: 0.710880, acc: 0.507812] [adversarial loss: 0.846919, acc: 0.046875]\n",
      "3936: [discriminator loss: 0.711807, acc: 0.468750] [adversarial loss: 0.683426, acc: 0.625000]\n",
      "3937: [discriminator loss: 0.695402, acc: 0.507812] [adversarial loss: 0.737182, acc: 0.281250]\n",
      "3938: [discriminator loss: 0.685712, acc: 0.570312] [adversarial loss: 0.746643, acc: 0.234375]\n",
      "3939: [discriminator loss: 0.687687, acc: 0.523438] [adversarial loss: 0.776860, acc: 0.218750]\n",
      "3940: [discriminator loss: 0.687787, acc: 0.578125] [adversarial loss: 0.723090, acc: 0.343750]\n",
      "3941: [discriminator loss: 0.694964, acc: 0.546875] [adversarial loss: 0.656273, acc: 0.671875]\n",
      "3942: [discriminator loss: 0.686864, acc: 0.523438] [adversarial loss: 0.728458, acc: 0.375000]\n",
      "3943: [discriminator loss: 0.694430, acc: 0.531250] [adversarial loss: 0.745369, acc: 0.281250]\n",
      "3944: [discriminator loss: 0.688401, acc: 0.484375] [adversarial loss: 0.683840, acc: 0.625000]\n",
      "3945: [discriminator loss: 0.692750, acc: 0.515625] [adversarial loss: 0.656525, acc: 0.734375]\n",
      "3946: [discriminator loss: 0.702401, acc: 0.468750] [adversarial loss: 0.736137, acc: 0.203125]\n",
      "3947: [discriminator loss: 0.680232, acc: 0.593750] [adversarial loss: 0.741517, acc: 0.234375]\n",
      "3948: [discriminator loss: 0.704582, acc: 0.437500] [adversarial loss: 0.651460, acc: 0.812500]\n",
      "3949: [discriminator loss: 0.691558, acc: 0.507812] [adversarial loss: 0.723677, acc: 0.375000]\n",
      "3950: [discriminator loss: 0.691183, acc: 0.539062] [adversarial loss: 0.723109, acc: 0.421875]\n",
      "3951: [discriminator loss: 0.694230, acc: 0.460938] [adversarial loss: 0.713414, acc: 0.453125]\n",
      "3952: [discriminator loss: 0.684245, acc: 0.578125] [adversarial loss: 0.673004, acc: 0.578125]\n",
      "3953: [discriminator loss: 0.694954, acc: 0.500000] [adversarial loss: 0.770208, acc: 0.140625]\n",
      "3954: [discriminator loss: 0.692658, acc: 0.531250] [adversarial loss: 0.648188, acc: 0.656250]\n",
      "3955: [discriminator loss: 0.704348, acc: 0.476562] [adversarial loss: 0.707365, acc: 0.343750]\n",
      "3956: [discriminator loss: 0.697129, acc: 0.523438] [adversarial loss: 0.749905, acc: 0.125000]\n",
      "3957: [discriminator loss: 0.701268, acc: 0.460938] [adversarial loss: 0.671623, acc: 0.734375]\n",
      "3958: [discriminator loss: 0.686896, acc: 0.523438] [adversarial loss: 0.711160, acc: 0.359375]\n",
      "3959: [discriminator loss: 0.686837, acc: 0.554688] [adversarial loss: 0.693618, acc: 0.453125]\n",
      "3960: [discriminator loss: 0.692766, acc: 0.531250] [adversarial loss: 0.779300, acc: 0.171875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3961: [discriminator loss: 0.692980, acc: 0.515625] [adversarial loss: 0.684395, acc: 0.671875]\n",
      "3962: [discriminator loss: 0.693123, acc: 0.507812] [adversarial loss: 0.713300, acc: 0.437500]\n",
      "3963: [discriminator loss: 0.693420, acc: 0.539062] [adversarial loss: 0.739014, acc: 0.328125]\n",
      "3964: [discriminator loss: 0.696418, acc: 0.468750] [adversarial loss: 0.742669, acc: 0.234375]\n",
      "3965: [discriminator loss: 0.682734, acc: 0.585938] [adversarial loss: 0.679927, acc: 0.562500]\n",
      "3966: [discriminator loss: 0.699432, acc: 0.468750] [adversarial loss: 0.764820, acc: 0.234375]\n",
      "3967: [discriminator loss: 0.690457, acc: 0.484375] [adversarial loss: 0.675895, acc: 0.640625]\n",
      "3968: [discriminator loss: 0.676768, acc: 0.578125] [adversarial loss: 1.058795, acc: 0.078125]\n",
      "3969: [discriminator loss: 0.751028, acc: 0.492188] [adversarial loss: 0.631519, acc: 0.875000]\n",
      "3970: [discriminator loss: 0.704028, acc: 0.476562] [adversarial loss: 0.588982, acc: 0.906250]\n",
      "3971: [discriminator loss: 0.695582, acc: 0.484375] [adversarial loss: 0.742022, acc: 0.125000]\n",
      "3972: [discriminator loss: 0.689449, acc: 0.546875] [adversarial loss: 0.625025, acc: 0.843750]\n",
      "3973: [discriminator loss: 0.678636, acc: 0.562500] [adversarial loss: 0.692518, acc: 0.500000]\n",
      "3974: [discriminator loss: 0.690287, acc: 0.554688] [adversarial loss: 0.717831, acc: 0.265625]\n",
      "3975: [discriminator loss: 0.678832, acc: 0.570312] [adversarial loss: 0.748226, acc: 0.281250]\n",
      "3976: [discriminator loss: 0.684904, acc: 0.515625] [adversarial loss: 0.820247, acc: 0.140625]\n",
      "3977: [discriminator loss: 0.713855, acc: 0.500000] [adversarial loss: 0.633970, acc: 0.765625]\n",
      "3978: [discriminator loss: 0.685870, acc: 0.531250] [adversarial loss: 0.749638, acc: 0.281250]\n",
      "3979: [discriminator loss: 0.690157, acc: 0.570312] [adversarial loss: 0.679249, acc: 0.515625]\n",
      "3980: [discriminator loss: 0.684628, acc: 0.531250] [adversarial loss: 0.801862, acc: 0.218750]\n",
      "3981: [discriminator loss: 0.694795, acc: 0.476562] [adversarial loss: 0.688392, acc: 0.578125]\n",
      "3982: [discriminator loss: 0.698151, acc: 0.445312] [adversarial loss: 0.654837, acc: 0.640625]\n",
      "3983: [discriminator loss: 0.700256, acc: 0.484375] [adversarial loss: 0.831326, acc: 0.078125]\n",
      "3984: [discriminator loss: 0.708629, acc: 0.476562] [adversarial loss: 0.616617, acc: 0.859375]\n",
      "3985: [discriminator loss: 0.695864, acc: 0.492188] [adversarial loss: 0.707690, acc: 0.421875]\n",
      "3986: [discriminator loss: 0.691059, acc: 0.507812] [adversarial loss: 0.683261, acc: 0.562500]\n",
      "3987: [discriminator loss: 0.683965, acc: 0.570312] [adversarial loss: 0.674048, acc: 0.593750]\n",
      "3988: [discriminator loss: 0.708490, acc: 0.445312] [adversarial loss: 0.738906, acc: 0.281250]\n",
      "3989: [discriminator loss: 0.685369, acc: 0.585938] [adversarial loss: 0.749173, acc: 0.265625]\n",
      "3990: [discriminator loss: 0.708934, acc: 0.492188] [adversarial loss: 0.652178, acc: 0.812500]\n",
      "3991: [discriminator loss: 0.690256, acc: 0.523438] [adversarial loss: 0.717855, acc: 0.281250]\n",
      "3992: [discriminator loss: 0.688874, acc: 0.570312] [adversarial loss: 0.729438, acc: 0.343750]\n",
      "3993: [discriminator loss: 0.685312, acc: 0.593750] [adversarial loss: 0.626623, acc: 0.875000]\n",
      "3994: [discriminator loss: 0.693853, acc: 0.507812] [adversarial loss: 0.754385, acc: 0.250000]\n",
      "3995: [discriminator loss: 0.689162, acc: 0.476562] [adversarial loss: 0.625474, acc: 0.843750]\n",
      "3996: [discriminator loss: 0.694000, acc: 0.500000] [adversarial loss: 0.769040, acc: 0.203125]\n",
      "3997: [discriminator loss: 0.702743, acc: 0.468750] [adversarial loss: 0.698375, acc: 0.468750]\n",
      "3998: [discriminator loss: 0.691068, acc: 0.539062] [adversarial loss: 0.734156, acc: 0.296875]\n",
      "3999: [discriminator loss: 0.693992, acc: 0.484375] [adversarial loss: 0.686048, acc: 0.546875]\n",
      "cgan_mnist  labels for generated images:  [0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 158.4x158.4 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "build_and_train_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maninaya\\Anaconda3\\envs\\env\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_outputs  labels for generated images:  [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 158.4x158.4 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = load_model(\"cgan_mnist.h5\")\n",
    "class_label = 8\n",
    "test_generator(generator, class_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_outputs  labels for generated images:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACaCAYAAAAuLkPmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJztnXtQU2f+/59zcgMS7iCg6LLIakZZZcStTKUWhlbqdGtltxcdxZZx7JTp1tVpd9WpzrYyvV9s69CxutV22FbbXavVrbVbKxam2q0XRFbkItKY4RJIDCTN9eSc9+8Pvuf8jCQhIQkBet4zzzgDB9/nOc/rPLfzPJ+HAkBEiYqU6EjfgKhftkQARUVUIoCiIioRQFERlQigqIhKBFBURCUCKCqiEgEUFVGJAIqKqKRjaUZR1Jh8dgFAif7jz9+TxBpQVEQlAigqohIBFBVRTUgAo6OjQ/L/yGQyolariUQiIUqlkiiVSuF3FEURiUQSEp9ApVAoiEKhIAkJCaS4uJg88cQTJCMjgyQnJxOK8rt7FZQoiiIxMTFEoVCE1wjAmCVCCHyl6Oho3HPPPaioqMD3338Ps9kMm80Gq9UKl8sFhmHAsiwAwG634/861cOSP/6zZs2CXq8HAKxZswZff/01BgYG8NZbb6G5uRkcxwEATCYTmpqaMGfOHJ/3Hqi/p7RhwwaYzWZwHCf4AwDDMOjv70d7eztmzZoFiUQSFv+nn34aLpcLt8vpdOK5555Dbm5uUPn3eE/jAcCXX35ZAK23txft7e1YvXo1KisrkZ+fj/z8fMTFxYGiKNA0jcceewwcx+HVV18ddQEkJycLMAMAx3GwWCx46aWXsGXLFjzxxBN46qmncOHCBbdr4uLiwgKgWq3GuXPncPz4cTQ2NsJqteLGjRs4f/48bDab4H/gwAHExsaG3F+lUqGvrw/d3d1Yt24d5HI5JBIJ0tLSoNPpwHEcTCYT5HL55ANQo9GA4zh0dXXhwQcfHPEN379/PwBg2bJloy4AiqKwZs0aGI1GvPbaa5DJZMN8KYrCfffd51YblZWVgabpkAMglUqRm5uL9PR0xMbGQqFQIDU1FcnJydDpdIK/RqOBSqUKSw2oVCqH/YyiKBiNRnAcB5vNNqL3hARQp9PhzJkzSE5OHjFzcrkcDMMAAObNmxfSArg9SSQSrF27Frfq2rVrSExMDDkAEokEd955JyoqKqBSqZCSkoL4+Hj88Y9/dHsBqqurw9YEe0rR0dHC87ZarSN6T0gAH374YcycOdOvB7Jnzx5wHAej0QipVBqWAqBpGvn5+di9ezccDgeAoebP5XKhqakJZWVlIQeApmmo1WosW7YM+fn5SEhIQF5eHvr6+gT4NBpNUAAECh9N01i6dCmsVis4jsOJEyf8+rsJB2B0dLTXAcWtqaysTKgNFi5cGJYCWL58OZxOJ24Xx3Gw2+1wOBzYs2dPWACgKAoFBQXIy8tDUVERHnzwQSG/NpsNUVFRQQHgz99GR0fj2WefRVVVFe68805cvHgRTqcTHMdBq9X6VU4TDsCRkkKhQFlZGRiGgcvlgsFgCEsBSKVSVFVVDYOPZVmYTCY4HA44HA5YLJawAUDTNM6cOQOHwyGM+i0Wi98tRDD+sbGxsFqtwgv34osvYufOnfjxxx9htVrBMAxqamomL4AymQyZmZlobm6GyWSCxWKBw+GATqeDyWQCy7Lo6OgIWyc8KysLHR0dbn2uwcFBlJeX4+jRo2hpacHAwABsNlvYACSEICcnx60Wrqur86vzH6w/RVFYsmQJzGYzGhsb8cYbbyAzMxNKpRK1tbXC/QwMDExOAAkhyMvLE6YdePFA2O12lJSUhK0AaJrGu+++K/T7GIbBhg0boFQqERsbixkzZuDEiRPgOC6sAPL3Yrfbhfz//ve/DzuAI6Xk5GRYLBZg6D+bnADeXhM89NBD6Orqgt1uh9Vq9WtCOBj/6OhoNDY2wuFwoKamBlFRUaAoSuj7/PWvfwUAn/OBocp/aWmp8BK2tLREHEBCCDIyMsBxHHJyciY/gHwqKipCX18fXC4XKioqwloAUqkU69evR1FRkccO98WLFwEAWq027AC8/fbbAoBGo3FcAJiQkAAAuHTp0sQG0J/RFJ/UarXQOW5sbIRMJgtbAdA07fPe7r//fgBDzXMwnwJHShRFCfNvANDZ2elz5D8a/0DKgE+5ubkAhuZDQwFgRBYjyGQycvz4cVJQUODX9UuXLiUymYw4nU7S29tL5HJ52O6N4zi+sDyquLiYEEIITdNhWxgQHx9PGhoaiEQiIU6nk7hcLpKRkUF27txJpNLQrCF+/PHHidPpJNu2bfP7b2iaJsePHycASGFhYUjuIyI1oEwmw8WLF7F//36Pn39uTSqVSuiMu1wu5OfnR6wJmjJlCgYHBwEAZrPZ6ye5YPyjo6PR09MDu92Ouro6zJkzBx0dHbDb7XA6nX59LfLH/6OPPgLHcXA4HCN+WiRkaIbg1hp5NP4e7ykSANI0LTSpAPDMM88gPj5eaBZkMhmWLVsGvV4Ph8MBq9WKU6dOISYmJqRN0K2pqKgIZrMZP/74I+RyOeRyOaRSKWiaRlpaGqqqqnDq1ClhhN7W1hZS/3nz5sHhcMBut+Pjjz/GZ599hvvvvx+dnZ2Cp9PpxK9+9auQ5V8ul2Pfvn3Q6XSoq6tDa2srampqsH//ftTW1qK3txdOp1OYj2QYBjt27Bh1F2jcACiXy2E2m3G7XC6X28oPfoHC448/7vdXgNECkJycLCxFstlsOHfuHMrKyrBixQqcPHkSRqMRZrMZLpcLHMf5HI0H6k9RlPAFhi9w3ufWZDQag1oM4O36s2fP4ubNm8Int9vFsiyMRqNfK4EmBICEEKSmpiI1NRVpaWno7OwUMs4/7L6+Puj1esyZM8ev75/BAkgIwdSpU+F0OoVlWt4Ko7e3N+T+UqkUH3744bD1eC6XC4ODg6isrPSr9hutv0wmQ3Z2Nk6dOgWGYWC1WtHY2Ii9e/diypQpAQ1YJgSAvmqD0YzOQgEgn4qLiz3C193djaSkpLD6p6SkoL6+HmazGS0tLZg7d+6Y5z/YFAgT1P/d2Jgo0tsCA/Xnl+W7XK6I+I9W49XfkybknpCxEoCA4RMVmEQARUVUY9oEixJ1u8QaUFREJQIoKqISARQVUYkAioqoxPBsov+Y+XuSWAOKiqhEAEVFVCKAHkTTdPijQokihIwjACmKIklJSUQqlY5ZCDJPeuWVVwjLssRutxMAhGEYkpycHFbPSOaX91epVJG5j/GwGkYmk2HTpk3CujOWZYXVKCzLwul0ore3F08++WRYV4NIJJJhK2CAoWVZd999d1hWo5SUlKCzsxPr169HdHR0WFejeLu+oqICFRUVUCgUws9mzpyJwcFBaLVarFq1KmyrYSIO4FtvvQWHwwGn04kzZ87ggQcewL333ovZs2fjySefRF5eHt588018/fXX0Gq12Ldv34gwjAZAuVyOGzduYNOmTcN+t3TpUgHEUAIwY8YMtxdtcHDQ66JQi8Xi97rIQPJPUZSbj8vlEiqAnp4eaDQasCwLu93u99KwCQOgRCLB/v37UVVV5XMNIEVRSE9PR3t7Oy5fvoylS5eGHEDex9PPq6urBQCDiY3i6drZs2fjgw8+wJYtW7Bjxw4hTg5N05g3bx7Onz8vwOHv3uBA83+7nE4nLl68iNOnT7tFaKisrJxcACoUCnz00UcjZigpKQlHjhzB4OAgLBYLPvvss7AA6C3dGqHAnw08ofSnKAotLS0AMGJMmtH6Hz58GDabDWazGU1NTfjkk09w9epV9Pb2uq1Ur6qqmlwApqam4o033hixAJ5++mlcvnwZBoMB7e3tyMrKGjMA0tLShBrAZDKFBYCR0oIFC9DY2Ig333xzTPwTExOh1+tht9uFLRInT570Gg5vwgIYFRWF1atXe82IQqHAnj17YDabYTKZYDQaMTAwgL1794atABQKBWJiYqBQKLBo0SK3/pi3kMDhBJCiKOzcuRNVVVVBbwoKxLe4uFjYJNXb2zvi9tkJCaBarcZXX301LANKpRLp6ekoKipCcXExqqursXXrVrS2tsJsNqOjoyPkBUBRFI4cOQJvYlkWP/30E0pLS8MWnYtPd9xxB5YuXYrFixejoKAAc+fORXx8vN97ZUIBoFKpFDZnffrpp37XfhMGQJqmsWTJEnR3dwsPlv9XqVQiKirKbdRHURQ2b96MgYEB2O32kBYARVFITU31uFWU4ziUlZXh2WefFbZL/vTTTyEHQKlUYtGiRejo6BBC0XEcB4ZhYDAYoFargwYgEAD5GDC89Hr95BsFx8bG4tVXX/X4Znv6mVwux4oVK2Cz2Xz2h0ZTALzfzJkzcezYMezYsWPYNRkZGWhpaQl5fECaprF69WqcPXsWAwMD0Ol0Qhg0Xna7HdOnTx8zABcuXDjsZWRZFqmpqZMHQEJIQBvO+cKqq6sDAKSkpIStALwlPmyuryZptP4pKSl45plnUFhYiNraWvT397sBwHEcli9fPiYAJiYmYtWqVUhPT8eXX34p9IVdLhfWrFkzeQAcTbrjjjsADIX0GGsACwsLAQB5eXlhBYCmaRQWFiIlJQUbNmzAP//5T7AsC5Zl8ac//SmsAHrra37xxRfCy/CLBpCfvT948GDQBcBHp/fXu6ioCBzHYcWKFWEF0FNqaGgAMNQURgJAQgiuXbsGjuNw7Ngxr63XpAeQpmk4HA6sX78+6AKIjo6GRqPB4sWL/fI+dOgQAO/NfzgBjI2NFZrCcALoK/EnFZjNZuzevRtpaWkTH0CpVBpQCAq+D+htNByoPz/i9NXBpigK169fB69IAEBRFNra2sIaozouLs7roojU1FQwDAOO47B7927Ex8d7/DI04QAsLi5Ga2urULj8UQj8PJTD4YDRaMSNGzdgMBiEyFFTpkwJSQG0tbUJ3k6nE5s2bcIDDzyArVu34ty5czAajXC5XHC5XDhw4MCIo9FwARgTE4MrV66M+EVmtP4bNmyAwWDACy+8IJzYpFKpsHHjRrcDeywWi89mesIBSMjQwoS8vDyYzWa3QwR5sSyLvr4+4auIr6VLgfpnZ2cLxzN4825sbPQrOGS4AKQoCjt27IDdbofdbkdGRkbI/Z955hl0dXXBYDDAbDajv7/f7XmwLIvOzk6fA7AJC+CtSa1Wo7i4GF1dXbDZbOjr64PJZEJDQwOsVitOnjwZFgAoikJKSgp0Oh26u7vR2dmJRYsWjVl4OP4e+IMTKYpCbm4uqqur3Y5x1ev1wu9D6S+TyfDQQw/h/fffd2uBXC4XLl26hLS0tMl5VlwgSaVSjfhJKpz+4QZw7ty5HmthXkaj0WvXI1T5j4mJwbVr18AwjNeB3i8WwHADMB78582b5wahzWbDTz/95PdhNaHIfzAxGgNhQowPKPqPmb8njZtNSaJ+mRLDs4mKqMQaUFREJQIoKqISARQVUYkAioqoRABFRVRifEDRf8z8PUmsAUVFVCKAoiIqEUBREdWEBDBccewoiiJSqTSsp6EHI4lEQhQKBaHpsSk2mqaJUqkkBQUFJD09PWSntbtpIqyGkclk6OjoEA5N/uabb0K+GoQPz3blyhXs3bsXDQ0NaG5uxpQpUxAfHx/QmsBQ5J+maZSUlKCrq8vtpHJe69atC6v/Sy+9JCzQ5ThOOEa2pqZGOFw8UH+P9zSeAKRpGtOmTcOVK1eGPXCGYaDX64XoBb725gbqr1ar3eL0mc1mDA4Owmw2o7e3V4hfqNVqUVhYOOLJ7YH6UxSFyspKtLa2wmKxeIwP6HK58MUXX2DLli3CQl1v5wcH6r9s2TL88MMPbuckW61W4aDwOXPmYOXKlWhtbYVWq0VNTc3kApCmaezevRsMw8DpdAqF4HK54HA4cPjwYcTGxiIzMxMmkwkA0NraGjIANm3ahJaWlmFxCiUSCWJjY3H+/HnhxPSSkpIRa8NA/ZcvXy6seO7u7oZGo4HZbIbdbkdbW9uwBahSqdRnrL5A/HNzc91A5zgOBw4cgEwmE55BdnY2ysvL0d7ejg0bNowYJGnCAUgIwaJFi2A2m7Ft2za8/PLLeP311/HGG29g7dq1KC4uRlxcHNrb24W31FecvkD9VSoV5HK51/+vubkZg4ODsNls2L59O2JjY0MGIEVRqK+vh8vlQnd3N+Li4pCSkoKysjLMnDnT69L7hISEoP2VSiUMBoMAH8uy0Gq1WLhwofBMioqKoNPpcPbsWfT29iInJ2fUK9LHNYCpqalYtmwZVCoV4uPjERMTA5lMBpVKBalUitLSUlitVgBDzVEoayBf6eDBg8JeDKvVitLSUrdYysH60zSN2tpa1NXVYeHChaBpGkqlEgqFAnK5HBKJBFKp1K3QQxWdSyaTobe3F3a7HefPn8e//vUvzJ8/H48++igOHjyIffv2Qa/Xw+Vywel0oq6uzq9QKhMSQL42oGkaKpUKd999N8rKynDHHXegsrISTU1NcDgcsFqtyM3NDTuANE2jp6cHt8pkMgnbFUPpf+nSJWi1WqjVashkMkyfPh2JiYlYvHgx3nnnHXR1dWHDhg2QyWSYNWsW+vr6QuYvlUqRlZWFqVOnIioqCvHx8Xjuueeg1Wpx8+ZNt/4owzCYNm3a5AZQIpFg+/btuHLlCgYHB4XOf39/PzQajc+mcjQFQFGUW+1CURT27ds3bGMQ3zwdOnQICxYsCCmAX331Fex2O6xWK9ra2nDy5Ek0NTUJ8Zk5jsPFixeh0WiEGonvo4XCn38GUVFRUCgUePfdd6HVatHX14empia0trZCr9eDZVkcPnx48gEYFRWFadOm4fLly8PeOl4cx6GlpSWobYGerq2oqEB7ezv6+/u9jj7/85//YNWqVaivr8fVq1dx/fr1kAJ49epVj968GIbBlStXhKgE4YgNQ9M0SktL8Ze//EUYAF27dg1ZWVmgaRr79u0Dx3FwOp3Ytm3b5AJw06ZNQv/udvERofiCKC0tDSmA69ev91rwBoPBrc+jVCpx9913w2az+ewLBZp/mUyGL7/80uM9dHZ2Ij8/H21tbcJcHIb+s5ACKJFIMH/+fFy4cEHwaG9vR3JyMuRyOWJjY3H16lW4XC4wDDO5AKysrMSPP/4ohNxwuVxuUw8KhQIDAwPCgxkpsn4g/ikpKcIgg+M42Gw26PV6vPjii15rirNnz6K+vt7rSHw0ABAy1BTGxcXhxIkTSExMdPvdvffeK4THAEI7C3Drs+CnufgX8PXXX0deXh6ioqKQlZUlTIpPKgD5hz/SA/rqq68AjHxgTKD+U6dORWJiIpKTk4WBkK/73LVrl89+2GgB8JUUCgUaGxuFptrXVNBo/WfPno3m5mYAQ10Pq9UKvV6PmpoaYXQeTHSucQ2gv4mft/I1GAmnP0VRuHTpEjiO83oP4fCXyWQ4ePAgeHn7ChKMv1QqxYIFC7Bnzx5hGigpKQm5ubmgKEqInA+MrgaeFAB++OGHACITn48QgpUrV4JlWdy8eXNM/SmKQlVVlQCgr2vDlX+lUgmbzTZq/0kBoE6nAwCf8VHC4a9SqXDkyBFhQHTPPfcE5T/SZPbtaevWrbhVYw2gTCZDV1dXUP7jDkCpVIpdu3b59QAee+wxmM1mOJ1O3LhxI6TzYIQM1TB33nknqqur8d577+HYsWMoLy9HTU0NTpw4gZ6eHlgsFtjtdnz77bdBARAfHy9MqXg6f46maaxbtw5z5sxBYWEhmpqahIK32+0hP6xRIpGgqKgImzZtwpIlS9zuJyEhAadPnxbu1el0oqSkZHIAmJiYCJZl8dprr3mc1uAnSMvLy2EwGGAwGNDZ2enxcJtQ1AAJCQkoLCzEsWPHcOHCBRw6dAhNTU3Q6XSwWq3QaDR4/fXXfcLvjz9FUTh58iRuldPpBMuyMJlMsNvtGBwcFCDl1d/fj8zMzBFf1kDzz0+EG41GtLa2Ys2aNZg2bRq++OILN3+n04lFixaN2n/cAUhRlLC8ymg04oUXXkBSUhKSkpKQlZWFr7/+2i0untPpxMqVK1FQUBAWAPlUWFiIv/3tb2hvb4fD4RCmh3p7e0P6JUYmk+HatWtuIPJet09M9/X1+ez3BpP///73vzh16hROnz7tNtXDi2EYzJgxI+iTmsYdgDyE/Ow+wzCw2WxuE668bDYbHn300ZAfl+op0TSNEydOYHBwUIhIeuTIEb/gG42/SqXCPffcg6qqKly7dg06nQ4Mw4BlWdTX1/sNXrD559cl3ip/o8KOFsBxFZ5NJpORxMREYrfbSXZ2NhkYGCAGg4GYzeaAfBCCbYnTp08n06dPJ42NjcRisYy5fzAKhT9FUWS0bHjz9+gzngAMlSYDAJPR35Mm5KYkUZNHYnxAURGVWAOKiqhEAEVFVCKAoiIqEUBREZUYnk30HzN/TxJrQFERlQigqIhqwgD48MMPk5ycnHEZtUpUEIr0YoSRUnl5ufBhnGVZ2O12fPzxx2FbjCCXy5Gbm4sZM2YEfEpmKPyDOaMtWP/p06ejpKQEeXl5SE5ODnjR7Ej+Hu9pPAJIURTmzp0rrIwxmUzQ6/Xo6uoSViSHowCkUimuX7+Orq4uNDc34+233x5x7V+o/OPj49HX14cLFy7gkUcewdatW9HY2IicnBzI5XLQNC28EMGuR/SU+A1HfHSwmzdvCkEBDh8+jJqaGqjV6sl5XOvUqVPR1NQEk8kkbHrhw4Pxb2FMTAw++eQTAAj5xuzi4mJhrwMwtPCytrYWu3btQllZGVasWCHUiuEAcHBwUPDmo3Dx2x/5dYH8LjWGYcAwjM/wIKMBMDs7G01NTWhqasLnn3+O1atXY9asWUhLS8PmzZvR1NSE559/Hrm5uXjooYd8LoydcAD++c9/Rl1dHQYGBvDOO+/gkUceGbbtMDs7WyiUo0ePhgwAiqJgt9sFALZs2YL09HTQNI3KykocOnQIp0+fxqeffopVq1b53A02WgB4aTQaqFQqREdHY968edBoNDh+/DiuXLmC8vJyvPfee8ICXV8b9EcDICEEcXFxXms4fruqWq3GoUOHYLfbsXHjRo8n1084AOPj4xEVFSUs+Ly9H5SbmytsTGcYJuQHNisUCiQkJAwLeKlWq9HQ0ACNRoOzZ8/iu+++Q2pqakgBnDp1qhB25PZtCfyWhJiYGEilUrz//vvQ6/VwOBx49dVXQw6grySXy/H++++jublZiGJhNBoxe/bsiQ+gr5SYmIjOzk4hYsLSpUtDXgN5SlFRUTh8+DCsVitsNhtYlsXAwMCI2wEC9W9vb4dGo4FarfY6AJFIJMjMzERTU5MQMnc0TeBo4SsuLhbCcvAyGo1edwZOKgDnz58vBMtpbW0NSxN4e5LJZPjmm2/cImSxLIu+vr6g4uN5unbXrl3YvHkzpk2bBoVCIUBIURSysrKwe/duHDx40C1OtNPp9DlaDsXzpygKJSUlsFqtcLlcsNlsMJvN0Gg0WLly5ah2JU5IAHNycrB371788MMPeOWVV/z6m2D99+/fLwTh4TgOPT092LlzJx544AG/piYC8c/JycG6detw9OhRsCzr5gvALVA4ADQ0NCA1NTWkAEZFRWHGjBmoqKjAhQsX0NHRIQyEBgYG8MILL0Amk/n18k8qAOVyOdasWYM9e/ZAr9ejr68v7JuScnJyhJ1pDMOgoaEB8+fPR2VlJT7++GOUl5ePOkTt7ddJpVLExMQgIyMD/f398CYePrvdLgRI9xagfTT5VyqVaG9vH+ZrNpvx97//HUqlMqD5yUCYGNdfQpxOJ6mtrSV33XUXUSqVxGq1koyMjLB6/vrXvxbO4dBoNKSjo4PMmTOHSCQS8tvf/pZkZWXxhRmwKIoiFEURlUpFkpOTiUQiIVarlfT09JD09HQye/ZscuDAAWI2mwnHcYRhGKLRaIhOpyOEEMKyLAFAKIoiarU6ZF+FLBYLyc/PJ2fOnCEDAwMEAHG5XCQmJoasWrWKHDhwgMTGxobEa5jGcw3Ip8zMTNTX18Nms+HixYvIyMgIWw24fft2mEwmaLVaZGdng6ZpSKVSzJ8/H59//jkaGxs9jvwC9Q/kK0txcbHQPJeXl2Px4sX49ttvMXfu3JDnn5Ch/p9SqUR9fb3QBdi9e3dYasAJASAhQxPRfJhYnU4XNgDnzp0LvV7vcbBx7tw5AEB1dXXY/L3l/fLly9DpdPj8889x6dIl2Gw2ZGVlhdWfnwvVarV4+eWXkZaWhpiYmBH3Ck9KAAkh+PTTT9Hf3w+Hw+EzWHYw/rGxsUhKSvL4O37+S6vVjimANE2joaEBWq0Wer0eRqMRHMeFrQa8Pc2aNQvFxcX44IMP0NbWhoMHD/4yAVQqlTCbzWBZFs8//3xYAKBp2iuAjY2NAIZitAQLYCCdepqmkZ6ejqqqKpw7d04YKXv7HBdo/kfqDlAUhSlTpuD48eMwGAywWCwTG0A+Q/4O6wkZGhGfPn1amAwNZYzm25Ongk1NTRVGo9u3bw8aQH8/7BNCkJycjHXr1uHdd9/FwMCAMEIPFYAFBQVen6dEIkFBQQHq6+uFeVGNRhMyACMSGYGiKLJ48WKSmZlJSktLycaNG4lCoSAmk4lIpVLCsiyZOnUq+cMf/kA2b95MFAoFiY6OJoQQwjAMOX/+PLnrrru8+iCIJek0TRMARCKREADkN7/5DTl79iyJi4sjhBBy5swZsmTJEuLrufnj393dTZKTk4lMJnM7/ZKmaULTNLn//vvJggULiN1uJxkZGSQ3N5fMmDGDuFwuMn36dNLW1kby8/MJx3FB5/+dd94hK1euJD///DP53//+R2pra8krr7wilIXT6SQGg4F8//33ZO3atYRlWa959+Xv7eKINcHx8fF4/vnn4XQ64XA44HA4YDQahcEGL/6Nb25uxsaNG0N+UMytiT8yYtmyZTh69KiwSsbpdKK6utrjx/fR+F+/fh2+xAdr0mg0OHLkCL777jvU19fj0qVL6Onp8Vl7Bpr/b7/9Vnje/Bwofw99fX146623sHDhQr9bq3FfA972M9Lb20sIIUShUBCXy0Wio6OFGujy5cvkyy+/JPX19eTy5cvEYDD3GtPtAAABdklEQVSM6DPaGlAmk5G9e/eSoqIikp6eThQKBQFAfv75Z/K73/2OtLW1+az5AvHPz88n586dG3Eujy8ohmGIRCIhP/zwA9m2bRv57rvvgvK/Vfn5+WTt2rUkKiqKNDc3k+XLl5N//OMfxGKxkH//+9/E5XIRhmH8yrsvf28XR6wG9JRGilLvTxqN/6xZszweGFNRURE2f4qihh06SNM07rvvPuH41N7eXrS0tODEiRMoLi4ek/B0waaAmBhvAIbzAfj6G7VajRs3bqCrqwtPPfUU5s+fP+rl8RMx/5ECMOJNcDiEUTTBwcTDC4V/KDVe/T1pXH8LHkuN5Yso6v9LDM8mKqISa0BREZUIoKiISgRQVEQlAigqohIBFBVRiQCKiqhEAEVFVCKAoiIqEUBREZUIoKiISgRQVEQlAigqohIBFBVRiQCKiqhEAEVFVCKAoiIqEUBREZUIoKiISgRQVEQlAigqohIBFBVRiQCKiqhEAEVFVP8PISErXqQ030QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 158.4x158.4 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_generator(generator, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_outputs  labels for generated images:  [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 158.4x158.4 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_generator(generator, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
